<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[oracle序列]]></title>
    <url>%2Famberwest.github.io%2F2019%2F04%2F14%2Foracle%E5%BA%8F%E5%88%97%E7%9A%84%E5%88%9B%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[创建序列oracle可以用sequence关键字创建序列，语法如下： 12345678create sequence [user.]sequence_nameincrement by nstart with nmaxvalue | nomaxvalueminvalue | nominvaluecycle | nocyclecache | nocacheorder | noorader 参数说明： user：也就是schema，如果没有表明，则默认是用当前的 increment：默认1，即每次增加的步长。-1则是减少 start with：开始的数字。如果有设定最小值minvalue，初始值一定不能小于minvalue maxvalue：为序列指定可生成的最大值 minvalue：指定序列的最小值 nocycle：一直累加，不循环 cache：预分配，可以加快访问速度。参数可接受最小值为2。在循环序列中，cache值必须小于cycle的值。cache最大值要小于(ceil(maxvalue-minvalue))/abs(increment) 1234567create sequence seqminvalue 1maxvalue 10000start with 1increment by 1nocyclecache 10; 获取序列的时候使用nextval，需要注意的是，第一次返回的其实是初始值，之后返回的是每次完成increment后的值。如： 1select seq.nextval from dual; 修改序列修改序列还是用alter，不过呢，并不能修改start参数，需要想要修改start值，需要删除序列再重新创建。 1alter sequence [user.]sequence_name; 删除序列删除自然用drop 1drop sequence sequence_name; 参考文章create sequence]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>sequence</tag>
        <tag>序列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql中的isnull、ifnull和nullif]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F10%2Fmysql%E4%B8%AD%E7%9A%84isnull%E3%80%81ifnull%E5%92%8Cnullif%2F</url>
    <content type="text"><![CDATA[关于ifnull和nullif两个函数刚看的时候有点混淆，所以记录一下。 IFNULL(*expr1*,*expr2*)：如果expr1不为NULL则返回expr1，否则返回expr2。 12345678select ifnull(1, &apos;yes&apos;);&gt; 1select ifnull(0, &apos;yes&apos;);&gt; 0select ifnull(null, &apos;yes&apos;);&gt; yesselect ifnull(1/0, &apos;yes&apos;);&gt; yes NULLIF(*expr1*,*expr2*)：如果expr1=expr2时返回NULL，否则返回expr1。 1234select nullif(1, 1);&gt; NULLselect nullif(1, 0);&gt; 1 isnull(expr)：如果expr为NULL返回1，否则返回0。 1234select isnull(null);&gt; 1select isnull(1);&gt; 0 最后用员工表将这三个函数都利用起来：判断员工是否有获得奖金，没有获得奖金的员工可以得到100元的安慰奖金。在ifnull函数中还使用了nullif函数，直接用ifnull(comm,100)的话，员工TURNER的奖金为0元将不能获得安慰金。 12345678910111213141516171819select ename, comm, isnull(comm), nullif(comm, 0), ifnull(nullif(comm, 0), 100) from emp;&gt;&gt;&gt;+--------+---------+--------------+-----------------+------------------------------+| ename | comm | isnull(comm) | nullif(comm, 0) | ifnull(nullif(comm, 0), 100) |+--------+---------+--------------+-----------------+------------------------------+| SMITH | NULL | 1 | NULL | 100.00 || ALLEN | 300.00 | 0 | 300.00 | 300.00 || WARD | 500.00 | 0 | 500.00 | 500.00 || JOENS | NULL | 1 | NULL | 100.00 || MARTIN | 1400.00 | 0 | 1400.00 | 1400.00 || BLAKE | NULL | 1 | NULL | 100.00 || CLARK | NULL | 1 | NULL | 100.00 || SCOTT | NULL | 1 | NULL | 100.00 || TURNER | 0.00 | 0 | NULL | 100.00 || ADAMS | NULL | 1 | NULL | 100.00 || JAMES | NULL | 1 | NULL | 100.00 || FORD | NULL | 1 | NULL | 100.00 || MILLER | NULL | 1 | NULL | 100.00 |+--------+---------+--------------+-----------------+------------------------------+ 参考资料： control flow functions Comparison Functions and Operators]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>isnull</tag>
        <tag>ifnull</tag>
        <tag>nullif</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过值列表选择pandas DataFrame中的行]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F07%2F%E9%80%9A%E8%BF%87%E5%80%BC%E5%88%97%E8%A1%A8%E9%80%89%E6%8B%A9pandas-DataFrame%E4%B8%AD%E7%9A%84%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：Use a list of values to select rows from a pandas dataframe [duplicate] 如下： 123456789101112import pandas as pddf = pd.DataFrame(&#123;'A' : [5,6,3,4], 'B' : [1,2,3, 5]&#125;)df""" A B0 5 11 6 22 3 33 4 5""" 现在根据列表[3,6]从df中选择出对应的行，可以使用isin： 1234567df1 = df[df['A'].isin([3,6])]df1""" A B1 6 22 3 3""" 如果是根据特定的值来筛选行记录，则是： 123456df2 = d[d['A']==3]df2""" A B2 3 3"""]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>select</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[保存DataFrame到csv文件]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F06%2F%E4%BF%9D%E5%AD%98DataFrame%E5%88%B0csv%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：Pandas writing dataframe to csv file，如题，将dataframe写入到csv文件可以使用to_csv方法。 123df.to_csv( [&apos;path_or_buf=None&apos;, &quot;sep=&apos;,&apos;&quot;, &quot;na_rep=&apos;&apos;&quot;, &apos;float_format=None&apos;, &apos;columns=None&apos;, &apos;header=True&apos;, &apos;index=True&apos;, &apos;index_label=None&apos;, &quot;mode=&apos;w&apos;&quot;, &apos;encoding=None&apos;, &apos;compression=None&apos;, &apos;quoting=None&apos;, &apos;quotechar=\&apos;&quot;\&apos;&apos;, &quot;line_terminator=&apos;\\n&apos;&quot;, &apos;chunksize=None&apos;, &apos;tupleize_cols=None&apos;, &apos;date_format=None&apos;, &apos;doublequote=True&apos;, &apos;escapechar=None&apos;, &quot;decimal=&apos;.&apos;&quot;],) 解释几个常用的参数，详细信息查看官方文档： path_or_buf：路径，string类型。df保存为csv文件的路径，相对路径或者绝对路径。 sep：分隔符，默认是“，”，可以自由设置。 na_rep：空值替换，默认为空字符串。 float_format：float类型数字的格式设置。如float_format=”%.2f”。 columns：序列。选择要写入的列。 header：默认为True（Series默认为False），可以是string和list。列名。 index：默认True，即行名。 mode：默认为w，即写模式。 encoding：编码方式。 chunksize：一次写入行数。 例子如下： 1234567891011121314151617181920import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'A': 'foo bar foo bar foo bar foo foo'.split(), 'B': 'one one two one two two one one'.split(), 'C': np.arange(8), 'D': np.arange(8) * 2&#125;)df""" A B C D0 foo one 0 01 bar one 1 22 foo two 2 43 bar one 3 64 foo two 4 85 bar two 5 106 foo one 6 127 foo one 7 14"""# 直接保存df.to_csv('out.csv', sep='\t', encoding='utf-8', index=False)]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[删除DataFrame中某列值为NaN的记录/行]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F05%2F%E5%88%A0%E9%99%A4DataFrame%E4%B8%AD%E6%9F%90%E5%88%97%E5%80%BC%E4%B8%BANaN%E7%9A%84%E8%AE%B0%E5%BD%95-%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：How to drop row of Pandas DataFrame whose value in certain columns is NaN 如题。我们先创建数据 1234567891011import pandas as pdimport numpy as npinput_rows = [[1,2,3], [2,3,4], [np.nan, 2, np.nan, 5], [np.nan, 5, 7]]df = pd.DataFrame(input_rows, columns=['a', 'b', 'c', 'd'])out: a b c d 0 1.0 2 3.0 NaN 1 2.0 3 4.0 NaN 2 NaN 2 NaN 5.0 3 NaN 5 7.0 NaN 1）DataFrame.dropna 12# 删除所有带有NaN的行df.dropna() 关于dropna(axis=0, how=&#39;any&#39;, thresh=None, subset=None, inplace=False)参数的说明： axis：默认是0，即删除行。1或者columns则是删除列 how：删除方式。any删除至少有一个NaN的行/列；all删除全部都是NaN的行/列 thresh：阈值。int，删除的行/列至少有n个NaN值 subset：列表。columns或者index，只删除指定列/行 2）pandas.notnull 1df = df[pd.notnull(df['a'])] 3）pandas.isnull 1df = df[~pd.isnull(df['d'])] 4）numpy.isnan 1df = df[~np.isnan(df['a'])] 5）query 1df = df.query('a == a')]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>drop</tag>
        <tag>删除</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改变pandas中列的数据类型]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F05%2F%E6%94%B9%E5%8F%98pandas%E4%B8%AD%E5%88%97%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：change data type of columns in pandas，如何将列的数据转为合适的类型，如将下例中数字从string类型转为float。 1234567891011121314151617181920import pandas as pda = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]df = pd.DataFrame(a, columns=['aa', 'bb', 'cc'])# df''' aa bb cc0 a 1.2 4.21 b 70 0.032 x 5 0'''df.dtypes'''aa objectbb objectcc objectdtype: object''' 1）pd.to_numeric方法 12# 不改变原本的数据df['bb'] = pd.to_numeric(df['bb']) pd.to_numeric()方法不能对DataFrame格式的数据直接转换，所以如果有多个column需要转换，可以使用apply方法。 12# apply返回处理之后的结果，并不改变原本的数据，所以需要赋值df[['bb', 'cc']] = df[['bb', 'cc']].apply(pd.to_numeric) pd.to_numeric(arg, errors=&#39;raise&#39;, downcast=None)的参数说明： arg：list、tuple、一维数组、Series errors：{‘ignore’, ‘raise’, ‘coerce’}，默认是raise，有错误直接抛出。ignore将返回不能被转换的原始数据；coerce将不能被转换为数字类型的数据设置为NaN downcast：{‘integer’, ‘signed’, ‘unsigned’, ‘float’} , 默认是none。如果设置了某一类型的数据，那么pandas会将原始数据转为能存储的最小子型态。如float的子型态有float16，float32，float64，所以设置了downcast=float，则会将数据转为能够以较少bytes去存储一个浮点数的float16。另外，downcast参数和errors参数是分开的，如果downcast过程中出错，即使errors设置为ignore也会抛出异常。 关于downcast参数用法，例子如下： 123456789# 这个例子是用concat方法将处理之后float类型数据块和非float类型的数据块合并# 也就是只对data中float类型的数据进行处理data = pd.concat( [ data.select_dtypes( include=['float'] ) .apply( pd.to_numeric, downcast='float' ), # apply中接收downcast参数传递给to_numeric data.select_dtypes( exclude=['float'] ) ], axis=1 ) 2）astype()方法 astype是pandas对象的方法，可以将DataFrame和Series转为指定的类型。 1234567891011# 以下例子不以上面数据为例# 将全部的列都转为intdf = df.astype(int)# 将不同类型的列用字典分别设置df = df.astype(&#123;'a': int, 'b': complex&#125;)# 转换Series的类型s = s.astype(np.float16)# 转换Series为python string类型s = s.astype(str) # 转换Series为序列类型s = s.astype('category') astype也会转换出错：s.astype(np.unit8) 3）infer_objects()方法 该方法不接受任何参数，尝试将object类型的数据转为更合适的格式。不能被转换的数据则保持原样。 123456789df = df.infer_objects()# 只成功修改了bb列df.dtypes'''aa objectbb float64cc objectdtype: object''']]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>数据类型</tag>
        <tag>data type</tag>
        <tag>astype</tag>
        <tag>to_numeric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[添加一行数据到DataFrame]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F04%2F%E6%B7%BB%E5%8A%A0%E4%B8%80%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%B0DataFrame%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：Add one row to pandas DataFrame，要求一行一行地添加数据到已有的空白DataFrame中。 12345import pandas as pdimport numpy as np# 创建空白DataFramedf = pd.DataFrame(columns=['lib', 'qty1', 'qty2']) 1）使用loc 123for i in range(4): df.loc[i] = [np.random.randint(-1, 1) for n in range(3)] # df.loc[i] = 5 添加一条数据都为5的记录 2）使用append 12345df.append(&#123;'lib': 2, 'qty1': 3, 'qty2': 4&#125;, ignore_index=True) # append也可以直接添加DataFramedf2 = pd.DataFrame([[1,2,3], [2,3,4]], columns=['lib', 'qty1', 'qty2'])df.append(df2, ignore_index=True) # ignore_index设置为True，index将会忽略df2的index 3）重新生成DataFrame 循环将要添加的数据以字典的形式保存到一个列表中，在用列表创建出DataFrame 123456row_list = [] input_rows = [[1,2,3], [2,3,4]] # 待插入数据for row in input_rows: dict1 = dict(lib=row[0], qty1=row[1], qty2=row[2]) # 将数据转为字典 row_list.append(dict1) # 保存到列表中df = pd.DataFrame(row_list)]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>添加行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何改变DataFrame列的顺序]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F03%2F%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98DataFrame%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：How to change the orders of DataFrame columns 还是用之前的数据： 12345import pandas as pddf = pd.DataFrame(&#123;'A': 'foo bar foo bar foo bar foo foo'.split(), 'B': 'one one two three two two one three'.split(), 'C': np.arange(8), 'D': np.arange(8) * 2&#125;) 解决思路很简单，就是改变原本DataFrame的列名的位置即可。 1）直接用改变列名位置的列表 1df1 = df[['C', 'A', 'B', 'D']] 2）使用columns方法，用列名下标来代替 12new_columns = [3, 2, 1, 0]df2 = df[df.columns[new_columns]] 3）使用insert方法 题主新增了一列数据，希望能够放到第一列的位置 1df.insert(0, 'new_column_name', df['D'].mean()) 4）使用reindex 1234df['new_column'] = df['D'].mean()df4 = df.reindex(['new_column'] + list(df.columns[:-1]), axis=1) # axis='columns'，如果没有axis参数，则必须在列表前加上columnsdf5 = df.reindex(columns=['new_column'] + list(df.columns[:-1]))]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>改变顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取DataFrame的行数]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F03%2F%E8%8E%B7%E5%8F%96DataFrame%E7%9A%84%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：How do I get a row count of a Pandas DataFrame 如下： 12345678910import pandas as pddf = pd.DataFrame(&#123;'A': 'foo bar foo bar foo bar foo foo'.split(), 'B': 'one one two three two two one three'.split(), 'C': np.arange(8), 'D': np.arange(8) * 2&#125;)# 方法一：使用shaperow_count, column_count = df.shape# 方法二：使用len函数row_count = len(df)]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>行数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择DataFrame中的多列数据]]></title>
    <url>%2Famberwest.github.io%2F2019%2F03%2F03%2F%E9%80%89%E6%8B%A9DataFrame%E4%B8%AD%E7%9A%84%E5%A4%9A%E5%88%97%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：selecting multiple columns in pandas dataframe，如题，选择DataFrame中的多列数据。 假设原始数据如下： 123456import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'A': 'foo bar foo bar foo bar foo foo'.split(), 'B': 'one one two three two two one three'.split(), 'C': np.arange(8), 'D': np.arange(8) * 2&#125;) 1）传递包含列名的列表 注：要有[] 1df1 = df[['A','C']] 2）使用iloc方法 注：索引是从0开始，并且不包括最大值 1df2 = df.iloc[:, 0:2] # 取前两列数据 3）使用loc方法 12df3 = df.loc[:, 'A':'C'] #从A列到C列，总共取三列数据df3 = df.loc[0:4, ['A', 'B', 'C']] # 也可以直接写出所要的列名，注意要加上[]；0:4是取前四列，因为没有重新设置行索引，所以看起来像是直接用的索引，其实这里是索引的名字 4）使用columns 12df4 = df[df.columns[2:4]] # 取第3和4列rows = df.loc[df.index[:3]] # 类似的，使用index可以取整行数据，效果跟df[:3]一样]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>筛选</tag>
        <tag>column</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在已有的DataFrame上增加新列]]></title>
    <url>%2Famberwest.github.io%2F2019%2F02%2F26%2F%E5%9C%A8%E5%B7%B2%E6%9C%89%E7%9A%84DataFrame%E4%B8%8A%E5%A2%9E%E5%8A%A0%E6%96%B0%E5%88%97%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：adding new column to existing DataFrame in Python pandas，添加新的列到原有数据中，我们可以分几种情况来看。 假设原始数据如下： 123456import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'num_legs': [4, 2], 'num_wings': [0, 2]&#125;, index=['dog', 'hawk'])slen = len(df['num_legs']) 1）直接赋值 123456789df['a'] = pd.Series(np.random.randn(slen), index=df.index) # index要记得添加df['b'] = None # 添加一列值为Nonedf['c'] = [2, 4] # 添加列表数据# c1和c3列的顺序是一样的， c2则与之相反，具体看下文df['c1'] = ['no', 'yes']df.index = [1, 0]df['c2'] = pd.Series(['no', 'yes'])df['c3'] = pd.Series(['no', 'yes'], index=df.index) 2）loc方法 12df.loc[:,'d'] = pd.Series(np.random.randn(slen), index=df.index)df.loc[:, 'd'] = [2, 4] 3）insert方法 insert方法使用的列名不能有重复值，连更新都不能 12df.insert(len(df.columns), 'e', pd.Series(np.random.randn(slen)), index=df.index)df.insert(len(df.columns), 'ee', [1,2]) 4）assign方法 assign方法参数可以是Series、标量、列表，还可以同时添加多列 12df = df.assign(f=df.num_wings.mean()) # 将num_wings这列的平均值作为新增列f的结果df = df.assign(A=df.num_wings.sum(), B=[1,2]) # 新增列A和B 5）concat方法 1pd.concat([df, pd.Series(['yes', 'yes']).rename('t')], axis=1) # 增加列t 注意点： 每个方法的参数都可以是Series、标量、列表 insert方法中新增的列名不能跟已有的一样，即使更新刚刚新增的列也会出错 df[&#39;a&#39;]=pd.Series([&#39;no&#39;, &#39;yes&#39;]的index顺序如果被修改，默认是以Series的index为准，可以通过index=df.index来指定按照原始DataFrame的index顺序]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>column</tag>
        <tag>assign</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取DataFrame列名的列表形式]]></title>
    <url>%2Famberwest.github.io%2F2019%2F02%2F26%2F%E8%8E%B7%E5%8F%96DataFrame%E5%88%97%E5%90%8D%E7%9A%84%E5%88%97%E8%A1%A8%E5%BD%A2%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：get list from DataFrame column headers，具体方法有如下几种： 12345678910111213141516171819202122import pandas as pddf = pd.DataFrame(&#123;'num_legs': [4, 2], 'num_wings': [0, 2]&#125;, index=['dog', 'hawk']) # 1list(df)# 2list(df.columns.values)# 3 df.columns.values.tolist()# 4df.keys.tolist()# 5[c for c in df]# 6list(df.columns)]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>column</tag>
        <tag>tolist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[删除DataFrame中的列]]></title>
    <url>%2Famberwest.github.io%2F2019%2F02%2F24%2F%E5%88%A0%E9%99%A4DataFrame%E4%B8%AD%E7%9A%84%E5%88%97%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原题目：Delete column from Pandas，题主用了del df[&#39;name&#39;]可以成功删除数据，但是del df.name却不能够，根据答案，这是因为python语法的限制，而且在pandas会引起混乱（column名字跟pandas已有属性相同的情况下），前者在python中相当于调用了df.__delitem__(name)。 我们可以用drop方法来实现。 123456import pandas as pddf = pd.DataFrame(&#123;'num_legs': [4, 2], 'num_wings': [0, 2]&#125;, index=['dog', 'hawk'])new_df = df.drop(labels='num_legs', axis='columns') 关于drop方法参数的说明： 1df.drop([&apos;labels=None&apos;, &apos;axis=0&apos;, &apos;index=None&apos;, &apos;columns=None&apos;, &apos;level=None&apos;, &apos;inplace=False&apos;, &quot;errors=&apos;raise&apos;&quot;]) lables：所要删除的行/列，可以是字符串，也可以是列表，依axis而定 axis：指定要删除的方向。0或index删除的是行，1或columns删除列 index：制定要删除行的索引 columns：指定要删除的列，有这个参数不需要再设置axis level：多重索引下使用]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>drop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何迭代DataFrame中的行]]></title>
    <url>%2Famberwest.github.io%2F2019%2F02%2F24%2F%E5%A6%82%E4%BD%95%E8%BF%AD%E4%BB%A3DataFrame%E4%B8%AD%E7%9A%84%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：How to iterate over the columns in DataFrame in Pandas 如题所示，就是对DataFrame的行进行迭代，可以用DataFrame.iterrows()或DataFrame.itertuples()。 12345678910111213141516import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'num_legs': [4, 2], 'num_wings': [0, 2]&#125;, index=['dog', 'hawk'])# row是Seriesfor index, row in df.iterrows(): print(row) print(row['num_legs'])# index=False则index不作为tuple的第一个元素返回for row in df.itertuples(index=False): print(row) print(getattr(row, 'num_legs')) # row[0] 或 row.num_legs print(np.asarray(row)) # 将nametupled转为列表 类似的，对DataFrame的列进行迭代则可以用DataFrame.iteritems()方法。三者的区别如下： DataFrame.iteritems()不接受任何参数，返回结果是元组(column name, Series) DataFrame.iterrows()也不接受任何参数，返回结果也是元组(index, Series) DataFrame.itertuples(index=True, name=&#39;Pandas&#39;)以nametupled的形式迭代返回DataFrame的行。效率比DataFrame.iterrows()快一些]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>迭代</tag>
        <tag>iterrows</tag>
        <tag>iteritems</tag>
        <tag>itertuples</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pandas筛选出指定列值所对应的行]]></title>
    <url>%2Famberwest.github.io%2F2019%2F02%2F23%2F%E4%BD%BF%E7%94%A8pandas%E7%AD%9B%E9%80%89%E5%87%BA%E6%8C%87%E5%AE%9A%E5%88%97%E5%80%BC%E6%89%80%E5%AF%B9%E5%BA%94%E7%9A%84%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：Select rows from a DataFrame based on values in a column in pandas。根据问题描述，希望通过pandas实现类似mysql查找语句的功能： 1select * from table where column_name = some_value; pandas中获取数据的有以下几种方法： 布尔索引 位置索引 标签索引 使用API 假设数据如下: 123456import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'A': 'foo bar foo bar foo bar foo foo'.split(), 'B': 'one one two three two two one three'.split(), 'C': np.arange(8), 'D': np.arange(8) * 2&#125;) 布尔索引该方法其实就是找出每一行中符合条件的真值(true value)，如找出列A中所有值等于foo 1df[df['A'] == 'foo'] # 判断等式是否成立 位置索引使用iloc方法，根据索引的位置来查找数据的。这个例子需要先找出符合条件的行所在位置 123456mask = df['A'] == 'foo'pos = np.flatnonzero(mask) # 返回的是array([0, 2, 4, 6, 7])df.iloc[pos]#常见的iloc用法df.iloc[:3,1:3] 标签索引如何DataFrame的行列都是有标签的，那么使用loc方法就非常合适了。 12345678df.set_index('A', append=True, drop=False).xs('foo', level=1) # xs方法适用于多重索引DataFrame的数据筛选# 更直观点的做法df.index=df['A'] # 将A列作为DataFrame的行索引df.loc['foo', :]# 使用布尔df.loc[df['A']=='foo'] 关于loc和iloc的区别，官网是这么说的： DataFrame.loc Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc Purely integer-location based indexing for selection by position. 使用APIpd.DataFrame.query方法在数据量大的时候，效率比常规的方法更高效。 1234df.query('A=="foo"')# 多条件df.query('A=="foo" | A=="bar"') 数据提取不止前面提到的情况，第一个答案就给出了以下几种常见情况： 1、筛选出列值等于标量的行，用== 1df.loc[df['column_name'] == some_value] 2、筛选出列值属于某个范围内的行，用isin 1df.loc[df['column_name'].isin(some_values)] # some_values是可迭代对象 3、多种条件限制时使用&amp;，&amp;的优先级高于&gt;=或&lt;=，所以要注意括号的使用 1df.loc[(df['column_name'] &gt;= A) &amp; (df['column_name'] &lt;= B)] 4、筛选出列值不等于某个/些值的行 123df.loc[df['column_name'] != 'some_value']df.loc[~df['column_name'].isin('some_values')] #~取反]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>筛选</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用pandas对列重命名]]></title>
    <url>%2Famberwest.github.io%2F2019%2F02%2F21%2F%E5%A6%82%E4%BD%95%E7%94%A8pandas%E5%AF%B9%E5%88%97%E9%87%8D%E5%90%8D%2F</url>
    <content type="text"><![CDATA[这是系列文章，我会按照stackoverflow上pandas相关问题投票数排序进行整理学习。不学习是会变咸鱼的～ 原问题：renaming columns in pandas，根据问题描述，需要对已有的DataFrame的列重新命名，去掉前面的$符号。 123import pandas as pddf = pd.DataFrame([&#123;'$a':1, '$b':2, '$c':3&#125;]) 方法一，不适用于有很多列的或者只修改个别列名的DataFrame 1df.columns = ['a', 'b', 'c'] # 直接在原始数据上进行修改 方法二，rename方法适用于只修改个别列名或者列名有一定规律的DataFrame 1234567df2=df.rename(columns=&#123;'$a':'a', '$b':'b'&#125;) # 不修改原始数据df.rename(columns=&#123;'$a':'a', '$b':'b'&#125;, inplace=True) # 修改原始数据df.rename(&#123;'$a':'a', '$b':'b'&#125;, axis='columns', inplace=True) # pandas 0.21的新方法，指定axis='coloumns'或者1# 列名有规律，方法可以有很多df.rename(columns=lambda x: x[1:], inplace=True)df.rename(columns=lambda x: x.lstrip('$'), inplace=Tru) 方法三，str方法 1df.columns = df.columns.str.replace('$','')]]></content>
      <categories>
        <category>今天你学pandas了吗</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>重命名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何去除pandas groupby每个分组最大最小值再求描述性统计]]></title>
    <url>%2Famberwest.github.io%2F2019%2F01%2F18%2F%E5%A6%82%E4%BD%95%E5%8E%BB%E9%99%A4pandas-groupby%E6%AF%8F%E4%B8%AA%E5%88%86%E7%BB%84%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E5%80%BC%E5%86%8D%E6%B1%82%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[如题，之前在处理数据的时候，一直想不到好的方法。有人建议将数据导入mysql再做处理，嗯，导入之后，发现我很少用mysql，也不是很清楚要怎么做😢最后直接上stackoverflow发问。这真是个良心网站啊～ 原始数据如下： 123456789101112data = &#123;'class': ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b'], 'num': [-10,18,12,15,50, 10,60,51,54,100]&#125;df = pd.DataFrame(data)df.groupby('class').describe()output：num count mean std min 25% 50% 75% maxclass a 5.0 17.0 21.494185 -10.0 12.0 15.0 18.0 50.0b 5.0 55.0 31.984371 10.0 51.0 54.0 60.0 100.0 做法1: 12mask = df['num'].ne(df['max'])&amp;df['num'].ne(df['min']) # DataFrame.ne() not equal吧，反正不符合条件的返回True，&amp;是并集，即返回同时不等于最大和最小值的True/Falsedf[mask].groupby('class')['num'].describe() # 这里简化了，df[mask]得到的就是符合条件的DataFrame 做法2： 12345df.groupby('class').apply(lambda x: x.drop([x['num'].idxmax(),x['num'].idxmin()])).rename_axis([None,None]).groupby('class').describe() # 原答案df.groupby('class').apply(lambda x: x.drop([x['num'].idxmax(), x['num'].idxmin()])['num']).groupby('class').describe() # 跟原答案不太一样df.groupby('class').apply(lambda x: x.drop([x['num'].idxmax(), x['num'].idxmin()])['num']).reset_index(level=0).groupby('class')['num'].describe() # 其实drop之后数据变成了以class为index，并且原本的rangeIndex还在 当时知道idxmax()方法，就是不知道要怎么跟groupby结合起来，处理之后的数据格式又会变成什么样？又要做何处理？说到底还是对pandas不熟。 更详细的做法可以参考原始答案]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>groupby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决matplotlib中文乱码]]></title>
    <url>%2Famberwest.github.io%2F2019%2F01%2F18%2F%E8%A7%A3%E5%86%B3matplotlib%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[关于matplotlib中文显示的问题，最常见的解决方法就是： 12plt.rcParams['font.sans-serif']=['SimHei']plt.rcParams['axes.unicode_minus']=False 但是每次都要写这两句话也是挺麻烦的。最惨的是，如果使用pyenv进行版本管理，上面那两句话也不管用了，因为不同版本的python下可能没有字体文件。所以，我参考知乎上matplotlib图例中文乱码? - 在不在的回答 - 知乎的做法，解决了这个问题。 下载字体文件 点击进入下载黑体文件 查看所使用的版本 123&gt;&gt;pyenv activate test(test是所创建的虚拟环境)&gt;&gt;which python/Users/test/.pyenv/shims/python（在这个路径下找到matplotlib对应的字体文件夹） 将字体文件放在matplotlib/mpl-data/fonts/ttf/下 1mv ~/Downloads/SimHei.ttf /Users/amber/.pyenv/versions/amber/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/ 修改matplotlib/mpl-data下的matplotlibrc文件 在命令中输入vim matplotlibrc进入文件，在当前模式（命令行模式），使用:/font.family进行搜索，快速定位。 123font.family : sans-serif # 取消注释font.sans-serif : SimHei, DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif # 添加SimHei字体axes.unicode_minus : False #将True修改为False，正确显示负号 在python脚本中重新加载配置文件 运行以下脚本就可以了 12from matplotlib.font_manager import _rebuild_rebuild() 我没有在字体册中启用黑体好像也没什么问题。哎，全部改用pyenv管理python版本之后就一直解决不了显示中文的问题，但是一直都没想过是这个问题。嗯，以此为戒，遇到问题要全面思考。]]></content>
      <categories>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>中文乱码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib：图像元素]]></title>
    <url>%2Famberwest.github.io%2F2018%2F12%2F12%2Fmatplotlib%EF%BC%9A%E5%9B%BE%E5%83%8F%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[标准颜色缩写 b： 蓝色 g： 绿 r： 红 c： 青 m： 品红 y： 黄 k： 黑 w： 白 标准样式字符 实线样式 – 短划线样式 -. 点实线样式 : 虚线样式 . 点标记 , 像素标记 o 圆标记 v 向下三角形标记 ^ 向上三角形标记 &lt; 向左三角形标记 &gt; 向右三角形标记 1 Tri_down标记 2 Tri_up标记 3 Tri_left标记 4 Tri_right标记 s 方形标记 p 五边形标记 * 星号 h 六角形标记1 H 六角形标记2 + 加号 x X标记 D 菱形标记 d 细菱形标记 | 垂直标记 12345678910111213141516import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltnp.random.seed(1000)y = np.random.standard_normal(20)x = range(len(y))plt.figure(figsize=(7,4))plt.plot(y.cumsum(), 'b', lw=1.5)plt.plot(y.cumsum(), 'ms')plt.grid(True)plt.axis('tight')plt.xlabel('index')plt.ylabel('value')plt.title('A simple plot') 图例loc参数plt.legend(*args,**kwargs)有很多参数，常用的就是loc参数，可以是数字，也可以是英文，还可以直接用表示位置的元组。如：plt.legend(loc=&#39;best&#39;) or plt.legend(loc=(0.5, 0.8)) 空白 自动 0 最佳 best 1 右上 upper right 2 左上 upper left 3 左下 lower left 4 右下 lower right 5 右 right 6 中左 center left 7 中右 center right 8 中下 lower center 9 中上 upper center 10 中 center 例子中除了常用的loc参数，还设置了其他的样式，具体的使用指南可以参考官网 123456789101112131415161718import matplotlib.pyplot as pltimport numpy as npax = plt.subplot(111)t1 = np.arange(0.0, 1.0, 0.01)for n in [1, 2, 3, 4]: plt.plot(t1, t1**n, label="n=%d"%(n,)) # 设置每个plot对应的图例leg = plt.legend(loc='best', ncol=2, # 两行 mode="expand", # 水平填充整个Axes区域，默认为none。如果bbox_to_anchor设置了legend的size则会覆盖mode shadow=True, # 设置legend框的阴影 fancybox=True # 设置为False变化不大 )leg.get_frame().set_alpha(0.5) # 获取legend框并设置透明度plt.show() text方法给plot添加文本可以使用plt.text()方法，完整参数查看Text类 matplotlib.text.Text(x=0, y=0, text=’’, color=None, verticalalignment=’baseline’, horizontalalignment=’left’, multialignment=None, fontproperties=None, rotation=None, linespacing=None, rotation_mode=None, usetex=None, wrap=False, *\kwargs*) 常用参数： x，y：是文本s放置位置 s：要添加的文本信息 fontdict：设置文本样式，使用该参数是dict形式fontdict={&#39;fontsize&#39;: 12}，也可以直接用关键字参数如：fontsize=12 12345678910111213141516171819202122232425262728293031323334353637383940import matplotlib.pyplot as pltfig = plt.figure()# 这里是suptitle，而不是subtitlefig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')# 给Figure添加Axes对象ax = fig.add_subplot(111)# 调整ax的位置，参数有left=None, bottom=None, right=None, top=None,wspace=None, hspace=Nonefig.subplots_adjust(top=0.85) ax.set_title('axes title')ax.set_xlabel('xlabel')ax.set_ylabel('ylabel')ax.text(3, 8, 'boxed italics text in data coords', style='italic', # style有三种选择，'normal', 'italic', 'oblique' bbox=&#123;'facecolor':'red', 'alpha':0.5, 'pad':10&#125; # 给文本加上边框，并设置样式 )ax.text(2, 6, r'an equation: $E=mc^2$', # 记得带上r fontdict=&#123;'fontsize': 15&#125; # 也可以直接用fontsize=15 )# unicode也能识别ax.text(3, 2, 'unicode: Institut f\374r Festk\366rperphysik')ax.text(0.95, 0.01, 'colored text in axes coords', verticalalignment='bottom', horizontalalignment='right', transform=ax.transAxes, color='green', fontsize=15)ax.plot([2], [1], 'o') # 默认颜色为bax.annotate('annotate', xy=(2, 1), xytext=(3, 4), arrowprops=dict(facecolor='black', shrink=0.05))ax.axis([0, 10, 0, 10]) # 设置x轴和y轴的范围[xmin, xmax, ymin, ymax]plt.show() 对于数学表达式，可以用LaTeX语法直接编辑 1234567891011121314import numpy as npimport matplotlib.pyplot as pltt = np.arange(0.0, 2.0, 0.01)s = np.sin(2*np.pi*t)plt.plot(t,s)plt.title(r'$\alpha_i &gt; \beta_i$', fontsize=20)plt.text(1, -0.6, r'$\sum_&#123;i=0&#125;^\infty x_i$', fontsize=20)plt.text(0.6, 0.6, r'$\mathcal&#123;A&#125;\mathrm&#123;sin&#125;(2 \omega t)$', fontsize=20)plt.text(0.3, 0.98, r'$local\ max$', fontsize=12, horizontalalignment='left', verticalalignment='center', )plt.xlabel('time (s)')plt.ylabel('volts (mV)')plt.show() scales直接看例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.ticker import NullFormatter # useful for `logit` scale# Fixing random state for reproducibilitynp.random.seed(19680801)# make up some data in the interval ]0, 1[y = np.random.normal(loc=0.5, scale=0.4, size=1000)y = y[(y &gt; 0) &amp; (y &lt; 1)]y.sort()x = np.arange(len(y))# plot with various axes scalesplt.figure(1)# linearplt.subplot(221)plt.plot(x, y)plt.yscale('linear')plt.title('linear')plt.grid(True)# logplt.subplot(222)plt.plot(x, y)plt.yscale('log')plt.title('log')plt.grid(True)# symmetric logplt.subplot(223)plt.plot(x, y - y.mean())plt.yscale('symlog', linthreshy=0.01)plt.title('symlog')plt.grid(True)# logitplt.subplot(224)plt.plot(x, y)plt.yscale('logit')plt.title('logit')plt.grid(True)# Format the minor tick labels of the y-axis into empty strings with# `NullFormatter`, to avoid cumbering the axis with too many labels.plt.gca().yaxis.set_minor_formatter(NullFormatter())# Adjust the subplot layout, because the logit one may take more space# than usual, due to y-tick labels like "1 - 10^&#123;-3&#125;"plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.90, # 调整子图的布局 hspace=0.35, wspace=0.35 # 设置子图之间的距离 )plt.show()]]></content>
      <categories>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib, text,</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib：boxplot箱线图]]></title>
    <url>%2Famberwest.github.io%2F2018%2F12%2F12%2Fmatplotlib%EF%BC%9Aboxplot%E7%AE%B1%E5%BD%A2%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[箱线图一般用来看数据的分布，比如四分位数，异常点等。简单的数据信息也可以用DataFrame.describe()进行查看。 boxplot参数看下箱线图的相关参数： Axes.boxplot(x, notch=None, sym=None, vert=None, whis=None, positions=None, widths=None, patch_artist=None, bootstrap=None, usermedians=None, conf_intervals=None, meanline=None, showmeans=None, showcaps=None, showbox=None, showfliers=None, boxprops=None, labels=None, flierprops=None, medianprops=None, meanprops=None, capprops=None, whiskerprops=None, manage_xticks=True, autorange=False, zorder=None, **, data=None*) 具体参数： x ： 数组或者向量序列，指定要绘制箱线图的数据 notch：是否是凹口箱线图，默认为否。凹口表示中值附近的置信区间。bootstrap有关如何计算槽口位置的信息 sym：指定异常点的形状，默认是+号显示。不想显示，传入空字符串(‘’) vert：是否将框垂直显示，默认为True whis：指定上下须跟上下四分位的距离，默认是1.5倍四分位差 bootstrap：是否为凹口位置的中位数设置置信区间 usermedians：设置每个元素的中位数，默认由matplotlib计算 positions：设置框的位置 widths：设置每个框的宽度，标量或者序列，默认0.5 patch_artist：默认是用二维线性绘制（Line2D artist） labels：序列，长度与x一样，为每个数据集设置标签 manage_xticks：调整xlim和xtick的位置，默认True autorange：是否用线的形式来表示均值，默认是点 zorder：设置箱线图的zorder showcaps：在箱线图两端显示那两条线，默认True showbox：是否显示中间的框，默认True showfliers：是否显示异常值，默认True showmeans：是否显示均值，默认False capprops：指定两端线的样式，dict boxprops：指定框的样式，dict whiskerprops：指定须的样式，dict medianprops：设置中值的样式，dict，None meanprops：设置均值样式，dict，None boxplot例子123456789101112131415161718192021222324import numpy as npimport matplotlib.pyplot as plt# Fixing random state for reproducibilitynp.random.seed(19680801)# fake up some dataspread = np.random.rand(50) * 100center = np.ones(25) * 50flier_high = np.random.rand(10) * 100 + 100flier_low = np.random.rand(10) * -100# 将多个array按指定axis（默认为0）合并data = np.concatenate((spread, center, flier_high, flier_low))fig1, ax1 = plt.subplots()ax1.set_title('Box Plot')ax1.boxplot(data, showmeans=True, # 显示均值 showfliers=True, # 显示异常点 flierprops=dict(markerfacecolor='g', marker='D'), # 异常值样式 notch=True, # 默认95%的置信区间，在中值附近 patch_artist=True, #使用自定义颜色填充盒形图，一定要先设置才可以定义颜色 boxprops = &#123;'color':'black','facecolor':'lightgrey'&#125;, ) 更多关于props参数设置查看官方文档 水平箱线图 12345fig5, ax5 = plt.subplots()ax5.set_title('Horizontal Boxes')ax5.boxplot(data, vert=False, # 水平显示 flierprops=dict(markerfacecolor='r', marker='s'), # 设置异常值 ) 获取并显示异常值 1234567891011121314def show_flier(ax, x, y): for i in range(len(x)): ax.annotate('(%s, %.1f)' % (x[i], y[i]), (x[i],y[i]), xytext=(-0.2+x[i], 0.1+y[i]))fig4, ax4 = plt.subplots()ax4.set_title('Hide Outlier Points')f = ax4.boxplot(data[2], showfliers=True, # 一定要设置为True，不然不会显示异常值 whis=0.75)for i in f['fliers']: show_flier(ax4, i.get_xdata(), i.get_ydata()) # print(i.get_xdata(), i.get_ydata()) # print(i.get_xydata()) 直接返回二维数组]]></content>
      <categories>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib, boxplot, 箱线图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib：annotate标注]]></title>
    <url>%2Famberwest.github.io%2F2018%2F12%2F12%2Fmatplotlib%EF%BC%9Aannotate%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[给图片添加注释或者标出某个数值，需要用到annotate。Figure对象(plt.annotate)和Axes对象(ax.annotate)都有annotate()方法，参数也是一样的。 annotate参数先看下Axes.ax.annotate(*args, **kwargs)的参数，也就是matplotlib.axes.Axes.annotate： s：str，注释文本内容 xy：元组，被标注的坐标点 xytext：元组，可选。注释文本s的放置位置，默认是xy xycoords：str，Artist，Transform，可调用对象或者元组，xy的坐标系。参数如下： figure points：在figure左下方的点 figure pixels：在figure左下方的像素 figure fraction：figure左下角的数字部分 axes points：在axes左下角的点 axes pixels：在axes左下角的像素 axes fraction：在axes左下角的数字部分 data：默认选项，使用被标注对象的坐标系 polar：极坐标系，(theta,r) if not native ‘data’ coordinates textcoords：str，Artist，Transform，可调用对象或者元组，xytext给定的坐标系，可能跟xy的坐标系不同。参数如下： offset points：距离xy值的偏移量（点） offset pixels：距离xy值的偏移量（像素） 默认跟xycoords的参数一致 arrowprops：dict，绘制文本xytext和坐标点xy之间的箭头。不包含arrowstyle参数。其他参数如下 width：点的箭头宽度 headlength：箭头底部的宽度，以磅为单位 shrink：小数，从两端收缩到总长度的一部分 如果含有arrowstyle参数，那么上面的参数都失效。具体查看官方文档，还有各种例子 annotation_clip：控制注释在Axes区域外是否可见。True，只当点xy在Axes范围内才显示注释文本，False则一直都可见。默认为None，当xycoords=”data”时，相当于True annotate基本例子来自官方例子 1234567891011121314151617import numpy as npimport matplotlib.pyplot as pltfig, ax = plt.subplots()t = np.arange(0.0, 5.0, 0.01)s = np.cos(2*np.pi*t)line = ax.plot(t, s, lw=2) # 线# 文本内容是local max，标注的点是xy=(2,1)，文本放置的位置是(3,1.5),shrink是箭头到文本的距离所占总长度的比例# 换成0.5，会看到箭头只有从点(2,1)开始的那前半节，facecolor是箭头填充色ax.annotate('local max', xy=(2, 1), xytext=(3, 1.5), arrowprops=dict(facecolor='black', shrink=0.05), )ax.set_ylim(-2, 2)plt.show() Annotate Transform另一个比较复杂的例子，链接样式(connection style)可以参考官网 12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport matplotlib.pyplot as pltx = np.arange(0, 10, 0.005)y = np.exp(-x/2.) * np.sin(2*np.pi*x)fig, ax = plt.subplots()ax.plot(x, y)ax.set_xlim(0, 10)ax.set_ylim(-1, 1)xdata, ydata = 5, 0xdisplay, ydisplay = ax.transData.transform_point((xdata, ydata))print(xdisplay, ydisplay)# 注释文本的外框和底色bbox = dict(boxstyle="round", fc="0.8")# 设置箭头样式arrowprops = dict( arrowstyle = "-&gt;", connectionstyle = "angle,angleA=0,angleB=90,rad=10")offset = 72# (xdata, ydata)是注释的位置，xytext是文本的位置，都是元组ax.annotate('data = (%.1f, %.1f)'%(xdata, ydata), (xdata, ydata), xytext=(-2*offset, offset), textcoords='offset points', bbox=bbox, arrowprops=arrowprops)disp = ax.annotate('display = (%.1f, %.1f)'%(xdisplay, ydisplay), (xdisplay, ydisplay), xytext=(0.5*offset, -offset), xycoords='figure pixels', textcoords='offset points', bbox=bbox, arrowprops=arrowprops)plt.show()]]></content>
      <categories>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>annotate</tag>
        <tag>标注</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】matplotlib：使用matplotlib进行python绘图（指南）]]></title>
    <url>%2Famberwest.github.io%2F2018%2F12%2F11%2Fmatplotlib%EF%BC%9A%E4%BD%BF%E7%94%A8matplotlib%E8%BF%9B%E8%A1%8Cpython%E7%BB%98%E5%9B%BE%EF%BC%88%E6%8C%87%E5%8D%97%EF%BC%89%2F</url>
    <content type="text"><![CDATA[在python中画图肯定少不了matplotlib库，几行代码就可以将画出基本图来，但是大部分时候还是要去查阅文档或者去stackover flow查找，因为这个库真的太庞大了。最近刚好看到一篇文章，对matplotlib的一些概念解释的很好，也有对应的例子，所以根据文章做个简化版翻译，更详细的内容可以查看原文。 matplotlib对象的层次结果使用过matplotlib，应该都用过这行代码plt.plot([1,2,3])。这一行隐藏了一个事实：一个plot实际上是嵌套python对象的层次结果，也就是说每个图下面都有一个matplotlib对象的树形结构。 如下图所示，一个Figure对象是matplotlib图像最外层的一个容器，可以包含一个或多个Axes对象(实际绘图的盒状容器)。容易引起混淆的一个原因：Axes实际上是一个独立的plot或者graph，而不是我们所认为的axis/轴的复数。Axes对象之下就是图表的“元素”，例如：刻度线、线条、图例、文本框、刻度、标签等，都是Axes对象可操作的python对象。 文中代码都是在jupyter notebook中运行的，最后的注释内容是输出结果，因为懒得截图了。。。 通过例子来重新认识matplotlib吧： 12345678import matplotlib.pyplot as pltimport pandas as pdimport numpy as npfig, _ = plt.subplots()type(fig)# matplotlib.figure.Figure 例子中从plt.subplots()中创建了两个变量，一个fig是顶级的Figure对象，另一个是暂时不需要用到的“一次性”变量，用下划线表示。使用属性表示法，可以轻松遍历层级结构并查看第一个Axes对象的y轴的第一个刻度 1234one_tick = fig.axes[0].yaxis.get_major_ticks()[0]one_tick# &lt;matplotlib.axis.YTick at 0x7f0b569acac8&gt; 例子中，fig（一个Figure类实例）有多个Axes（一个列表，这里取第一个）。每个Axes都有一个yaxis和xaxis，每个都有一个“major ticks”的集合，例子中取第一个。 各个部分可以参考下图： 上图的代码实现 stateful和stateless在进一步了解matplotlib之前，需要明白stateful(基于状态，状态机)和无状态(面向对象，OO)接口之间的区别。 几乎所有来自pyplot的函数，如plt.plot()，要么隐式地指向现有的Figure对象和Axes对象，要么不存在时直接创建新的Figure和Axes对象。matplotlib文档是这样说的： 【使用pyplot】简单的函数是用来为当前Figure对象的Axes图像添加元素的（线条，图片，文本等）【强调添加】 “plt.plot（）是一个隐式跟踪当前数字的状态机界面！”在英语中，这意味着： 有状态接口使用plt.plot（）和其他顶级pyplot函数进行调用。在给定的时间内只有一个图或轴可以操作，不需要明确地引用它。 直接修改底层对象是面向对象的方法。我们通常通过调用Axes对象的方法来做到这一点，Axes对象是表示绘图本身的对象。 整个过程的流程就像下图所示： 用代码来表示上述流程： 12345678# plt.plot()获取当前轴的一个流程表示，缩简版def plot(*args, **kwargs): ax = plt.gca() return ax.plot(*args, **kwargs)def gca(**kwargs): """获取当前Figure中的Axes对象""" return plt.gcf().gca(**kwargs) 调用plt.plot()只是获取当前Figure的当前Axes对象，这也就是有状态接口总是“隐式跟踪”它想要引用的图。 pyplot是一系列函数的集合，这些函数实际上只是matplotlib面向对象接口的包装器。例如，plt.title()在面对对象方法中有相应的setter和getter方法：ax.set_title()和ax.get_title()。调用plt.title()等同于gca().set_title(s, *args, **kwargs)，这行代码包含了以下操作： gca()获取并返回当前Axes set_title()是一个setter方法，用于设置该Axes对象的标题。这里我们就不需要明确指定任何对象的plt.title() 类似地，其他顶级函数，如plt.grid()，plt.legend()和plt.ylabels()，都是跟plt.title()一样的操作，先是获取当前Axes，gca()调用当前Axes的一些方法。 理解plt.subplots()接下来主要是依赖于无状态/面向对象方法，这种方法可定制性更高，图形变得更加复杂更派得上用场。在面向对象使用单个Axes创建对象规定是使用plt.subplots()。这是面向对象使用pyplot创建Figure和Axes的唯一时间。 12345# 没有传递任何参数，默认是调用subplots(nrows=1, ncols=1)，所以返回一个Figure和一个Axesfig, ax = plt.subplots()type(ax)# matplotlib.axes._subplots.AxesSubplot 我们可以用ax的实例方法来操作绘图，接下来用三个时间序列的堆积面积图来说明 1234567891011rng = np.arange(50)rnd = np.random.randint(0, 10, size=(3, rng.size))yrs = 1950 + rngfig, ax = plt.subplots(figsize=(5,3))ax.stackplot(yrs, rng+rnd, labels=['Eastasia', 'Eurasia', 'Oceania'])ax.set_title('Combined debt growth over time')ax.legend(loc='upper left')ax.set_ylabel('Total debt')ax.set_xlim(xmin=yrs[0], xmax=yrs[-1])fig.tight_layout() # tight_layout()作为一个整体应用于图形对象来清理空白填充 如果是一个Figure中包含多个Axes/子图呢？下面用离散均匀分布绘制两个相关数组，跟上一个例子的不同点可以查看代码里的注释 12345678910111213141516x = np.random.randint(low=1, high=11, size=50)y = x + np.random.randint(1, 5, size=x.size)data = np.column_stack((x, y)) # column_stack将多个一维array作为column转为多维数组# 1.创建一个Figure和两个Axes，也可以用fig, axs = plt.subplots(1, 2, figsize=(8,4))# 2.分别处理两个子图fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))ax1.scatter(x, y, marker='o', c='r', edgecolor='b') # edgecolor设置marker边缘颜色ax1.set_xlabel('$x$') # 加了$符号，x，y会变成斜体ax1.set_ylabel('$y$') # label定义两组数据对应的标签，DataFrame一般不需要，因为有columnsax2.hist(data, bins=np.arange(data.min(), data.max()), label=('x', 'y')) ax2.legend(loc=(0.7, 0.8))ax2.set_title('Frequencies of $x$ and $y$')ax2.yaxis.tick_right() # 将ax2的y轴tick设置在右边 需要注意的是，多个Axes都是包含在给定的Figure中的。如上面的例子，fig.axes能获取到所有的Axes对象列表 12(fig.axes[0] is ax1, fig.axes[1] is ax2) # 这里的fig.axes中的axes是小写# (True, True) 我们还可以创建一个包含2*2Axes对象的网格图形Figure 1234567891011121314fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(7, 7))type(ax) # 可以看到，ax是numpy的数组类型# numpy.ndarrayax""" array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0b541b29b0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0b547d2710&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0b548efc88&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0b5451e240&gt;]], dtype=object)"""ax.shape# (2, 2) ax是数组类型，需要使用flatten函数将数组平展为一维的才能使用 123ax1, ax2, ax3, ax4 = ax.flatten()# or((ax1, ax2), (ax3, ax4)) = ax 为了展示更多的用法，接下来使用加利福尼亚州的房价数据 12345678910111213141516171819202122232425262728293031# 获取数据。原文是通过代码下载解压数据的housing = np.loadtxt('CaliforniaHousing/cal_housing.data', delimiter=',')y = housing[:, -1]pop, age = housing[:, [4, 7]].T # pop地区人口，age是房子的平均屋龄def add_titlebox(ax, text): """在Axes中添加文本""" ax.text(.55, .8, text, horizontalalignment='center', transform=ax.transAxes, bbox=dict(facecolor='white', alpha=0.6), fontsize=12.5) return ax# 创建带有多个Axes的Figure网格gridsize = (3, 2)fig = plt.figure(figsize=(12, 8))ax1 = plt.subplot2grid(gridsize, (0, 0), colspan=2, rowspan=2) # ax1占用两行两列ax2 = plt.subplot2grid(gridsize, (2, 0)) # ax2在第三行第一列ax3 = plt.subplot2grid(gridsize, (2, 1)) # ax3在第三行第二列# ax1是散点图，并带有colormap，其他两个图为直方图（查看数据分布）ax1.set_title('Home value as a function of home age and area population', fontsize=14)# c是color，y的序列映射到cmap，这样y的值与cmap的颜色可以一一对应上，cmap选择颜色变化，跟参数c结合使用sctr = ax1.scatter(x=age, y=pop, c=y, cmap='RdYlGn') plt.colorbar(sctr, ax=ax1, format='$%d')ax2.hist(age, bins='auto') # bins可以是整数，序列或者‘auto’ax3.hist(pop, bins='auto', log=True) # log标签显示为log形式add_titlebox(ax2, 'Histogram: home age')add_titlebox(ax3, 'Histogram: area population (log scl.)') 跟color map不一样的是，colorbar()是由Figure直接调用，而不是Axes对象，并且第一个参数是散点图的返回结果，它的作用就是将y值映射到color map。在y轴上，房价并没有太大的变化(颜色)，x轴上则由明显变化，说明屋龄是房屋价值的一个更大的决定因素 屏幕后面的“Figures”每次调用plt.subplots()或者plt.figure()(只创建Figure，没有Axes)，都会在内存里新建一个Figure对象。在终端运行的结果跟原文一样，如果是在jupyter notebook则每一次返回id都不同。 12345678910&gt;&gt;&gt; fig1, ax1 = plt.subplots()&gt;&gt;&gt; id(fig1)4525375272&gt;&gt;&gt; id(plt.gcf()) # fig1是当前Figure对象4525375272&gt;&gt;&gt; fig2, ax2 = plt.subplots()&gt;&gt;&gt; id(fig2) == id(plt.gcf()) # 当前Figure对象已经更改为fig2True&gt;&gt;&gt; plt.get_fignums()[1, 2] 可以通过plt.figure()获取到存在内存中的Figure 12345def get_all_figures(): return [plt.figure(i) for i in plt.get_fignums()]get_all_figures()# [&lt;Figure size 640x480 with 1 Axes&gt;, &lt;Figure size 640x480 with 1 Axes&gt;] 使用plt.close()方法则可以关闭对应的figure 12plt.close(num) # 关闭序号为num的Figureplt.close('all') # 关闭全部 imshow和matshow除了plt.plot()可以绘图之后，还有其他的方法，如：imshow()和matshow()。后者是前者的封装。只要原始数值数组可以显示为彩色网格，就可以使用。 例子： 123456789101112131415161718192021222324252627# 创建两个数组x = np.diag(np.arange(2, 12))[::-1]x[np.diag_indices_from(x[::-1])] = np.arange(2, 12)x2 = np.arange(x.size).reshape(x.shape)# 使用字典参数，将所有的轴标签和刻度都不显示sides = ('left', 'right', 'top', 'bottom')nolabels = &#123;s: False for s in sides&#125;nolabels.update(&#123;'label%s' % s: False for s in sides&#125;)from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable# 使用上下文管理器来禁用网格，并在每个Axes上使用matshow()with plt.rc_context(rc=&#123;'axes.grid': False&#125;): fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4)) ax1.matshow(x) img2 = ax2.matshow(x2, cmap='RdYlGn_r') # 加上_r就是将颜色反转 for ax in (ax1, ax2): ax.tick_params(axis='both', which='both', **nolabels) for i, j in zip(*x.nonzero()): ax1.text(j, i, x[i,j], color='white', ha='center', va='center') # 将colorbar作为图中的新轴 divider = make_axes_locatable(ax2) cax = divider.append_axes('right', size='5%', pad=0) plt.colorbar(img2, cax=cax, ax=[ax1, ax2]) fig.suptitle('Heatmaps with `Axes.matshow`', fontsize=16) pandas中的绘图pandas的DataFrame和Series上的plot()是plt.plot()的封装，所以在pandas绘图也是很方便的。如果DataFrame的索引是mdate，pandas调用gcf().autofmt_xdate()来获取当前的图并自动格式化x轴。plt.plot()是基于状态的方法，也就是隐式的知道当前的Figure和Axes，pandas也是遵循这点进行扩展的。 12345678s = pd.Series(np.arange(5), index=list('abcde'))ax = s.plot()type(ax)# matplotlib.axes._subplots.AxesSubplotid(plt.gca()) == id(ax)# True 接下来用一个时间序列的例子来看一下pandas中的画图 123456789101112131415161718192021222324import matplotlib.transforms as mtransformsvix = pd.read_csv('VIXCLS.csv', index_col=0, parse_dates=True, na_values='.', infer_datetime_format=True, squeeze=True).dropna()ma = vix.rolling('90d').mean()state = pd.cut(ma, bins=[-np.inf, 14, 18, 24, np.inf], labels=range(4))cmap = plt.get_cmap('RdYlGn_r')ma.plot(color='black', linewidth=1.5, marker='', figsize=(8, 4), label='VIX 90d MA')ax = plt.gca()ax.set_xlabel('')ax.set_ylabel('90d moving average: CBOE VIX')ax.set_title('Volatility Regime State')ax.grid(False)ax.set_xlim(xmin=ma.index[0], xmax=ma.index[-1])trans = mtransforms.blended_transform_factory(ax.transData, ax.transAxes)for i, color in enumerate(cmap([0.2, 0.4, 0.6, 0.8])): ax.fill_between(ma.index, 0, 1, where=state==i, facecolor=color, transform=trans)ax.axhline(vix.mean(), linestyle='dashed', color='xkcd:dark grey', alpha=0.6, label='Full-period mean', marker='')ax.legend(loc='upper center') 上面的例子中有一些点需要了解一下： ma是VIX指数的90天移动平均线，衡量近期股市波动的市场预期。state是将移动平均线分类为不同的state。高VIX被表示市场中恐惧程度加剧。 cmap是color map，matplotlib对象。它实质上是浮点数到RGBA颜色的映射。’_r是颜色反转。 pandas调用ma.plot()，也就是调用了plt.plot()。因此为了继承面向对象，需要通过ax=plt.gca()来显示引用当前的Axes对象。 最后一段代码是创建了与每个状态对应的颜色填充块。cmap([0.2, 0.4, 0.6, 0.8])是指沿着color map的光谱，在20%，40%，60%，80%的位置获取RGBA序列。使用enumerate()将每个RGBA颜色跟state一一对应。 原文最后还给出了很多不错的链接推荐，值得学习。]]></content>
      <categories>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib, 画图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用pandas获取前k个数据]]></title>
    <url>%2Famberwest.github.io%2F2018%2F11%2F29%2Fpandas%EF%BC%9A%E8%8E%B7%E5%8F%96%E5%89%8Dk%E4%B8%AA%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[最近在看SQL，什么每科成绩前三名，看得我好懵。pandas其实也有个nlargest方法可以简单实现这一需求 nlargest有三个参数，df.nlargest(n, columns, keep=&#39;first&#39;)，返回columns前n项的数据，默认是降序。 数据： 12345678910import pandas as pdraw_df = &#123; 'subject': ['chinese', 'chinese','chinese','chinese','english','english', 'english', 'english','math','math', 'math', 'math'], 'grade': ['first', 'first', 'second', 'second', 'first', 'first', 'second', 'second', 'first', 'first', 'second', 'second'], 'teacher': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'c', 'c', 'c', 'c'], 'student': [10, 12, 14, 23,10, 12, 14, 23,10, 12, 14, 23], 'avg_score': [56, 56, 45, 76, 77, 85, 68, 79, 73, 76, 70, 85]&#125;df = pd.DataFrame(raw_df)df 获取平均分最高的前三项： 因为nlargest函数没有逆序，如果要取平均分最低的三项，可以借助辅助项，axis=1是在列方向上删除多余的项 如果想要获得每个科目的最高分，直接用groupby，然后再取最大值即可。groupby中有个as_index=True设置为False，那么分组的列就作为普通的列，而不是index。 如果想要取得每个科目的平均分的前2名，再分组之后可以使用apply方法将nlargest应用到每个小组 如果还想在此基础上计算其他的，比如求和啊，求平均值啊，只要继续修改一下就可以 关于groupby之后的情况 groupby的相关参数 1Signature: df.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, observed=False, **kwargs)]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>nlargest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用pyinstall打包python项目为exe文件及其问题总结]]></title>
    <url>%2Famberwest.github.io%2F2018%2F11%2F12%2F%E7%94%A8pyinstall%E6%89%93%E5%8C%85python%E9%A1%B9%E7%9B%AE%E4%B8%BAexe%E6%96%87%E4%BB%B6%E5%8F%8A%E5%85%B6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[为了可以在没有安装python的环境下运行程序，我们需要将程序打包为可执行文件。这次用的pyinstall模块，操作方便（虽然前面也踩了好多坑），坑的一点就是，想要在什么平台下使用，就需要在对应的平台上进行打包。比如说这次吧，因为要在Windows系统下使用，我只能临时搭个Windows虚拟机进行打包。 安装安装很简单，直接用pip命令 1pip install pyinstall 打包不管是简单还是复杂的项目，只需要对入口文件（启动整个项目的py文件）进行打包即可。 app目录下只有一个文件p.py，内容如下 1print('test pyinstaller') 进入app根目录，打包 1pyinstaller p.py 运行命令之后，看到以下输出就说明成功了 123456…4583 INFO: Building EXE from EXE-00.toc completed successfully.4585 INFO: checking COLLECT4586 INFO: Building COLLECT because COLLECT-00.toc is non existent4586 INFO: Building COLLECT COLLECT-00.toc5412 INFO: Building COLLECT COLLECT-00.toc completed successfully. 这时候app目录下多了两个文件夹和spec文件，其中spec文件是最先生成的，pyinstaller再根据这个文件进行打包。所以对于比较复杂的项目，可以先生成spec，修改里面对应的配置，比如添加其他需要的多媒体文件等 双击dist下的p可执行文件就可以正常运行程序了 在打包程序的时候可以指定参数onefile，这样打包出来的dist下就只有一个可执行文件，不过运行速度可能会稍微慢一点 1pyinstaller --onefile p.py 运行可执行文件时会出现终端窗口，如果不希望看到，可以使用w参数 1pyinstall --onefile -w p.py 其他更详细的使用和配置，查看官方文档 可能遇到的问题 OSError: Python library not found: Python, libpython3.5m.dylib, libpython3.5.dylib, .Python 我是在pyenv创建的环境下安装使用pyinstall模块的，之所以会出现这个问题是因为python在安装pyenv时没有执行--enabled-shared。重新安装即可 12345678# 检查echo $&#123;PYENV_ROOT&#125;/versions/3.5.3/Python.framework# 更新env PYTHON_CONFIGURE_OPTS="--enable-shared" pyenv install 3.5.0# 重新打包pyinstaller --onefile p.py AttributeError: ‘str’ object has no attribute ‘items’ 关了pycharm编辑器就不报错了。。。 windows下路径问题 因为项目是在unix下开发的，测试时在Windows下进行的，当时也没报错，就是找不到运行之后保存的数据文件。后面想了想，可能还是路径的问题。所以将代码需要用到的路径的代码都改了成类似E:/data这样的路径。果然就可以了。需要注意的是，Windows下的路径一般是E:\data，但是\是转义符，所以要么写成上面那样，要么进行反转义，比如：E:\\data 参考using spec files pyenv issue Python 3.5 - missing libpython3.5m.so (shared library)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pyinstall</tag>
        <tag>exe</tag>
        <tag>打包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb shell与mongodb server版本不一致引起的问题]]></title>
    <url>%2Famberwest.github.io%2F2018%2F11%2F06%2Fmongodb-shell%E4%B8%8Emongodb-server%E7%89%88%E6%9C%AC%E4%B8%8D%E4%B8%80%E8%87%B4%E5%BC%95%E8%B5%B7%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[最近从服务器的mongodb导出数据时遇到问题Unrecgonized field &#39;snapshot&#39;，一开始以为是我把要提取的字段名给写错了，前前后后对了好几次，还是有错。具体错误提示看下图： 错误原因其实这就是因为本地的mongodb shell版本跟服务器mongodb server版本不一致造成的。一直都没有注意到这个问题，是因为每次远程连接mongo，都能够正常进行查询、删除操作。从图片上也可以看到，版本不同，而且也有warning。 解决方法降低版本对于这个情况，网上大部分都是通过降低其中一个的版本，再换成对应版本的。不过过程有点坎坷，换了本地的mongo shell，连带mongo也要更新，麻烦，而且在处理的过程中，也不是那么顺利。。。 想通过降低mongodb server的版本，结果需要验证 验证之后还是不行 就算验证过了，还要重新安装配置一遍，想想都心累。 python脚本虽然直接连接拿不到数据，但是可以用pymongo去连接数据库，读出数据保存到excel中。 123456789101112131415161718192021from pymongo import MongoClientimport pandas as pd# 假设数据库、账号、密码都是houseuri = "mongodb://house:house@193.111.222.33:27017/?authSource=house"client = MongoClient(uri)db = client['house']table = db['house’]data = table.find(&#123;&#125;, &#123;'_id': 0&#125;)df = pd.DataFrame()data_list = []for i, item in enumerate(data): data_list.append(item)# for key in item.keys():# print(item)# value = str(item[key]) if isinstance(item[key], list) else item[key]# df.loc[i, key] = item[key]df = pd.DataFrame(data_list)df.to_excel('house.xlsx', header=True, startrow=0, index=False) 远程传输懒得写代码可以登录登录服务器，在服务器上先导出数据，然后再用scp传输到本地 12345# 远程服务器 ssh username:ipmongoexport --username house --password house --db house --collection house --type=csv --fields url,address,area,city,district,house_disposal,house_type,pay_type,pics,phone,rent,rent_type,title --out ~/house.csv# 在本地远程连接并下载scp username@193.111.222.33:/home/amber/house.csv ~/Desktop/house.csv 参考文章mongoexport downgrade]]></content>
      <tags>
        <tag>mongodb</tag>
        <tag>版本不一致</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用docker管理jupyter]]></title>
    <url>%2Famberwest.github.io%2F2018%2F11%2F06%2F%E4%BD%BF%E7%94%A8docker%E7%AE%A1%E7%90%86jupyter%2F</url>
    <content type="text"><![CDATA[使用docker来管理jupyter，可以摆脱各种烦人的环境依赖，比如tabula需要Java环境，我本地已有的Java跟tabula又对应不上，比较麻烦。 使用也是非常方便，只有两点要求： 选择合适的jupyter notebook 选择启动docker镜像的路径/文件夹，也是用来存放jpynb文件的 接下来分几步走： 选择notebook pyspark-notebook是带有Java环境的，所以这次用的是这个 配置文件 启动docker jupyter的命令比较长，需要指定notebook、端口等信息，所以就直接写入配置文件docker-compose.yml，方便启动。 如果启动失败，可能是端口被占用，可以尝试其他端口，比如&quot;9999:8888&quot;。其中存放的路径跟本地选择路径的并没有影响。docker预装的是Linux系统，用户jovyan下的work目录就相当于你启动docker的目录下。 123456789101112version: '3'services: notebook: image: jupyter/pyspark-notebook volumes: - $PWD:/home/jovyan/work ports: - "8888:8888" environment: - JUPYTER_LAB_ENABLE=yes command: "start.sh jupyter lab" 启动 12cd Notebook # 存放配置文件和jpynbdocker-compose up 关闭jupyter 1docker-compose down 在浏览器打开 根据日志信息，在浏览器打开链接就可以了 至此，就可以像往常一样使用jupyter了。随时随地，环境崩了，直接删了重新拉个镜像就可以。 需要注意的：配置文件不能是隐藏文件，名字也不能改动 参考文章： jupyter-docker-stacks JupyterLab]]></content>
  </entry>
  <entry>
    <title><![CDATA[Jupyterlab不能调用asyncio event loop]]></title>
    <url>%2Famberwest.github.io%2F2018%2F11%2F06%2FJupyterlab%E4%B8%8D%E8%83%BD%E8%B0%83%E7%94%A8asyncio-event-loop%2F</url>
    <content type="text"><![CDATA[最近在Jupyterlab上使用asyncio时，一直报错，说是event loop已经在运行了，但我之前根本就没有启动。上网着了一遍，才知道是因为tornado的版本过高导致的。因为使用Jupyter Docker Stacks启动的Jupyterlab默认是最新版本的，python也是3.6，所有需要将tornado的版本稍微降低一下就可以了。 安装低版本的tornado 1pip install tornado==4.5.3 重启kernel 也就是重新启动jupyter notebook 参考文章： Can’t invoke asyncio event_loop after tornado 5.0 update]]></content>
      <tags>
        <tag>docker</tag>
        <tag>jupyter</tag>
        <tag>loop</tag>
        <tag>asyncio</tag>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas.to_datetime的format参数不起作用]]></title>
    <url>%2Famberwest.github.io%2F2018%2F11%2F05%2Fpandas-to-datetime%E7%9A%84format%E5%8F%82%E6%95%B0%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[之前用pandas处理日期之后，想要保存结果到excel，发现不管怎么设置，日期这一列数据总是带有time部分。 在pyCharm或者Jupyter Notebook打印出来的结果都是正常的，但实际上并不是这样的： 具体原因我也没找到，好在有方法可以解决。 使用Series的dt方法 1234df['日期'] = pd.to_datetime(df['日期'], format='%Y-%m-%d').dt.date# 或者df['日期'] = df['日期'].dt.strftime('%Y-%m-%d') 使用日期的date方法 123df['日期'] = pd.to_datetime(df['日期'], format='%Y-%m-%d')df['日期'] = [time.date() for time in df['日期']]# 获取时间部分则是用time()]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[celery多队列]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F25%2Fcelery%E5%A4%9A%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[celeryCelery是基于分布式消息传递的异步任务队列/作业队列。它支持实时操作，也支持调度。 celery工作流程：生产者通过celery发送任务到broker（Redis/RabbitMQ），worker从broker接收任务，完成任务。 1user --&gt; broker --&gt; worker1, worker2, worker3, ... celery默认只有一个队列，我们可以根据实际情况使用多个队列，比如，有些任务不多，但是耗时长，像下载图片之类的，我们可以单独设置一个队列，RabbitMQ还可以设置任务的优先级，这次用的是Redis，所以暂时不考虑这一块。 最终结果文件列表如下： 123456/multi_queue __init__.py app.py celeryconfig.py tasks.py dispatcher.py 安装1pip install celery 配置celery为了方便管理，我将celery所需要的配置都写在同一个文件中celeryconfig.py，现在已经可以不用大写的了，具体每个字段的意思看代码中的注释。更多的设置可以查看官方文档。 123456789101112131415161718192021222324252627282930313233343536373839# 设置broker中间商，存放消息broker_url = 'redis://root:password@localhost:6379/4'# 存放消息状态和结果result_backend = 'redis://root:password@localhost:6379/4'# Celery启动时，会从imports指定的路径导入任务，如果有多个任务，继续添加imports = ['tasks']# 链接丢失或者错误的情况下，重试发布消息队列时定义默认策略，跟task_publish_retry类似task_publish_retry_policy = &#123; 'max_retries': 3, 'interval_start': 0, 'interval_step': 0.2, 'interval_max': 0.2&#125;# 队列task_queues = &#123; 'add': &#123; 'routing_key': 'add' &#125;, 'download': &#123; 'exchange': 'media', 'routing_key': 'download' &#125;&#125;# 路由，这里设置了各个task对应的队列，其中的queue和routing_key要跟task_queues对task_routes = &#123; 'tasks.add': &#123; 'queue': 'add', 'routing_key': 'add', 'serializer': 'json', &#125;, 'tasks.download_img': &#123; 'queue': 'download', 'routing_key': 'download' &#125;&#125; 创建celery实例在app.py文件中创建celery实例，并加载配置。 123456from celery import Celery# 名字可以随意app = Celery('multi')# 从celeryconfig文件中导入配置，如果不在同一级目录，则这样写：proj.path.to.celeryconfigapp.config_from_object('celeryconfig') 一般比较小的任务，可以直接在同一个文件中进行设置，上面两个文件可以整合成这样👇： 12345678910111213141516from celery import Celery# 名字可以随意app = Celery('multi', broker='redis://root:password@localhost:6379/4', backend='redis://root:password@localhost:6379/4', include=['tasks'], # 跟配置文件中的imports一样 )# 根据实际情况进行配置app.conf.update( CELERY_TIMEZONE='Asia/Shanghai', CELERY_ENABLE_UTC=True, CELERY_ACCEPT_CONTENT=['json'], CELERY_TASK_SERIALIZER='json', CELERY_RESULT_SERIALIZER='json',) 创建任务tasks.py文件中，定义了add和download_img两个任务，使用app.task将任务装饰成task。 1234567891011121314151617# tasks.pyimport requestsfrom app import app@app.taskdef add(a, b): print('add a and b: ', a+b) return a + b@app.taskdef download_img(url): print(url) response = requests.get(url) with open('img.jpg', 'wb') as f: for chunk in response.iter_content(1024): f.write(chunk) 启动celery因为文件中我都是直接使用文件名，所以启动命令时需要先进入/multi_queue路径下，不然有些文件会找不到。启动任务时需要指定对应的队列，比如启动add队列： 12cd ~/../multi_queuecelery worker -A tasks -c=5 -l info -Q add 参数说明： worker：运行worker模块 -A App：--app APP，指定使用的celery实例，这里是tasks.py，如果imports没有设置，可能要用path.to.tasks这样的方式指定 -Q：指定队列，如果有多个队列，用逗号隔开，hipri,lopri -c：-c CONCURRENCY, --concurrency CONCURRENCY，指定并发数，可以不用，默认是4 -l：-l LOGLEVEL, --loglevel LOGLEVEL，指定日志级别，非必须 关于celery worker更多的参数可以使用help查看：celery worker --help。 运行之后，可以看到这样的结果 调用任务启动celery之后，celery会一直处于监听状态，等待任务。任务的调用可以写成一个普通的函数，然后运行脚本，也可以在交互模式下异步调用。使用apply_async()和send_tasks()进行调用。 脚本运行： 任务脚本 1234567# dispatcher.pyfrom tasks import download_img, addif __name__ == '__main__': url = 'https://ss0.baidu.com/94o3dSag_xI4khGko9WTAnF6hhy/image/h%3D300/sign=cb94d0439e13b07ea2bd56083cd69113/35a85edf8db1cb1379326b71d054564e92584b7c.jpg' download_img.apply_async(args=(url, ), queue='download') add.apply_async(args=(3,4), queue='add') 终端启动，这里没有指定具体哪个队列，所以会启动已配置的所有队列 1celery -A tasks worker -l info 运行脚本 1python dispatcher.py 异步调用： 在终端进入python，调用任务： 123456# 启动python$ python&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; add.apply_async(args=(3,4), queue='add')&lt;AsyncResult: b914e4f8-f888-4fde-a8df-20f312213c0a&gt; 这时候可以看到celery已经接收到任务，并返回结果： 有一点需要注意的是，这里不能用delay来代替apply_async，在默认队列的时候，两个方法都可以用，如果启动celery指定队列，直接使用add.delay(2,3)，这个任务会一直处于PENDING状态，直到你不指定队列启动celery，任务才会被执行。 也不能使用add.delay(args=(3,4), queue=&#39;add&#39;)，会报错。 如果要启动多个worker，只需要在另一个终端窗口启动命令就可以了。 扩展阅读在网上了解到，有很多人遇到任务重复执行的问题，我只是简单测试了一下，自然没有遇到这个问题，不过呢，Celery4.0已经有解决方法了，详情请戳：celery-once。 需要在多个服务器启动celery，使用-n参数，具体查看命令：celery worker --help。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>celery</tag>
        <tag>多队列</tag>
        <tag>queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记八】排序（下）]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F18%2F%E6%8E%92%E5%BA%8F%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[归并排序什么是归并排序归并排序的核心思想就是：先将数组从中间分为前后两部分，分别排序之后，再将两部分合并在一起。 这里用到的思想是“分而治之”，将一个大问题分解为两个或多个相同或相似的子问题。小的问题解决了，大问题也就解决了。大问题的答案就是子问题的答案的合并。 这整个分治过程非常符合递归思想，所以我们可以用递归来实现。 步骤： 分解子问题 将问题拆分为相互独立但相似的子问题，如将数组不断分组 求解子问题 子问题容易解决就直接解决，否则继续拆分子问题，如当只有一个元素时返回原数组（有序） 合并子问题的解 将有序数组进行合并然后返回 12345678910111213141516171819202122232425262728293031323334353637383940# 归并排序：将数组分为两部分，分别排序之后再合并# 递归调用函数def merge_sort(a): """分治递归：a是数组""" n = len(a) if n &lt;= 1: return a middle = int(n / 2) # 将数组分为两部分 left = merge_sort(a[:middle]) right = merge_sort(a[middle:]) # 返回合并之后的有序数组 return merge(left, right) def merge(a1, a2): """合并两个有序数组""" # tmp临时数组，用于存放结果，这个是导致归并排序算法不是原地排序算法的原因 i, j, tmp =0, 0, [] while i &lt; len(a1) and j &lt; len(a2): if a1[i] &lt;= a2[j]: tmp.append(a1[i]) i += 1 elif a1[i] &gt;= a2[j]: tmp.append(a2[j]) j += 1 # 将剩余的有序数据直接合并在结果中 tmp.extend(a1[i:]) tmp.extend(a2[j:]) return tmpif __name__ == '__main__': # 测试合并数组 l1 = [1,3,7,9] l2 = [2,4,5,7] data = merge(l1, l2) print(data) b = [3,2,1,6,9,3,4,7] res = merge_sort(b) print(res) 归并排序的性能分析 归并排序是稳定的排序算法吗 是。合并两个有序子数组的过程，当遇到值相等的元素，我们可以将前半部分数组的元素插入到后半部分数组的元素前面，所以合并前后，元素的前后顺序不变。归并排序用到一个临时数组来存放结果，所以不是原地排序算法。 归并排序的时间复杂度是多少 这里涉及到递归代码的时间复杂度分析。 我们可以假设求解大小为n的原问题a的时间为T(n)，两个子问题的b、c的求解时间则是T(n/2)，合并时间为n(merge函数的时间复杂度为n)。每次拆分问题，子问题所需要时间都是上级问题时间的一半，所以有以下关系： 1234567T(n) = 2*T(n/2) + n = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n ...... = 2^k * T(n/2^k) + k * n ...... 根据上面的推算，得到T(n)=2^k * T(n/2^k) + k * n。当T(n/2^k)=T(1)时，也就是当原数组拆分到只有一个元素时解决子问题所需要的时间，这里不需要合并（也就是没有kn项，2^k则是子问题的个数），根据对数公式可以求的k=log2n，代入上面公式就可以得到T(n)=Cn+nlog2n，用大O标记法来表示，可以得到归并排序的时间复杂度为O(nlogn)。 因为归并排序跟数组的有序程度无关，所以不论是最好情况、最坏情况，还是平均情况，时间复杂度都为O(nlogn)。 归并排序的空间复杂度是多少 递归代码的空间复杂度不会像时间复杂度那样累加，而是在完成每一次合并操作之后就释放掉所占用的内存空间。在任何时刻，CPU只会有一个函数在执行，那么最大的临时内存空间就是数组的大小，所以空间复杂度为O(n)。 快速排序什么是快速排序快速排序的核心思想就是：在待排序数组中随机取一个数据作为区分点，小于该区分点的放在左边，大于该区分点的放在右边，区分点放在中间。根据分治、递归的思想，分别对左右两边的数据重复刚刚的操作（每次排序都有一个区分点被确定，不再参与下次排序），直到待排序区间为1。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 快速排序def quick_sort1(a): """排序""" left, right = [], [] if len(a) &lt;= 1: return a value = a[-1] for i in range(len(a) -1): if a[i] &lt; value: left.append(a[i]) else: right.append(a[i]) return quick_sort(left) + [value] + quick_sort(right)def quick_sort(lists, left, right): '''快速排序''' # 跳出递归判断 if left &gt;= right: return lists # 选择参考点，该调整范围的第1个值 key = lists[left] low = left high = right # 循环判断直到遍历全部 while left &lt; right: # 从右边开始查找大于参考点的值 while left &lt; right and lists[right] &gt;= key: right -= 1 lists[left] = lists[right] # 这个位置的值先挪到左边 # 从左边开始查找小于参考点的值 while left &lt; right and lists[left] &lt;= key: left += 1 lists[right] = lists[left] # 这个位置的值挪到右边 # 写回改成的值 lists[left] = key # 递归，并返回结果 quick_sort(lists, low, left - 1) # 递归左边部分 quick_sort(lists, left + 1, high) # 递归右边部分 return listsif __name__ == '__main__': b = [3,2,1,6,9,3,4,7] res = quick_sort(b) print(res) 快速排序的性能分析 快速排序是稳定的排序算法吗 快速排序是一种原地、不稳定的排序算法，看代码👆 快速排序的时间复杂度是多少 因为快排也是用递归实现的，所以可以使用上面提到的方式求解时间复杂度。如果每次选择的区分点都能将数组分为大小相近的两个小区间，那么最好情况时间复杂度就是O(nlogn)。 如果原来的数组已经有序了，每次取到的分区点都是最大（最小），那么得到的两个分区就非常不均等，最糟的情况要n次分区操作才能完成排序，这时候的最坏情况时间复杂度就变成了O(n^2)。 至于平均时间复杂度，除了可以用上面的递归公式，还可以使用递归树。总的来说，大部分情况下都能实现O(nlogn)，极端情况下才会退化到O(n^2)。 快速排序的空间复杂度是多少 原地排序算法，空间复杂度为O(1)。 应用问题：如何在O(n)时间复杂度内求无序数组A中的第K大元素，如4，2，5，12，3，第三大就是4。 解决：选择最后一个点作为分区点，将原数组分为三部分A[0…p-1]、A[p]、A[p+1…n-1]。如果p+1=k，那么A[p]就是答案。如果K&gt;p+1，那么就在右分区继续查找；如果K&lt;p+1，我们就在左分区继续查找。因为每次分区，分区点的位置都是确定的了，而下标是从0开始的，所以需要加1之后再做判断。（思路很像最小二乘法）每次都只需要对当前分区的一半数据进行分区，也就是n、n/2、n/4…1，总的时间和为2n-1，也就是O(n)。 思考现在你有 10 个接口访问日志文件，每个日志文件大小约 300MB，每个文件里的日志都是按照时间戳从小到大排序。你希望将这10个较小的日志文件，合并为1个日志文件，合并之后的日志仍然按照时间戳从小到大排序。如果处理上述排序任务的机器内存只有1GB，该如何处理 参考文章数据结构与算法之美]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>排序</tag>
        <tag>插入排序</tag>
        <tag>快速排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[browsercookie使用]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F16%2Fbrowsercookie%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[安装你可能需要提前安装一些依赖库 1pip install pkgconfig setuptools_scm pytest-runner google-apputils 安装browsercokie 1pip install browsercookie 使用首先，在浏览器打开目标网站，进入需要登录才能看到的页面，如GitHub profile页面。 1234567891011import browsercookieimport requestsfrom lxml import etreeurl = "https://github.com/settings/profile"cj = browsercookie.chrome()res = requests.get(url, cookies=cj)response = etree.HTML(res.text)heading = response.xpath('//*[@class="menu-heading"]/text()')print(heading) 在执行代码的时候可能会需要输入密码 返回结果如下： 1[&apos;\n Personal settings\n &apos;] 如果没有登录，直接请求profile页面，返回结果为空 123456url = "https://github.com/settings/profile"res = requests.get(url)response = etree.HTML(res.text)heading = response.xpath('//*[@class="menu-heading"]/text()')print(heading) 在抓取一些需要登录的网站信息的时候，就可以省去模拟登录，或者处理cookie，省下不少力气呀～ 参考文章browsercookie]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>cookie</tag>
        <tag>模块</tag>
        <tag>browsercookie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记七】排序（上）]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F15%2F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[如何分析一个“算法”排序算法的执行效率1）最好情况、最坏情况、平均情况时间复杂度 在分析排序算法的时候，要分别给出最好情况、最坏情况、平均情况下的时间复杂度，以及对应情况下要排序的原始数据是什么样的（有序或者无序）。 2）时间复杂度的系数、常数、低阶 实际开发中，需要排序的数据可能是10个、100个、1000个这种小规模的数据，所以需要将时间复杂度的系数、常数、低阶都考虑到。 3）比较次数和交换/移动次数 基于比较的排序算法在执行过程中，会涉及到两种操作，一是元素比较大小，二是元素交换/移动。因此需要将比较次数和交换/移动次数也考虑进去。 排序算法的内存消耗算法的内存消耗可以通过空间复杂度来衡量，排序算法也是。本文涉及到的三种排序算法都是空间复杂度为O(1)，也称为原地排序算法。原地排序算法是特指空间复杂度是O(1)的排序算法，也就是常量阶的时间复杂度。 排序算法的稳定性排序算法的稳定性是指，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。如果排序之后，原本的前后顺序发生变化，那么对应的排序算法就叫做不稳定的排序算法。 应用：我们要给电商平台的订单数据进行排序，按照金额从小到大对订单数据进行排序。如果金额相同，那么就按照下单时间来排序。这时候就可以用到稳定的排序算法，先将订单都下单时间排序，排完序之后再对金额相同的订单从小到大排序。因为使用的是稳定的排序算法，所以第二次排序之后，相同金额的订单仍然保持下单时间从早到晚。 有序度、无序度、满有序度一个长度为n的数组，有n！种排序方式，如果要计算排序算法的平均时间复杂度，涉及到的概率计算会非常复杂，所以使用“有序度”和“无序度”这两个概念来进行分析。这个方法并不严格，但是在很多时候很实用。 有序度是数组中具有有序关系的元素对的个数，用公式表示如下： 1有序元素对：如果i &lt; j，有a[i] &lt;= a[j] 如数组[2, 4, 3, 1, 5, 6]的有序度为11，因为有序元素对有以下11个： 1(2, 4), (2, 3), (4, 5), (4, 6), (2, 5), (2, 6), (3, 5), (3, 6), (1, 5), (1, 6), (5, 6) 对于一个倒序的数组来说，有序度就是0；如果是一个完全有序的数组，有序度是n(n-1)/2，也称为满有序度。那么，逆序度则与有序度的定义相反（默认从小到大为有序）。公式如下： 1逆序元素对：如果i &gt; j,有a[i] &lt; a[j] 根据有序度和无序度的定义，可以得到这么一个公式：逆序度=满有序度-有序度。 对元素进行排序，就是一个不断增加有序度、减少逆序度的过程，直到达到满有序度，排序完成。 冒泡排序（Bubble Sort）冒泡算法每次都只操作相邻的两个数据。如果不满足大小要求，就将其互换。 12345678910111213141516171819202122# 冒泡算法def bubble_sort(a): """a是数组，返回从小到大排序的数组""" n = len(a) # 只有一个元素就直接返回 if n &lt;= 1: return a # 循环，将每个元素与下一个元素进行比较 for i in range(n): flag = False for j in range(n-i-1): # 如果当前元素比下一个元素大，互换位置 if a[j] &gt; a[j+1]: a[j], a[j+1] = a[j+1], a[j] flag = True # 优化：没有数据交换，提前退出 if not flag: break return aa=[3,5,4,1,2,6,2,55,6,7,6,4]bubble_sort(a) 分析冒泡算法 冒泡算法是原地排序算法吗 因为只涉及到相邻数据的交换操作，需要的空间复杂度为O(1)，所以是原地排序算法。 冒泡算法是稳定的排序算法吗 冒泡排序中只有交换才会改变两个元素的前后顺序。当相邻两个元素相等时，我们不做交换，所以大小相等的元素在排序前后顺序不改变，是稳定的排序算法。 冒泡排序的时间复杂度是多少 最好情况下，要排序的数据已经是有序的了，只需要一次冒泡操作，所以最好情况时间复杂度为O(n)（遍历数组）。最坏情况则是，要排序的数据是倒序的，需要进行n次冒泡，所以最坏时间复杂度为O(n^2)。平均情况复杂度的计算，我们则可以根据有序度来计算。 最坏情况下，初始状态的有序度是0，要进行n(n-1)/2次交换。最好情况情况下，初始状态的有序度是n(n-1)/2，不需要进行交换。我们可以取两者的中间值作为平均情况下的时间复杂度，也就是n(n-1)/4，所以复杂度为O(n^2)。 插入排序(Insertion Sort)插入排序是在有序数组中插入新数据之后，仍然保持数据的有序性。所以需要遍历数组，找到数据应该插入的位置将其插入即可。 插入排序也只涉及到两种操作，一种是元素比较，另一种是元素的移动。 过程：我们需要将待排序数组分为已排序区和未排序区，一开始，已排序区只有一个元素，也就是数组的第一个元素。之后就不断在未排序区中取出元素，然后在已排序区遍历查找合适的位置插入新元素，同时将插入点之后的元素都顺序往后移动一位，并保证已排序区的数据一直有序。重复整个过程，直到未排序区的元素为空。 1234567891011121314151617# 插入排序def insert_sort(a): n = len(a) if n &lt;= 1: return for i in range(1, n): # 当前元素 value = a[i] for j in range(i, 0, -1): if a[j-1] &gt; value: a[j-1], a[j] = value, a[j-1] else: break return ab = [3,2,1,6,5,9,6,1]insert_sort(b) 分析插入算法 插入排序是原地排序算法吗 插入排序并不需要额外的储存空间，所以空间复杂度是O(1)，是原地排序算法。 插入排序是稳定的排序算法吗 对于值相同的元素，我们可以将后面出现的元素插入到前面出现元素的后面，这样就保持了原本的前后顺序，从代码实现中也可以看出来。所以是稳定的排序算法。 插入排序的时间复杂度是多少 最好情况下，待排序数组已经是有序的了，插入排序算法只是循环比较了一遍，并没有插入操作，所以最好情况时间复杂度为O(n)（遍历数组）。最坏情况下，待排序数组是倒序的，那么循环比较时，还需要移动数据，所以最坏情况时间复杂度为O(n^2)。插入排序类似于在数组中插入一个数据，只不过要循环执行n次插入操作，类比后者的平均时间复杂度是O(n)，插入排序算法的平均时间复杂度为O(n^2)。 选择排序(Selection Sort)选择排序思路跟插入排序类似，也分未排序区和已排序区。不同的是，选择排序每次都从未排序区选择最小值插入到已排序区的尾部，也就是将两者互换。（打破了原有数组元素的前后顺序） 12345678910111213141516171819# 选择排序def select_sort(a): n = len(a) if n == 1: return a for i in range(n): # 最小元素的下标 flag = i # 在未排序区选择最小元素 for j in range(i, n): # 不断更新最小值 if a[j] &lt; a[flag]: # 如果元素小于最小元素，更新下标 flag = j # 将最小元素与下标为i互换 a[flag], a[i] = a[i], a[flag] return aselect_sort(a) 分析选择排序算法 选择排序是原地排序算法吗 选择排序的空间复杂度为O(1)，所以是原地排序算法。 选择排序是稳定的排序算法吗 每次选择元素都是在未排序区选择最小值然后与已排序区的末尾(未排序的第一位元素)元素进行交换，会打破原有元素的前后顺序。 选择排序的时间复杂度是多少 不管待排序数组是有序还是倒序，每次选择最小元素的时候都会循环比较，执行n次这样的操作，所以不管最好情况时间复杂度、最坏情况时间复杂度和平均时间复杂度都是O(n^2)。 为什么插入排序要比冒泡排序更受欢迎从上文可以得知，这两个算法的时间复杂度都是O(n^2)，都是原地排序算法，唯一的区别就在于数据交换部分。冒泡排序交换操作有三个操作语句，而插入排序的元素移动操作只有一行代码，所以从理论上来说，插入排序更优。 参考文章数据结构与算法之美]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>排序</tag>
        <tag>冒泡排序</tag>
        <tag>插入排序</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记六】递归]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F15%2F%E9%80%92%E5%BD%92%2F</url>
    <content type="text"><![CDATA[这样理解“递归”看电影时，如果忘了自己在第几排，只需要问前面的人是第几排，一直到第一排，然后再把数字一排一排传回来，这样就知道答案了。 这整个过程就是标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。可以这样来表示： 12345def f(n): if n == 1: return 1 else: return f(n-1) + 1 时间复杂度为O(n)，因为递归调用一次会在内存栈中保存一次现场数据，所有需要额外考虑这部分开销。 递归需要满足的三个条件 一个问题的解可以分解为几个子问题的解 如上面例子，将“自己在哪一排”的问题，分解为“前一排的人在哪一排”这样的子问题 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 如上面例子，“自己在哪一排”这个问题跟“前一排的人在哪一排”是一样的求解思路 存在递归终止条件 对问题进行分解，必须存在终止条件，不能无限分解下去。如上面例子中，第一排的人不需要继续询问别人就知道自己在哪一排，这就是递归的终止条件 如何编写递归代码 写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。一句话总结：写出递推公式，找到终止条件 例子：假如有n个台阶，每次可以跨1个台阶或者2个台阶，请问走这n个台阶一共有多少种走法？ 递推公式：可以根据第一步的走法将所有走法分为两类，第一类是第一步走了1个台阶，第二类是第一步走了2个台阶。所以n个台阶的走法就是先走1个台阶之后n-1个台阶的走法加上先走2个台阶后，n-2个台阶的走法，公式如下： 1f(n) = f(n-1) + f(n-2) 终止条件：当只有1个台阶时，只有一种走法f(1)=1，当只有2个台阶的时候，有两种走法，所以终止条件只有f(1)=1并不够，需要加上f(2)=2。 递归代码： 123456def f(n): if n == 1: return 1 if n == 2: return 2 return f(n-1) + f(n-2) 编写递归代码时，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分析递归的每个步骤，因为这样很容易进入一个思维误区。 比如，有一个问题A可以分解为若干子问题B、C、D，我们可以假设子问题B、C、D已经解决，在此基础上思考如何解决问题A，并思考问题A与子问题B、C、D两层之间的关系即可。 递归代码要警惕堆栈溢出1.为什么递归代码容易造成堆栈溢出 函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等到函数执行完成返回时，才出栈。当函数返回时，栈就会减一层栈帧。由于栈的大小不是有限的，所以次数过多会导致堆栈会溢出。所以当递归求解的数据规模过大，调用层次太深，就有堆栈溢出的风险 2.如何避免出现堆栈溢出 可以通过在代码中设置递归调用的最大深度来解决问题，如python中可以这样设置： 12import syssys.setrecursionlimit(100000) 但是如果需要计算的深度过大，递归方法就不是那么实用。 递归代码要警惕重复计算为避免重复计算，可以用散列表之类的的数据结构来保存已经求解过的f(k)，如果已经求解过的就直接从散列表取值返回。 1234567891011res = dict()def f(n): if n == 1: return 1 if n == 2: return 2 if n in res: return res[n] ret = f(n-1) + f(n-2) res.update(&#123;n : ret&#125;) return ret 改写递归代码为非递归代码电影院例子，可以修改为 12345def (n): ret = 1 for i in range(2, n): ret = ret + 1 return ret 跨台阶问题，可以修改为： 123456789101112131415def f(n): # 终止条件有两个 if n == 1: return 1 if n == 2: return 2 # 当不满足终止条件时，每增加一个台阶都将之前的走法进行叠加 ret = 0 pre = 2 prepare = 1 for i in range(3, n): ret = pre + prepre prepre = pre pre = ret return ret 关于递归代码的调试方式 打印日志查看递归值 结合条件断点进行调试 参考文章数据结构与算法之美]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>递归</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记五】队列]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F15%2F%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[什么是队列跟栈一样，也是一种操作受限的线性表数据结构，不同的是，队列是先进先出。最基本的操作只有两个：入队enqueue()，也就是放一个数据到队列尾部；出队dequeue()，从队列头部取出一个元素。 具有某些特性的队列循环队列、阻塞队列、并发队列、循环并发队列等。 队列的实现1）顺序队列：用数组来实现 队列的实现需要两个指针：一个是head指针，指向对头；另一个是tail指针，指向队尾。下面是用Java基于数组实现的队列，代码来自《数据结构与算法之美》 12345678910111213141516171819202122232425262728293031323334// 用数组实现的队列public class ArrayQueue &#123;// 数组：items，数组大小：nprivate String[] items;private int n = 0;// head 表示队头下标，tail 表示队尾下标private int head = 0;private int tail = 0;// 申请一个大小为 capacity 的数组public ArrayQueue(int capacity) &#123;items = new String[capacity];n = capacity;&#125;// 入队public boolean enqueue(String item) &#123;// 如果 tail == n 表示队列已经满了if (tail == n) return false;items[tail] = item;++tail;return true;&#125;// 出队public String dequeue() &#123;// 如果 head == tail 表示队列为空if (head == tail) return null;// 为了让其他语言的同学看的更加明确，把 -- 操作放到单独一行来写了String ret = items[head];++head;return ret;&#125;&#125; 当有出队、入队操作时，head指针和tail指针都要往后移动。当tail移动到最右边的时候，就不能继续往队列里添加数据了，即使数组中还有空余空间。 像数组一样，可以使用数据搬移的方法，每次出队操作，都将剩余数据移动到队头，但是这样就会使得出队操作的时间复杂度从原来的O(1)变为O(n)。 同样的，也可以借鉴数组删除操作的优化方法——当没有空余空间时，再集中触发一次数据的搬移操作。现在修改入队函数enqueue()，代码如下： 12345678910111213141516171819// 入队操作，将 item 放入队尾public boolean enqueue(String item) &#123;// tail == n 表示队列末尾没有空间了if (tail == n) &#123;// tail ==n &amp;&amp; head==0，表示整个队列都占满了if (head == 0) return false;// 数据搬移for (int i = head; i &lt; tail; ++i) &#123;items[i-head] = items[i];&#125;// 搬移完之后重新更新 head 和 tailtail -= head;head = 0;&#125;items[tail] = item;++tail;return true;&#125; 2）链式队列：用链表来实现 同样需要两个指针：head指针和tail指针。入队时，tail-&gt;next=new_node, tail=tail-&gt;next；出队时，head=head-&gt;next 3）循环队列 使用数组实现的队列，我们将其首尾相连，构成一个环，这样就成了循环队列。循环队列很好的避免了出队操作时的数据搬移。 假设循环队列大小为8，当前head=4，tail=7，当有数据入队时，tail指针不更新为8，而是指向下标为0的位置。当有数据继续加入时，tail加1。 循环队列的队空队满判断条件也有了变化：队列为空时的判断条件依然是head=tail。队满时，(tail+1)%n=head，所以循环队列会浪费一个数组的储存空间(加1)。 4）阻塞队列 阻塞队列就是在队列的基础上添加阻塞操作，典型的“生产者-消费者模型”。队列为空时，从队头取数据会阻塞直到队列有了数据才会返回。队列满了，插入数据会被阻塞到队列中有空闲位置后才能插入数据，然后再返回。 5）并发队列 线程安全的队列叫做并发队列。最简单直接的实现方式就是在enqueue() 、dequeue()方法加上锁。但是锁力度大并发度会较低，同一时刻仅允许一个存或取操作。 实战问题：线程池没有空闲线程时，新的任务请求线程资源，线程池该如何处理？各种处理策略是如何实现的？ 基于链表可以实现一个支持无限排队的无界队列(unbounded queue)，但可能会造成过多的请求队列排队，请求处理的响应时间过长等问题。对于响应时间比较敏感的系统来说，这种解决方案并不合适。 基于数组实现的有界队列(bounded queue)，虽然适合对响应时间敏感的系统，但是如何设置一个合理的队列大小也是个问题。队列太大则等待的请求太多，队列太小会导致无法无法充分利用系统资源、发挥最大性能。 队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。 思考题：1、除了线程池这种池结构会用到队列排队请求，你还知道有哪些类似的池结构或者场景中会用到队列的排队请求？ 2、如何实现无锁并发队列？ 参考文章数据结构与算法之美]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapyd-client的安装使用]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F11%2Fscrapyd-client%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[scrapyd-client可以用来部署scrapy项目，它会帮我们把项目打包成egg文件，我们不用再动手调用add version.json接口去部署到scrapyd，操作简单。 安装1pip install scrapyd-client 安装成功之后，就可以使用其提供的scrapyd-deploy工具来部署了。 使用1）启动scrapyd 1scrapyd 2）修改scrapy.cfg文件 这里需要修改scrapy项目下的scrapy.cfg文件，原文件如下： 123456[settings]default = unfancy.settings[deploy]# url = http://localhost:6800/project = unfancy 如果只是在本地运行，就直接取消注释，如果是要部署到其他服务器，就将localhost修改为对应的ip，示例如下： 123456[deploy]url = http://193.112.107.55:6800/project = unfancy# 这里的账号密码是用来登录服务器的，如果不需要账号密码，就不用添加username = scrapypassword = secret 关于登录服务器这一块，可以使用nginx反向代理来实现用户登录。 3）部署 现在就可以在scrapy.cfg所在的路径下直接执行命令完成部署 1scrapyd-deploy 4）启动 使用scrapyd api启动任务，其他的API请看另一篇文章：scrapyd的安装使用 1234curl http://localhost:6800/schedule.json -d project=unfancy -d spider=fancy # spider就是scrapy爬虫中的name# 返回信息&#123;"jobid": "cd68179acd2911e8914cf45c89a40603", "status": "ok", "node_name": "Pro"&#125; 部署成功之后项目中多了一些文件，需要注意的是，如果项目本来就有setup.py文件，该文件会被使用，没有则会自动创建一个。 5）查看 打开浏览器查看详细信息 详解完整的部署命令如下： 1scrapyd-deploy &lt;target&gt; -p &lt;project&gt; --version &lt;version&gt; target：服务器的名字，在deploy后面，不写则是default，部署时可以省略，看上面例子👆。如果写了，在部署的时候一定要加上，否则会报Unknown target: default错误，看下面例子👇 project：项目名称，一般创建scrapy项目时就已经给定了，部署时可以不用带上 version：自定义，不写默认是当前时间戳 如果有多台服务器，需要指定target，同时修改url。 1234567891011[deploy:target1]url = http://193.112.107.55:6800/project = unfancyusername = scrapypassword = secret[deploy:target2]url = http://193.112.107.54:6800/project = unfancyusername = scrapypassword = secret 查看可用的target 1scrapyd-deploy -l 查看某个特定的target 1scrapyd-deploy -L target1 只部署其他的一台服务器，进入项目根目录 1scrapyd-deploy target1 如果要一次性部署项目到多台服务器，同样进入项目根目录 1scrapyd-deploy -a -p &lt;project&gt; 参考文章scrapyd-client]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>scrapyd-client</tag>
        <tag>分布式</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapyd的安装使用]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F11%2Fscrapyd%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[scrapyd可以管理多个项目，每个项目可以上传多个版本，但是只有最新的版本会被使用。所以，用scrapy完成爬虫任务之后，可以用scapyd来部署管理。 安装1pip install scrapyd 配置文件unix系统默认是在/etc/scrapyd/scrapyd.conf，应该是版本不同吧，安装后并没有看到，需要自己配置，具体参看官方文档 12mkdir /etc/scrapydvim /etc/scrapyd/scrapyd.conf 在配置文件中写入以下内容（官方） 1234567891011121314151617181920212223242526272829[scrapyd]eggs_dir = eggslogs_dir = logsitems_dir =jobs_to_keep = 5dbs_dir = dbsmax_proc = 0max_proc_per_cpu = 4finished_to_keep = 100poll_interval = 5.0bind_address = 127.0.0.1 # 这里可以修改为0.0.0.0http_port = 6800debug = offrunner = scrapyd.runnerapplication = scrapyd.app.applicationlauncher = scrapyd.launcher.Launcherwebroot = scrapyd.website.Root[services]schedule.json = scrapyd.webservice.Schedulecancel.json = scrapyd.webservice.Canceladdversion.json = scrapyd.webservice.AddVersionlistprojects.json = scrapyd.webservice.ListProjectslistversions.json = scrapyd.webservice.ListVersionslistspiders.json = scrapyd.webservice.ListSpidersdelproject.json = scrapyd.webservice.DeleteProjectdelversion.json = scrapyd.webservice.DeleteVersionlistjobs.json = scrapyd.webservice.ListJobsdaemonstatus.json = scrapyd.webservice.DaemonStatus 启动1scrapyd 运行之后，在浏览器打开http://localhost:6800，就可以看到以下页面： API1）daemonstatus.json 查看scrapyd当前状态 1curl http://localhost:6800/daemonstatus.json 示例结果 1&#123; &quot;status&quot;: &quot;ok&quot;, &quot;running&quot;: &quot;0&quot;, &quot;pending&quot;: &quot;0&quot;, &quot;finished&quot;: &quot;0&quot;, &quot;node_name&quot;: &quot;node-name&quot; &#125; 2）addversion.json 给项目添加一个版本，项目不存在则会创建。这个是用来部署项目的，前提是要先创建egg文件。三个参数都是必须的 1curl http://localhost:6800/addversion.json -F project=project_name -F version=v1 -F egg=project.egg 示例结果 1&#123;&quot;status&quot;: &quot;ok&quot;, &quot;spiders&quot;: 3&#125; 3）schedule.json 调度已经部署好的spider运行，返回job id。 参数说明 project：必须，项目名 spider：必须，spider name setting：选填，设置scrapy项目 jobid：选填，用来验证job，会覆盖默认生成的UUID _version：选填，选择运行的项目版本 其他的参数都会作为spider参数 1curl http://localhost:6800/schedule.json -d project=project_name -d spider=spider_name -d setting=DOWNLOAD_DELAY=2 -d arg1=val1 示例结果，status是项目运行结果，如果出错的话会是error，还有message 1&#123;"status": "ok", "jobid": "6487ec79947edab326d6db28a2d86511e8247444"&#125; 4）cancel.json 如果job是正在运行的，会被停止，如果是pending状态，则会被移除。两个参数都是必须的 1curl http://localhost:6800/cancel.json -d project=project_name -d job=6487ec79947edab326d6db28a2d86511e8247444 示例结果 1&#123;"status": "ok", "prevstate": "running"&#125; 5）listprojects.json 列出scrapy server的所有项目 1curl http://localhost:6800/listprojects.json 示例结果 1&#123;"status": "ok", "projects": ["myproject", "otherproject"]&#125; 6）listversions.json 获取某个项目的所有可用版本，版本号按顺序返回，最后一个是最近使用的版本 1curl http://localhost:6800/listversions.json?project=project_name 示例结果 1&#123;"status": "ok", "versions": ["r99", "r156"]&#125; 7） listspiders.json 返回项目中所有可用的spider 1curl http://localhost:6800/listspiders.json?project=project_name 示例结果 1&#123;"status": "ok", "spiders": ["spider1", "spider2", "spider3"]&#125; 8）listjobs.json 返回项目中所有状态的任务，pending、running、finished 1curl http://localhost:6800/listjobs.json?project=project_name | python -m json.tool 示例结果 123456789101112131415161718192021222324&#123; "status": "ok", "pending": [ &#123; "project": "myproject", "spider": "spider1", "id": "78391cc0fcaf11e1b0090800272a6d06" &#125; ], "running": [ &#123; "id": "422e608f9f28cef127b3d5ef93fe9399", "project": "myproject", "spider": "spider2", "start_time": "2012-09-12 10:14:03.594664" &#125; ], "finished": [ &#123; "id": "2f16646cfcaf11e1b0090800272a6d06", "project": "myproject", "spider": "spider3", "start_time": "2012-09-12 10:14:03.594664", "end_time": "2012-09-12 10:24:03.594664" &#125; ]&#125; 8) delversion.json 删除项目的某个版本，如果给定项目没有其他版本，那么整个项目都会被移除 1curl http://localhost:6800/delversion.json -d project=project_name -d version=v1 示例结果 1&#123;"status": "ok"&#125; 9) delproject.json 删除项目以及项目的所有版本 1curl http://localhost:6800/delproject.json -d project=project_name 示例结果 1&#123;"status": "ok"&#125; 关于部署需要的egg文件，可以用setuptools模块，不过有点麻烦呀～所以就直接使用Scrapyd-Client这个现成的工具来部署。具体可以看这篇文章：scrapyd-client的安装使用 参考文章Scrapyd API scrapyd和scrapyd-client使用教程]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>scrapyd</tag>
        <tag>分布式</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单链表反转的python实现]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F10%2F%E5%8D%95%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[参考了看图理解单链表的反转一文，楼主思路清晰，图片形象生动，讲得通俗易懂。当然，像我这样笨的人还是看了几遍，而且一开始还犯了个常识性错误：忘了p.next是包括之后的所有数据，哎～看不懂的时候可以尝试把每一行代码运行之后的结果给画出来，方便理解。 1234567891011121314151617181920class ListNode: """实现链表""" def __init__(self, x): self.val = x self.next = None # 逐个反转：使用3个指针遍历单链表，逐个点进行反转。def reverse_list(head): if head == None or head.next == None: #少于两个结点没有反转的必要 return head p = head q = head.next head.next = None # 旧的头指针是新的尾指针，next需要指向None while q: r = q.next # 先保留下一个step要处理的指针 q.next = p # 然后p q交替工作进行反向 p = q q = r head = p # 最后q必然指向None，所以返回了p作为新的头指针 return head 123456789101112131415# 尾插反转：从第2个结点到第N个结点，依次逐结点插入到第1个结点(head结点)之后，最后将第一个结点挪到新表的表尾。def reverse_list2(head): if head == None or head.next == None: #少于两个结点没有反转的必要 return head p = head.next # 第二个不需要参与循环 while p.next != None: q = p.next # 第一次循环，head-&gt;p-&gt;q-&gt;··· p.next = q.next # 第一次循环，p和q之间的链接断开 q.next =head.next # 这个不先执行会导致指针丢失，内存泄露。这里实现了第二第三个反转 head.next = q # 将head指针指向q head-&gt;q-&gt;p-&gt;··· p.next = head # 到最后，p.next已经为None，这时候指向head，形成环 head = p.next.next # 新的head变成原head的next p.next.next = None # 断开环 return head 123456789# 递归：实现反转 def reverse_list3(head): if head.next == None: # 只有一个的时候，不需要反转 return head new_head = func3(head.next) # 返回已经反转之后的结果 head.next.next = head # 形成环 head.next = None # 断开环，完成反转 return new_head # 返回已经反转之后的结果]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装scrapyd出错]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F09%2Fcentos7%E5%AE%89%E8%A3%85scrapyd%E5%87%BA%E9%94%99%2F</url>
    <content type="text"><![CDATA[在centos7下安装scrapyd出现编译中断的错误 在网上找了一圈，基本都是说缺乏python依赖包，只需要执行yum install python-dev就可以。结果呢，我从python-dev一路试到了python3.6.1-dev，把系统里存在的版本都试了个遍，还是不行。在临要放弃的时候找到了正确的解决方法，请戳文章 首先，确实是依赖包的问题，但是要找到对应的版本 1yum search python | grep -i devel 找到版本之后就可以直接安装了 1sudo yum install python36-devel.x86_64 最后一步 1pip install scrapyd --user 看到这个输出，真的是激动到老泪纵横啊～～]]></content>
      <tags>
        <tag>centos7</tag>
        <tag>安装</tag>
        <tag>scrapyd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记四】栈]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F08%2F%E6%A0%88%2F</url>
    <content type="text"><![CDATA[栈什么是栈栈是一种先进后出，后进先出的，“操作受限”的线性表。栈只允许在一端插入和删除数据，不能从中间任意抽出。 为什么选择栈从功能上来说，数组或者链表确实可以代替栈，但是数组或链表暴露了太多的操作借口，操作上的缺灵活自由，但使用时就比较不可控，容易出错。所以在某些特定场景，比如当某个数据集合栈的特性，就应该选择栈这种数据。 如何实现一个“栈”1）顺序栈：用数组实现的栈 2）链式栈：用链表实现的栈 不管是顺序栈还是链式栈，我们存储数据只需要一个大小为n的数组即可。但是不代表着空间复杂度就是O(n)，因为空间复杂度是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。 用Java实现的栈，代码来自《数据结构与算法之美》： 123456789101112131415161718192021222324252627282930313233// 基于数组实现的顺序栈public class ArrayStack &#123; private String[] items; // 数组 private int count; // 栈中元素个数 private int n; // 栈的大小 // 初始化数组，申请一个大小为 n 的数组空间 public ArrayStack(int n) &#123; this.items = new String[n]; this.n = n; this.count = 0; &#125; // 入栈操作 public boolean push(String item) &#123; // 数组空间不够了，直接返回 false，入栈失败。 if (count == n) return false; // 将 item 放到下标为 count 的位置，并且 count 加一 items[count] = item; ++count; return true; &#125; // 出栈操作 public String pop() &#123; // 栈为空，则直接返回 null if (count == 0) return null; // 返回下标为 count-1 的数组元素，并且栈中元素个数 count 减一 String tmp = items[count-1]; --count; return tmp; &#125;&#125; 支持动态扩容的顺序栈当数组内存不够时，通过重申一个更大的数组，再将原本的数据复制过去，顺序栈也一样。不过像这种支持动态扩容的顺序栈在实际开发中并不常见。 所以，对于入栈操作来说，最好情况时间复杂度是O(1)，最坏情况时间复杂度是O(n).当内存满了之后（假设栈大小为k），需要申请更大的内存，并且做k个数据的搬移操作，然后在入栈。因为在入栈时并不需要再申请内存和搬移数据，所以根据均摊还法可以得到平均情况时间复杂度也是O(1)。 栈的应用在函数调用中的应用12345678910def main(): a = 1 ret = 2 res = 0 ret = add(3, 5) print(ret) return 0def add(a, b): return a+b 在这个例子中，先是压入main函数中的变量，再压入add函数的变量。调用add函数时，也就是出栈，最后才是main函数的执行结果。整个调用的过程完全符合栈先进后出的特性。 在表达式求值中的应用使用两个栈求解式子3+5*8-6，一个用来保存操作数，一个用来保存运算符。具体过程：从左向右遍历表达式，遇到数字就压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，就从运算符栈中取出栈顶运算符，从操作数栈的栈顶取出2个操作数，计算之后将结果压入操作数栈，继续比较。 在括号匹配中的应用假设表达式中只有三种括号()[]{}，并且可以任意嵌套。那么在给定的包含这三种括号的字符串中，如何检测它是否合法。注：{[]}()是合法的，{[}()]则是不合法的。 具体过程：从左到右遍历字符串，如果是左边括号就将其压入栈中，如果是右括号就从栈顶取出一个元素进行比较，如果能够配对，则继续比较剩下的字符串。如果过程中遇到不能配对的右括号或者栈中没有数据，则是非法格式。整个字符串都比较结束之后，栈为空则是合法格式，否则为非法格式。 实现浏览器页面的前进后退同样使用两个栈a和b，首次浏览的页面依次压入a，点击后退按钮时，就从栈a中出栈，并依次放入栈b。当点击前进按钮时，则是相反的操作。从栈b中出栈，再放入栈a。当栈为空时，就不能再前进或者后退了。 如果在浏览的过程中，跳转到新的页面，这时候就没办法再通过前进/后退重复查看了，所以需要清空另一个栈的数据。 参考文章数据结构与算法之美]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记三】链表]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F06%2F%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[链表什么是链表 链表跟数组一样，也是一种线性表数据结构 不需要连续的内存块，对内存要求小，与数组相反 通过指针把一组零散的内存块串联在一起 内存块是链表的结点，保存下一个结点的地址的指针叫做后继指针next 链表特点 插入、删除操作时间复杂度为O(1)，只需要改变指针地址即可(不包括遍历，具体看后面分析) 随机访问效率低，时间复杂度为O(n)，需要从链头遍历到链尾 相对于数组，因为要保存每个数据结点的后继指针，内存空间消耗比较大 有哪些常见的链表结构常见的链表结构有三种：单链表、双向链表和循环链表。 1）单链表 每个结点只包含一个指针，即后继指针 第一个结点叫做头结点，用来记录链表的基地址 最后一个结点叫做尾结点，指针指向一个空地址NULL，表示这是链表上最后一个结点 2）循环链表 跟单链表相比，唯一的区别就在尾结点。循环链表的尾结点指向链表的头结点。适合用来处理具有环形结构特点的数据，比如约瑟夫问题。 3）双向链表 每个结点都有两个指针，一个指向后面结点的后继指针next，另一个指向前面结点的前驱指针prev 需要额外的两个空间来保存两个结点 支持双向遍历 支持O(1)时间复杂度的情况下找到前驱结点 4）双向循环链接 循环链表和双向链表的结合：首结点的前驱指针指向尾结点，尾结点的后继指针指向首结点。 链表的删除操作从链表中删除一个数据有以下两种情况： 1）删除结点中“值等于某个给定值”的结点 在单链表和双向链表中，都需要从头结点一个个依次遍历对比，直到找到值等于给定值的结点，然后通过改变指针将其删除。遍历的时间复杂度为O(n)，删除操作的时间复杂度为O(1)，根据加法法则，该情况下，总的时间复杂度为O(n) 2）删除给定指针指向的结点 要删除某个结点q需要知道其前驱结点，但是单链表不支持直接获取前驱结点，所以还是要从头结点开始遍历，直到p-&gt;next=q 双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历，所以删除操作的时间复杂度为O(1) 同理，要在指定结点前面插入一个结点，单链表的时间复杂度为O(n)，双向链表的时间复杂度为O(1) 链表VS数组 数组简单易用，需要连续的内存空间，借助CPU的缓存机制，预读数组中的数据，访问效率高 数组大小局定，需要占用整块连续内存空间。如果声明的数组过大，系统没有足够的连续内存空间分配给它，会导致“内存不足”。声明的数组过小，不够用时需要再申请一个更大的内存空间，再拷贝原数据进去，非常耗时 如果代码对内存的使用非常苛刻，数组相对更适合 链表在内存中不是连续存储，对CPU缓存不友好，没办法有效预读 链表本身大小没有限制，支持动态扩容 链表中的每个结点需要额外的内存空间存储指针，内存消耗会翻倍 对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片。如果是Java语言，有可能会导致频繁的GC(Carbage Collection， 垃圾回收) 应用什么是缓存缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有非常广泛的使用，比如常见的有CPU缓存、数据库缓存、浏览器缓存等。但是缓存大小有限，当缓存满了之后，应该清理哪些数据？哪些数据又应该被保留呢？如何决定？ 缓存淘汰策略有哪些常见的缓存淘汰策略有三种： 先进先出策略FIFO(First In First Out) 最少使用策略LFU(Least Frequently Used) 最近最少使用策略LRU(Least Recently Used) 如何基于链表实现LRU缓存淘汰算法当我们维护一个有序单链表，越靠近链表尾部的结点是越早之前的访问(比较久之前)。当有一个新的数据被访问时，我们从链表头部开始顺序遍历链表 如果此数据之前已经被缓存在链表中，我们遍历得到的这个数据对应的结点，并将其从原来的位置删除，再插入到链表的头部 如果此数据没有再缓存链表中，分两种情况 如果此时缓存未满，将此结点直接插入到链表的头部 如果此时缓存已满，将链表尾结点删除，将新的数据结点插入链表的头部 因为不管缓存有没有满，都需要遍历一遍链表，所以这样的实现思路，缓存访问的时间复杂度都是O(n) 设计思想1）用空间换时间 当内存空间充足的时候，同时也更加追求代码的执行速度，就可以选择空间复杂度相对较高、但时间复杂度相对较低的算法或者数据结构，例如缓存。我们把数据存储在硬盘上，节省内存，但每次查询都需要询问一个硬盘，速度较慢。通过缓存可以事先将数据加载到内存中，耗费了内存空间，不过大大提高了数据查询的速度 2）用时间换空间 与1）相反，如果内存紧缺，比如代码在手机或者单片机上运行，就需要选择空间复杂度相对较低、但时间复杂度相对高的算法 如何轻松写出正确的链表代码理解指针或引用的含义将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。也就是python中的“引用” 12p-&gt;next=q # p结点中的next指针存储了q结点的内存地址p-&gt;next=p-&gt;next-&gt;next # p结点的next指针存储了p结点的下下一个结点的内存地址 警惕指针丢失和内存泄漏假如我们想在结点p和相邻结点q之间插入一个结点x，以下代码将会发生指针丢失和内存泄漏 123// 错误！！需要颠倒一下顺序p-&gt;next = x; // 将 p 的 next 指针指向 x 结点；x-&gt;next = p-&gt;next; // 将 x 的结点的 next 指针指向 b 结点； 第一行代码执行之后，结点p不再指向结点q了，这时候再执行第二行代码，就相当于把x赋值给x-&gt;next，结果x指向x自己。因此，整个链表也就断开了。 利用哨兵简化实现难度针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。如下所示： 1234567891011121314// 插入：向空链表插入第一个结点if (head == null) &#123; head = new_node;&#125;// 在链表的结点p后插入结点new_nodenew_node-&gt;next = p-&gt;next;p-&gt;next = new_node;//删除：删除尾结点if (head-&gt;next == null) &#123; head = null;&#125;// 删除链表其他结点p-&gt;next = p-&gt;next-&gt;next; 对于这种情况，我们可以引入哨兵结点(解决边界问题，不参与业务逻辑)，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。带有哨兵结点的链表叫带头链表，相反，则叫做不带头链表。看图(图片来自《数据结构与算法之美》) 利用哨兵简化编程难度的例子有很多，比如插入排序、归并排序、动态规划等。看下面的例子 1234567891011121314151617181920212223242526int find(char* a, int n, char key) &#123; int i = 0; while (i &lt; n) &#123; if (a[i] == key) &#123; return i; &#125; ++i; &#125; return -1;&#125;//使用哨兵边界，仅作示例，一般并不需要追求这么极致的性能inf find2(char* a, int n, int key) &#123; if (a[n-1] == key) &#123; return n-1; &#125; char tmp = a[n-1]; a[n-1] = key; int i = 0; while (a[i] != key) &#123; ++i; &#125; a[n-1] = tmp; if (i == n-1) return -1; return i;&#125; 当字符串a很长的时候，第二段代码运行更快些。通过一个哨兵a[n-1] == key，省掉了一个比较语句i&lt;n。 重点留意边界条件处理常用来检查链表代码是否正确的边界条件有： 如果链表为空时，代码是否能正常工作 如果链表只包含一个结点时，代码是否能正常工作 如果链表只包含两个结点时，代码是否能正常工作 代码逻辑在处理头结点和尾结点的时候，是否能正常工作 如果链表代码在正常情况下能工作，也要对比着以上几个边界条件。如果都没有问题，那么基本上可以认为代码是没有问题的。当然，具体的还是要结合实际情况来思考，比如可能会遇到哪些边界条件或者异常情况。 举例画图，辅助思考对于比较复杂的链表操作，可以使用举例法和画图法。找一个具体的例子，画出指针变化前后的链表 多写多练最后一个技巧就是没有技巧，多练多练多练。。。 练习常见的链表操作： 单链表反转 链表中环的检测 两个有序的链表合并 删除链表倒数第n个结点 求链表的中间结点 还有上一小节遗留的问题，如何判断一个字符串是否是回文字符串？？ 参考数据结构与算法之美]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记二】数组]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F05%2F%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[数组什么是数组数组是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。 如何实现随机访问易错点：数组支持随机访问，根据下标随机访问的时间复杂度为O(1)，其他的查找，即使是排好序的数组，用二分查找的时间复杂度也是O(logn)，并不是O(1)。 计算机会给每个内存单位分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它首先会通过下面的寻址公式，计算出该元素存储的内存地址： 12int[] a = new int[10]a[i]_address = base_address + i * data_type_size 计算机给数组a[10]分配了一块连续空间1000～1039 data_type_size：数组中每个元素的大小（例子中是int，大小为4个字节） base_address：内存块的首地址 二维数组m*n，a[i]][j](i&lt;m, j&lt;n)的寻址公式： 1a[i]_address = base_address + (i*n+j) * data_type_size 低效的插入操作及其改进1）操作：将一个数据插入到长度为n的数据中的第k个位置，为了保持内存数据的连续性，那么我们需要将第k～n这部分的元素都顺序往后移一位，这样的插入操作时间复杂度为O(n) 2）改进：如果数组是有序的，则按1）来操作。如果数组是无序的，那么可以直接将第k个元素放到数组最后，将需要插入的数据直接放入位置k，时间复杂度为O(1) 低效的删除操作及其改进1）跟插入操作类似，时间复杂度也是O(n) 2）改进：在某些场景下，我们不一定非得追求数组中数据的连续性。这样在执行删除操作时，可以先记录已经删除的数据，等到数组没有多余空间存储数据时，再触发执行一次真正的删除操作 警惕数据的访问越界问题像python、Java这些语言会做越界检查，所以不用考虑这个问题。 123456789int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(; i&lt;=3; i++)&#123; arr[i] = 0; printf("hello world\n"); &#125; return 0;&#125; 在C语言中，数组越界是一种未决行为。而文中这个例子会无限循环打印“hello world”。因为函数体内的局部变量存在栈上，且是连续压栈。再Linux进程的内存布局中，栈区的高地址空间，从高向低增长。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界刚好访问到i。前提是i和arr元素是同类型，否则这段代码仍是未决行为。 线性表vs非线性表1）每个线性表上的数据最多只有前和后两个方向，如：数组、链表、队列、栈等 2）非线性表中，数据之间并不是简单的前后关系，如：二叉树、堆、图等 为什么大多数编程语言中，数组都是从0开始编号1）效率优化 下标最确切的定义应该是“偏移(offset)”。在前面例子中，a用来表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址。所以计算a[k]的内存地址只需要用这个公式： 1a[k]_address = base_address + k * type_size 如果下标是从1开始，那么计算a[k]的内存地址的公式则变成： 1a[k]_address = base_address + (k-1) * type_size 每次随机访问数组，CPU都会多了一次减法指令。 2）历史原因 C语言设计者用0做下标，之后的一些高级语言都效仿了它，沿用了从0开始技术的习惯。 因为python的数组特性跟C语言差别还是蛮大的，很多特征都要自己定义，对于这个可以查看原作者的代码示例，其他语言我也不懂，所以这篇笔记还有一些没有整理出来，后续再更新 参看文章数据结构与算法]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在虚拟环境中安装jupyter notebook]]></title>
    <url>%2Famberwest.github.io%2F2018%2F10%2F02%2F%E5%9C%A8%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%B8%AD%E5%AE%89%E8%A3%85jupyter-notebook%2F</url>
    <content type="text"><![CDATA[jupyter notebook是一个开源Web应用程序，可以创建和共享包含实时代码，方程式，可视化和叙述文本的文档。用途包括：数据清理和转换，数值模拟，统计建模，数据可视化，机器学习等等。所以在平时的工作中也会经常用到。 最近在一个虚拟环境中使用notebook时，发现版本跟虚拟环境的python版本号对不上，就打算直接upgrade，事实证明这种想法错的非常离谱，因为直接upgrade的是系统jupyter notebook。接下来才是在虚拟环境中安装对应版本jupyter的正确打开方式。 在虚拟环境下 创建虚拟环境 使用pyenv-virtualenv创建新的虚拟环境，具体使用查看：pyenv在centos7和mac下的安装使用 1234# 在sample文件夹下创建一个同名的基于python3.6的虚拟环境mkdir samplecd samplepyenv virtualenv 3.6.5 sample 激活虚拟环境 1pyenv activate sample 安装jupyter 1pip install jupyter 启动jupyter 1jupyter notebook 至此，就大功告成啦！！！ 添加python3到jupyter之前不知道怎么操作，居然把python3从jupyter notebook给删除了，需要重新安装一下 12python3 -m pip install ipykernelpython3 -m ipykernel install --user 又一个坑。。。 自从使用pyenv管理python之后，就觉得以前电脑里装的python3.5好多余，忍不住删删删，结果导致jupyter能打开，但是显示500错误，提示找不到python的bin路径。安装前面的方式建立虚拟环境再重新安装，还是不行。 1234567891011# 错误Uncaught exception POST /api/sessions (::1) HTTPServerRequest(protocol=&apos;http&apos;, host=&apos;localhost:8889&apos;, method=&apos;POST&apos;, uri=&apos;/api/sessions&apos;, version=&apos;HTTP/1.1&apos;, remote_ip=&apos;::1&apos;) Traceback (most recent call last): File &quot;/Users/amber/.pyenv/versions/3.5.3/envs/amber/lib/python3.5/site-packages/tornado/web.py&quot;, line 1592, in _execute ... File &quot;/Users/amber/.pyenv/versions/3.5.3/lib/python3.5/subprocess.py&quot;, line 676, in __init__ restore_signals, start_new_session) File &quot;/Users/amber/.pyenv/versions/3.5.3/lib/python3.5/subprocess.py&quot;, line 1282, in _execute_child raise child_exception_type(errno_num, err_msg) FileNotFoundError: [Errno 2] No such file or directory: &apos;/Library/Frameworks/Python.framework/Versions/3.5/bin/python3&apos; 最后是用了下面的方式安装成功的，其他步骤跟上面一样 1ipython kernel install --user --name=virtual_env_name 参考文章：install jupyter notebook in a virtual environment Installing the IPython kernel]]></content>
      <tags>
        <tag>jupyter</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义logging]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F30%2F%E8%87%AA%E5%AE%9A%E4%B9%89logging%2F</url>
    <content type="text"><![CDATA[组件关系logging模块有四大组件，分别是formatter、 filter、 handler和logger。这些组件各自的功能及关系如下： 组件名称 对应类名 功能描述 日志器 Looger 提供了应用程序可抑制使用的接口 处理器 Handler 将logger创建的日志记录发送到合适的目的输出 过滤器 Filter 提供了更细粒度的控制工具来决定输出哪条日志记录，丢弃哪条日志记录 格式器 Formatter 决定日志记录的最终输出样式 关系描述： logger需要通过handler将日志记录输出到目标位置，比如：文件、sys.stdout、网络等 不同的handler可以讲日志输出到不同的位置 logger可以设置多个handler讲同一条日志记录输出到不同的位置 每个handler都可以设置自己的filter来实现日志过滤，从而只保留感兴趣的日志 每个handler都可以设置自己的formatter实现同一条日志以不同的格式输出到不同的地方 配置日志常用的日志配置有几种方式，这里使用的是yaml配置文件，配置简单明了，不过灵活度不够。(注：yaml文件出现中文可能会报错) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748version: 1disable_existing_loggers: False# set the log formatformatters: simple: format: '%(asctime)s - %(name)s - %(levelname)s - %(filename)s - %(lineno)s - %(message)s' datefmt: '%Y-%m-%d %H:%M:%S'handlers: # log to screen console: class: logging.StreamHandler level: DEBUG formatter: simple error_handler: class: logging.handlers.RotatingFileHandler level: ERROR filters: [video] formatter: simple filename: logs/error.log maxBytes: 10485760 # 10MB backupCount: 5 encoding: "utf8" mode: a info_handler: class: logging.handlers.RotatingFileHandler level: DEBUG formatter: simple filename: logs/info.log maxBytes: 10485760 # 10MB backupCount: 5 encoding: "utf8" mode: aloggers: video: level: DEBUG handler: [console, error_handler] propagate: trueroot: level: DEBUG handlers: [console, info_handler, error_handler] propagate: true formatters定义了日志输出的样式，simple是formatter的id，handler可以直接调用 handlers定义日志的输出位置。这里定义了三种情况，id分别是console、error_handler和 info_handler。console是直接输出到屏幕，其他两种在不同的日志级别下分别写入文件。 loggers定义了记录器（logger）的ID和对应的handler。在代码中可以使用logging.getLogger(&#39;video&#39;)来获得ID对应的logger。 root则是默认的输出配置。如果在获取logger记录器时没有提供任何信息给logging.getLogger(type)，比如：当type为root，或者为空，或者是使用__name__时，就会使用该配置。 调用的时候也很简单： 1234567891011121314151617181920import loggingimport logging.configimport yamldef setup_logging(default_path='logging.yaml', default_level=logging.DEBUG): """配置日志""" path = os.path.join(BASE_DIR, default_path) if os.path.exists(path): with open(path, 'rt') as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config) else: logging.basicConfig(level=default_level) print(f'the log setting path &#123;default_path&#125; does not exist')if __name__ == '__main__': setup_logging() logger = logging.getLogger('video') # 这里不指定，默认会调用root logger.info('test') 运行之后会生成两个日志文件info.log和error.log，info.log文件中多了这么一行记录： 2018-10-01 00:16:51 - video - INFO - utils.py - 154 - test 参考文献logging.config Good logging practice in Python Python之日志处理（logging模块）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>logging</tag>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【数据结构与算法笔记一】复杂度分析]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F27%2F%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[大O复杂度表示法从CPU角度来说，每一行代码都有类似的操作：读数据-运算-写数据。而算法的执行效率可以大概理解为算法代码执行的时间，用大O来表示，形式如：T(n)=O(f(n))。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以也称为渐进时间复杂度（asymptotic time complexity），简称时间复杂度。在计算复杂度的过程中，我们常常用忽略共识中的常量、低阶、系数，最需要关注最大阶的量级就可以。关于这一点，可以从极限的角度来理解，当n趋向∞时，常量及低阶量级都可以被忽略。 时间复杂度分析关于时间复杂度分析有三个比较实用的方法： 只关注循环执行次数最多的一段代码 12345def cal(n): sum = 0 for i in range(n): sum += i return sum 在这段代码中，sum=0是常量级的执行时间，与n无关。在for循环中，两行代码都执行了n次，所以这段代码总的时间复杂度是T(n)=O(1+2n)，也就是O(n)。 加法法则：总复杂度等于量级最大的那段代码的复杂度 12345678910111213def cal(n): sum_1 = sum_2 = sum_3 = 0 for i in range(100): sum_1 += i for j in range(n): sum_2 += j for k in range(n): for l in range(n): sum_3 += k * l return sum_1 + sum_2 + sum_3 这三段代码的时间复杂度分别是O(1)，O(n)和O(n^2)，sum_1计算了100次，也是常量级，所以复杂度为O(1)。综合来看，这个函数的时间复杂度就是O(n^2)，也就是说，总的时间复杂度就等于量级最大的那段代码的时间复杂度。 我们可以将上述计算过程抽象为以下公式： 123T1(n) = O(f(n))T2(n) = O(g(n))T(n) = T1(n) + T2(n) = max(O(f(n)), O(g(n))) = O(max(f(n), g(n))) 乘法法则：嵌套代码的复杂度就等于嵌套内外代码复杂度的乘积 1234567891011def cal(n): sum = 0 for k in range(n): sum += k return sumdef cal2(n): sum = 0 for i in range(n): sum += cal(n) return sum 乍一看，可能会觉得这两个函数的时间复杂度都是O(n)，其实呢，cal2调用了cal函数，所以cal2的时间复杂度应该是T(n)=O(n*n)=O(n^2)。 类似地，可以抽象出如下公式： 123T1(n) = O(f(n))T2(n) = O(g(n))T = T1(n) * T2(n) = O(f(n)) * O(g(n)) = O(f(n) * g(n)) 多个规模求加法：O(m+n) 123456789def cal(n, m):sum_1 = sum_2 = 0for i in range(n):sum_1 += ifor j in range(m):sum_2 += jreturn sum_1 + sum_2 在代码中，因为没办法确定m和n哪个大，所以复杂度为O(m+n)。对应的加法法则就需要修改为： 123T1(n) = O(f(n))T2(m) = O(g(m))T(n) = T1(n) + T2(m) = O(f(n) + g(m)) 乘法法则仍然有效，不需要修改。 几种常见的时间复杂度实例分析 多项式量级 常量阶：O(1) 12# 执行代码不随n增大而增大，只要不存在循环语句、递归语句，即使有成千上万行代码，同样也只有O(1)sum = 1 对数阶：O(logn) 123456# i其实是个公比为2的等比序列。i的增长比较，用对数函数可以求出log(n, 2)=n就可以得运行效率i = 1n = 100while i &lt; n: # i=i*3，也是同样的运算过程，根据换底公式：log(n, 3)=log(2, 3)*log(n ,2)，常数可以忽略 i = i * 2 线性阶：O(n) 12345def cal(n): sum = 1 for i in range(n): sum += i * 2 return sum 线性对数阶：O(nlogn) 12345def cal(n): sum = 1 for i in range(n): while i &lt; n: sum += i * 2 跟对数阶类似，不过使用的乘法法则 平方阶：O(n^2) 立方阶：O(n^3) k方阶：O(n^k) 非多项式量级，也就是NP问题（Non-Deterministic Prolynomial）。 随着数据规模的增长，算法的执行时间和空间占用暴增，这类算法性能极差，应避免这样的代码。 指数阶：O(2^n) 阶乘阶：O(n!) 空间复杂度分析空间复杂度全称为渐进空间复杂度(asymptotic space complexity)，表示算法的存储空间和数组规模之间的增长关系。 例如：存储一个二进制数，空间复杂度是O(logn)bit。8用二进制表示就是3个bit，16用二进制表示是4个bit，以此类推，n用二进制表示就是logn个bit。 复杂度分析的四个知识点最好情况时间复杂度(best case time complexity)、最坏情况时间复杂度(worst case time complexity)、平均情况时间复杂度(average case time complexity)、均摊时间复杂度(amortized time complexity) 最好、最坏情况时间复杂度12345678910111213141516def find1(li, n, x): """在无序列表中查找变量x出现的位置，找不到返回-1""" # li是列表，n为列表长度 pos = -1 for i in range(n): if li[i] == x: pos = i return posdef find2(li, n, x): pos = -1 for i in range(n): if li[i] == x: pos = i break return pos 在find1函数中，时间复杂度为O(n)，在第二个函数中时间复杂度就不好确定了。最好的情况就是第一个元素就是要找的变量x，时间复杂度为O(1)，也就是最好情况时间复杂度。最坏的情况是x不在列表中，整个列表都会被循环一遍，时间复杂度为O(n)，也就是最坏情况时间复杂度。 平均情况时间复杂度还是刚才的例子，要查找的变量x在数组中的位置，有n+1种情况：在数组的0～n-1位置中和不在数组中。我们假设x在数组中和不在数组中的概率分别为1/2（为了方便理解和计算）。那么x在数组中有n种情况，每个位置出现x的概率都为1/(2n)，需要查找的元素个数分别对应着1～n；x不在数组，那么需要遍历整个数组，即n个元素。所以平均情况时间复杂度的计算过程如下所示： 这个值是概率论中的加权平均值/期望值，所以平均情况时间复杂度全称应该叫加权平均时间复杂度或者期望时间复杂度。不过当n趋向无穷大时，时间复杂度还是O(n)。所以只有当同一段代码在不同情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。 这类代码也有一个特点：只有在极端情况下，复杂度才为O(1)，大部分情况下都是O(n)。这一点跟均摊时间复杂度是不同的。 均摊时间复杂度均摊时间复杂度对应的分析方法是摊还分析/平摊分析，使用场景非常有限，也很特殊。 123456789101112131415# li是一个长度为n的列表li = [0] * ncount = 0def insert(val): """往数组插入数据""" if (count == len(li)): # 当count等于列表长度时，将列表中的所有数据相加并赋给列表第一个元素，val插入li[1] sum = 0 for i in range(len(li)): sum += sum + li[i] li[0] = sum count = 1 # 如果count未到n，那么直接插入到li[count] li[count] = val; count += 1 insert()方法跟平均情况时间复杂度中的find()有着两处不同的地方： 1、在大部分情况下，时间复杂度都为O(1)，只有个别情况下复杂度才比较高，为O(n)。 2、O(1)时间复杂度的插入和O(n)时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时许关系，一般都是一个O(n)插入之后，紧跟着n-1个O(1)的插入操作，循环往复。 具体分析如下：最理想状态下，就是count&lt;n，这时val直接插入，最好情况时间复杂度为O(1)；最坏的情况，是count=n，需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为O(n)。平均情况复杂度则是O(1)。因为在count&lt;n，共n种情况，每种情况的时间复杂度都是O(1)；最坏情况只有一种，时间复杂度为O(n)，所以平均时间复杂度就是： 针对均摊时间复杂度的特殊性，我们可以使用摊还分析法来分析。就拿刚刚的例子来说，每一次O(n)的插入操作，都会跟着n-1次O(1)的插入操作，所以把耗时多的那次操作均摊到接下来的n-1次耗时少的操作上，那么这一组连续的操作的均摊时间复杂度就是O(1)。 总而言之，对一个数据结构进行一组连续操作中，大部分时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那些操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间就等于最好情况时间复杂度。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>时间复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下mysql安装和链接]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F25%2Fcentos7%E4%B8%8Bmysql%E5%AE%89%E8%A3%85%E5%92%8C%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[最近弄了个虚拟机玩一玩，跟着视频教程安装mysql，结果遇到了蛮多问题的。 安装centOS7（略过）用的是VirtualBox，具体过程以后再整理 删除原有数据库因为CentOS7默认是安装了mariadb数据库，所以需要先删除再安装mysql。 启动虚拟机，登录 1ssh test@192.168.0.108 # test是用户名 删除mariadb 12yum search mariadb # 搜索mariadb，得到具体数据库版本，如mariadb-libs.x86_64sudo yum remove mariadb-libs.x86_64 清除原有mysql版本 12rpm -qa|grep mysqlsudo yum remove xxxx # xxxx是搜索结果中的数据库版本，依次删除 删除mysql配置文件 12find / -name mysql sudo rm -rf xxxx # xxxx是对应的配置文件 安装mysql 下载mysql源 找到mysql下载链接，选择对应的mysql版本后点击‘download’，进入新的页面，右键复制下载链接 12cd /tmpwget https://dev.mysql.com/get/mysql80-community-release-fc28-1.noarch.rpm 安装源 1sudo yum localinstall mysql80-community-release-fc28-1.noarch.rpm 搜索、安装mysql 12yum search mysql sudo yum install mysql-community-server # 根据搜索结果进行安装 安装完成之后，查看服务是否启动 123ps -ef | grep mysql# mysql服务操作相关命令sudo service mysqld restart/start/stop 运行以下命令来生成临时密码 临时密码是..for root@localhost:后面部分 1cat /var/log/mysqld.log | grep password 登录mysql 1mysql -uroot -pabc # abc是刚刚生成的临时密码 修改密码 MySQL8.0的密码规范很严格，需要由数字、大小写字母和特殊符号组成的八位长度，所以没办法设置类似123456这样简单的密码。可以通过调整密码验证规则，修改policy和length的值。不过需要先设置一个符合条件的密码再修改密码验证规则。 1234mysql&gt; set global validate_password.policy=0;mysql&gt; set global validate_password.length=1;mysql&gt; flush privileges; # 更新，需要先设置一个符合条件的密码才能执行该语句mysql&gt; alter user 'root'@'localhost' identified by '123456'; 修改密码之后，要重新登录验证 授权mysql远程链接 设置数据库host为% 电脑上用的mysql客户端是navicat，如下图填写链接信息。 链接很可能会失败了，因为当前用户的host是localhost，也就是只能本机链接，需要修改为’%’ 1234567# 查看用户，mysql库中的user表mysql&gt; use mysql;mysql&gt; select Host, User from user \G;# 修改用户hostmysql&gt; update user set host = '%' where Host = 'localhost' and User = 'root';# 刷新权限mysql&gt; flush privileges; 修改mysql的plugin 12345# 查看mysql&gt; select host, user, plugin, authentication_string from mysql.user \G;# 修改mysql&gt; alter user 'root'@'%' identified with mysql_native_password by '123' # 123是重新设置之后的密码mysql&gt; flush privileges; 设置之后再进行链接测试 查看防火墙之类的的设置 如果上面设置之后还是链接不到本地数据库，很可能是因为设置了防火墙。 12# 关闭防火墙sudo service firewalld stop 开启genelog进入msyql，设置开启日志，方便调试 123mysql&gt; set global general_log_file = '/tmp/general.log'mysql&gt; set global general_log=on;mysql&gt; set global generall_log=off; # 关闭日志 实时查看日志 1tail -f /tmp/generall.log # generalall的路径 新建用户给新用户设置权限的语法也跟mysql5.7有所不同了 12mysql&gt; create user 'username'@'%' identified by 'password'mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'username'@'%' WITH GRANT OPTION; 忘记密码怎么办如果真的忘记密码，可以修改mysql的配置文件，使得mysql不需要验证登录。这样操作其实是很危险的，所以修改完密码之后应该尽快将配置文件恢复 123sudo vim /etc/my.conf# 文件最后加上这样的一句话skip-grant-tables 重新启动服务就可以免密登录msyql 12sudo service mysqld startmysql -uroot -p 修改密码 123mysql&gt; select mysql;mysql&gt; update user set authentication_string=password('1234567') where user='root';mysql&gt; flush privileges; 取消对my.conf文件的修改 参考文章cnetos7安装 how to grant all privileges to root user in MySQL8.0 解决MySQL8.0报错：Unknown system variable ‘validate_password_policy’]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】SQLAlchemy ORM Tutorial for Python Developers]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F19%2F%E8%BD%AC-SQLAlchemy-ORM-Tutorial-for-Python-Developers%2F</url>
    <content type="text"><![CDATA[原文链接 刚开始接触sqlalchemy，以为很方便很好用，结果在各种关系中被打击到不行。无意间在网上看到这篇文章，写得挺通俗易懂的，还有个小例子，良心教程啊！忍不住转载到自己的笔记中，有时间再用自己渣渣的英语简单翻译一下，方便查阅。 In this article, we will learn how to use SQLAlchemy as the ORM (Object Relational Database) library to communicate with relational database engines. First, we will learn about some core concepts of SQLAlchemy (like engines and connection pools), then we will learn how to map Python classes and its relationships to database tables, and finally we will learn how to retrieve (query) data from these tables. The code snippets used in this article can be found in this GitHub repository. SQLAlchemy IntroductionSQLAlchemy is a library that facilitates the communication between Python programs and databases. Most of the times, this library is used as an Object Relational Mapper (ORM) tool that translates Python classes to tables on relational databases and automatically converts function calls to SQL statements. SQLAlchemy provides a standard interface that allows developers to create database-agnostic code to communicate with a wide variety of database engines. As we will see in this article, SQLAlchemy relies on common design patterns (like Object Pools) to allow developers to create and ship enterprise-grade, production-ready applications easily. Besides that, with SQLAlchemy, boilerplate code to handle tasks like database connections is abstracted away to let developers focus on business logic. Before diving into the ORM features provided by SQLAlchemy, we need to learn how the core works. The following sections will introduce important concepts that every Python developer needs to understand before dealing with SQLAlchemy applications. Python DBAPIThe Python DBAPI (an acronym for DataBase API) was created to specify how Python modules that integrate with databases should expose their interfaces. Although we won’t interact with this API directly—we will use SQLAlchemy as a facade to it—it’s good to know that it defines how common functions like connect, close, commit, and rollback must behave. Consequently, whenever we use a Python module that adheres to the specification, we can rest assured that we will find these functions and that they will behave as expected. In this article, we are going to install and use the most popular PostgreSQL DBAPI implementation available: psycopg. Other Python drivers communicate with PostgreSQL as well, but psycopg is the best candidate since it fully implements the DBAPI specification and has great support from the community. To better understand the DBAPI specification, what functions it requires, and how these functions behave, take a look into the Python Enhancement Proposal that introduced it. Also, to learn about what other database engines we can use (like MySQL or Oracle), take a look at the official list of database interfaces available. SQLAlchemy EnginesWhenever we want to use SQLAlchemy to interact with a database, we need to create an Engine. Engines, on SQLAlchemy, are used to manage two crucial factors: Pools and Dialects. The following two sections will explain what these two concepts are, but for now it suffices to say that SQLAlchemy uses them to interact with DBAPI functions. 任何时候，只要我们想用SQLAlchemy去连接数据库，都必须先创建engine。在SQLAlchemy中，engine是用来管理两大重要部分：Pools和Dialects。接下来两小节将会解释这两个概念，现在只要明白SQLAlchemy是用他们跟DBAPI交互的函数就够了。 To create an engine and start interacting with databases, we have to import the create_engine function from the sqlalchemy library and issue a call to it: 为了创建engine并与数据库交互，我们必须从sqlalchemy库中导入create_engine并且调用该方法。 12from sqlalchemy import create_engineengine = create_engine('postgresql://usr:pass@localhost:5432/sqlalchemy') This example creates a PostgreSQL engine to communicate with an instance running locally on port 5432 (the default one). It also defines that it will use usr and pass as the credentials to interact with the sqlalchemy database. Note that, creating an engine does not connect to the database instantly. This process is postponed to when it’s needed (like when we submit a query, or when create/update a row in a table). 这个例子在本地5432（默认端口）端口创建了一个PostgreSQL engine。同时也定义了该engie会使用usr和pass作为与sqlalchemy数据库链接的凭证。需要注意的是，创建了engine并不会马上连接到数据库，而是直到我们需要的时候才连接。（比如，当我们执行查询操作，或者插入/更新表记录）。 Since SQLAlchemy relies on the DBAPI specification to interact with databases, the most common database management systems available are supported. PostgreSQL, MySQL, Oracle, Microsoft SQL Server, and SQLite are all examples of engines that we can use alongside with SQLAlchemy. To learn more about the options available to create SQLAlchemy engines, take a look at the official documentation. sqlalchemy所支持的一些常见数据库管理系统。 SQLAlchemy Connection PoolsConnection pooling is one of the most traditional implementations of the object pool pattern. Object pools are used as caches of pre-initialized objects ready to use. That is, instead of spending time to create objects that are frequently needed (like connections to databases) the program fetches an existing object from the pool, uses it as desired, and puts back when done. 链接池是object pool pattern最传统的实践。对象池被用作预初始化对象准备使用时的缓存。也就是说，程序在需要的时候直接从池中获取一个已存在的对象，用完再放回去，而不是需要链接时再花时间去创建对象（比如链接到数据库）。 The main reason why programs take advantage of this design pattern is to improve performance. In the case of database connections, opening and maintaining new ones is expensive, time-consuming, and wastes resources. Besides that, this pattern allows easier management of the number of connections that an application might use simultaneously. 程序这么设计主要是为了提高性能。在数据库链接的例子中，创建并维持新的对象成本很高，耗时也浪费资源。除此之外，这个设计模式能够更好地控制应用在同一时间能够使用的链接数。 There are various implementations of the connection pool pattern available on SQLAlchemy . For example, creating an Engine through the create_engine()function usually generates a QueuePool. This kind of pool comes configured with some reasonable defaults, like a maximum pool size of 5 connections. sqlalchemy的链接池模式还有很多可用的实践。比如，通过create_engine()方法创建的engine通常会生成一个QueuePool。这类池会有默认的设置，比如最大链接池数为5. As usual production-ready programs need to override these defaults (to fine-tune pools to their needs), most of the different implementations of connection pools provide a similar set of configuration options. The following list shows the most common options with their descriptions: 一般来说，上线程序需要重写默认配置（微调已至符合需求），大部分链接池的不同实现都提供了一系列类似的设置选项(???)。以下是一些常见选项及其描述： pool_size: Sets the number of connections that the pool will handle.设置最大链接池数 max_overflow: Specifies how many exceeding connections (relative to pool_size) the pool supports.指定链接池支持的超出连接数（相对于pool_size） pool_recycle: Configures the maximum age (in seconds) of connections in the pool.设置链接池中最长链接时间（秒） pool_timeout: Identifies how many seconds the program will wait before giving up on getting a connection from the pool.标注程序在放弃从池中获取链接所等待的秒数 To learn more about connection pools on SQLAlchemy, check out the official documentation. SQLAlchemy DialectsAs SQLAlchemy is a facade that enables Python developers to create applications that communicate to different database engines through the same API, we need to make use of Dialects. Most of the popular relational databases available out there adhere to the SQL (Structured Query Language) standard, but they also introduce proprietary variations. These variations are the solely responsible for the existence of dialects. 因为SQLAlchemy能够让python开发者开发出通过同样的API连接到不同的数据库引擎的应用，我们需要使用Dialects。目前主流的关系数据库都遵循SQL（结构化查询语言）标准，但也有各自的变化。这些变化是dialects存在的原因（dialects？？） For example, let’s say that we want to fetch the first ten rows of a table called people. If our data was being held by a Microsoft SQL Server database engine, SQLAlchemy would need to issue the following query: 例如，我们想要获取people表的前十行。如果我们的数据是保存在Microsoft SQL Server数据库引擎，sqlalchemy需要作出以下查询： 1SELECT TOP 10 * FROM people; But, if our data was persisted on MySQL instance, then SQLAlchemy would need to issue: 但是，如果我们的数据持久化为MySQL实例，那么SQLAlchemy则需要这样写： 1SELECT * FROM people LIMIT 10; Therefore, to know precisely what query to issue, SQLAlchemy needs to be aware of the type of the database that it is dealing with. This is exactly what Dialects do. They make SQLAlchemy aware of the dialect it needs to talk. 因此，为了准确知道要做出何种查询，SQLAlchemy需要知道它正在处理的数据库类型。这就是dialects所做的。 On its core, SQLAlchemy includes the following list of dialects: Firebird Microsoft SQL Server MySQL Oracle PostgreSQL SQLite Sybase Dialects for other database engines, like Amazon Redshift, are supported as external projects but can be easily installed. Check out the official documentation on SQLAlchemy Dialects to learn more. SQLAlchemy ORMORM, which stands for Object Relational Mapper, is the specialization of the Data Mapper design pattern that addresses relational databases like MySQL, Oracle, and PostgreSQL. As explained by Martin Fowler in the article, Mappersare responsible for moving data between objects and a database while keeping them independent of each other. As object-oriented programming languages and relational databases structure data on different ways, we need specific code to translate from one schema to the other. ORM，也就是关系对象映射，是关系数据库如MYSQL，Oracle等的数据映射设计模式的专业化。正如Martin Fowler在文章中所解释的那样，Mappers（映射器？）负责在对象和数据库之间移动数据，同时保证它们彼此独立。对于面对对象的编程语言和关系数据库以不同的方式构造数据，我们需要特定的代码将一个模式转换到另一个模式。 For example, in a programming language like Python, we can create a Productclass and an Order class to relate as many instances as needed from one class to another (i.e. Product can contain a list of instances of Order and vice-versa). Though, on relational databases, we need three entities (tables), one to persist products, another one to persist orders, and a third one to relate (through foreign key) products and orders. 例如，在像python这样的编程语言中，我们可以创建一个Product类和一个Order类，以便根据需要将一个类与另一个类相关联（即Product可以包含Order实例列表，反之亦然）。然而，在关系数据库，我们需要三个实体（表），一个保存products，另一个保存orders，第三个链接（通过外键）products和orders。 As we will see in the following sections, SQLAlchemy ORM is an excellent Data Mapper solution to translate Python classes into/from tables and to move data between instances of these classes and rows of these tables. 在接下里的章节里我们将会看到，SQLAlchemy是一个很赞的数据映射解决方法，嗯，翻不下去了。。。 SQLAlchemy Data TypesWhile using SQLAlchemy, we can rest assured that we will get support for the most common data types found in relational databases. For example, booleans, dates, times, strings, and numeric values are a just a subset of the types that SQLAlchemy provides abstractions for. Besides these basic types, SQLAlchemy includes support for a few vendor-specific types (like JSON) and also allows developers to create custom types and redefine existing ones. To understand how we use SQLAlchemy data types to map properties of Python classes into columns on a relation database table, let’s analyze the following example: 为了理解我们是如何使用SQLAlchemy数据类型去映射python类属性到关系数据库表的列的，让我们来分析下面这个例子吧： 1234567class Product(Base): __tablename__ = 'products' id=Column(Integer, primary_key=True) title=Column('title', String(32)) in_stock=Column('in_stock', Boolean) quantity=Column('quantity', Integer) price=Column('price', Numeric) In the code snippet above, we are defining a class called Product that has six properties. Let’s take a look at what these properties do: 在上面的代码片段中，我们定义了一个带有6个属性的Product类。接下来看下这些属性都有什么作用： The __tablename__ property tells SQLAlchemy that rows of the productstable must be mapped to this class. __tablename__属性告诉SQLAlchemy products表的行都必须映射到这个类。 The id property identifies that this is the primary_key in the table and that its type is Integer. id 属性是表的primary_key，类型为Integer。 The title property indicates that a column in the table has the same name of the property and that its type is String. title属性指示表中有一个与该属性（title）同名的列，列的类型是 String。 The in_stock property indicates that a column in the table has the same name of the property and that its type is Boolean. in_stock 属性说明表中也有同名的列，列的类型为 Boolean The quantity property indicates that a column in the table has the same name of the property and that its type is Integer. The price property indicates that a column in the table has the same name of the property and that its type is Numeric. Seasoned developers will notice that (usually) relational databases do not have data types with these exact names. SQLAlchemy uses these types as generic representations to what databases support and use the dialect configured to understand what types they translate to. For example, on a PostgreSQL database, the title would be mapped to a varchar column. 经验丰富的开发者通常会注意到关系型数据库不会有这些确切名称的数据类型。SQLAlchemy使用这些类型作为通用表示形式，以支持和使用配置的方言来理解它们转换为的类型。比如在PostgreSQL数据库里，title将会被映射为varchar列。 SQLAlchemy Relationship PatternsNow that we know what ORM is and have look into data types, let’s learn how to use SQLAlchemy to map relationships between classes to relationships between tables. SQLAlchemy supports four types of relationships: One To Many, Many To One, One To One, and Many To Many. 现在我们了解到什么是ORM和各种数据类型，接下来让我们学习怎么使用SQLAlchemy将类中的关系映射到表之间的关系。SQLAlchemy支持四种关系：一对多，多对一，一对一和多对多。 Note that this section will be an overview of all these types, but in the SQLAlchemy ORM in Practice action we will do a hands-on to practice mapping classes into tables and to learn how to insert, extract, and remove data from these tables. The first type, One To Many, is used to mark that an instance of a class can be associated with many instances of another class. For example, on a blog engine, an instance of the Article class could be associated with many instances of the Comment class. In this case, we would map the mentioned classes and its relation as follows: 第一种，一对多，用来标注一个类的实例能够与另一个类的多个实例相关联。例如，在一个博客上， Article类的一个实例可以对应到 Comment 类的多个实例。在这个例子中，我们像下面这样对刚刚提到的类和关系进行映射： 1234567891011class Article(Base): __tablename__ = 'articles' id = Column(Integer, primary_key=True) comments = relationship("Comment")class Comment(Base): __tablename__ = 'comments' id = Column(Integer, primary_key=True) # 多的一方设置了外键 article_id = Column(Integer, ForeignKey('articles.id')) The second type, Many To One, refers to the same relationship described above but from the other perspective. To give a different example, let’s say that we want to map the relationship between instances of Tire to an instance of a Car. As many tires belong to one car and this car contains many tires, we would map this relation as follows: 第二种类型，多对一，跟上面提到的关系一样，但从反方来说。举个不同的例子，假设我们想要映射Tire多个实例（多个轮胎）和一个Car实例的关系。因为很多条轮胎属于同一辆车并且这辆车拥有多条轮胎，我们可能会这样来映射： 1234567891011class Tire(Base): __tablename__ = 'tires' id = Column(Integer, primary_key=True) # 跟一对多不同的是，这次relationship也是在多的一方，跟外键一起 car_id = Column(Integer, ForeignKey('cars.id')) car = relationship("Car")class Car(Base): __tablename__ = 'cars' id = Column(Integer, primary_key=True) The third type, One To One, refers to relationships where an instance of a particular class may only be associated with one instance of another class, and vice versa. As an example, consider the relationship between a Person and a MobilePhone. Usually, one person possesses one mobile phone and this mobile phone belongs to this person only. To map this relationship on SQLAlchemy, we would create the following code: 第三种类型，一对一是指一个特定类的实例只能关联到另一个类的一个实例这样的关系，反之亦然。举个例子，想象一个人和一个手机之间的关系。通常来说，一个人拥有一个手机，同时这个手机也只属于这个人。在SQLAlchemy映射这样的关系，我们可以来写： 12345678910111213class Person(Base): __tablename__ = 'people' id = Column(Integer, primary_key=True) # back_populates的值是属性的字符串名称，必须在mapper的另一端中显式定义（MobilePhone） # uselist为False，标量。在一对多和多对多中为True，对应是列表（forms a list），在多对一则是scalar mobile_phone = relationship("MobilePhone", uselist=False, back_populates="person")class MobilePhone(Base): __tablename__ = 'mobile_phones' id = Column(Integer, primary_key=True) person_id = Column(Integer, ForeignKey('people.id')) # 同样的，back_populates="mobile_phone"中的mobile_phone也是在Person中定义的了 person = relationship("Person", back_populates="mobile_phone") In this example, we pass two extra parameters to the relationship function. The first one, uselist=False, makes SQLAlchemy understand that mobile_phone will hold only a single instance and not an array (multiple) of instances. The second one, back_populates, instructs SQLAlchemy to populate the other side of the mapping. The official Relationships API documentationprovides a complete explanation of these parameters and also covers other parameters not mentioned here. 在这里例子中，我们传递两个参数给relationship函数。第一个参数uselist=False，告知SQLAlchemy mobile_phone列仅包含单个实例，而不是实例列表（多个实例）。第二个参数， back_populates，指示SQLAlchemy填充到映射的另一端。official Realtionships API documentation提供了这些参数的完整解释，也包括了这里没提到的参数。 The last type supported by SQLAlchemy, Many To Many, is used when instances of a particular class can have zero or more associations to instances of another class. For example, let’s say that we are mapping the relationship of instances of Student and instances of Class in a system that manages a school. As many students can participate in many classes, we would map the relationship as follows: SQLAlchemy所支持的最后一种类型，多对多，被用在当一个特定类的实例（多个）与另一个类的实例有零个或者多个联系的时候。例如，假设我们在学校管理系统中映射 Student 实例和 Class实例。因为很多学生可以参加多个课程，我们可以这样映射： 1234567891011121314students_classes_association = Table('students_classes', Base.metadata, Column('student_id', Integer, ForeignKey('students.id')), Column('class_id', Integer, ForeignKey('classes.id')))class Student(Base): __tablename__ = 'students' id = Column(Integer, primary_key=True) # 为什么只在这边写 classes = relationship("Class", secondary=students_classes_association)class Class(Base): __tablename__ = 'classes' id = Column(Integer, primary_key=True) In this case, we had to create a helper table to persist the association between instances of Student and instances of Class, as this wouldn’t be possible without an extra table. Note that, to make SQLAlchemy aware of the helper table, we passed it in the secondary parameter of the relationship function. 在这种情况下，我们必须创建一个中间表来持久化Student 实例和 Class实例之间的关联，因为如果没有另一张表是没办法实现的。需要注意的是，为了让SQLAlchemy知道中间表的存在，我们将它传递给relationship函数的secondary参数。 The above code snippets show just a subset of the mapping options supported by SQLAlchemy. In the following sections, we are going to take a more in-depth look into each one of the available relationship patterns. Besides that, the official documentation is a great reference to learn more about relationship patterns on SQLAlchemy. SQLAlchemy ORM CascadeWhenever rows in a particular table are updated or deleted, rows in other tables might need to suffer changes as well. These changes can be simple updates, which are called cascade updates, or full deletes, known as cascade deletes. For example, let’s say that we have a table called shopping_carts, a table called products, and a third one called shopping_carts_products that connects the first two tables. If, for some reason, we need to delete rows from shopping_carts we will need to delete the related rows from shopping_carts_products as well. Otherwise we will end up with a lot of garbage and unfulfilled references in our database. 任何时候，某个表中的行被更新了或者删除，其他表格中的行也应该要有相应的变动。这些变化可以是简单的更新，称为级联更新，也可以是完全删除，称为级联删除。例如，假设我们有三张表，分别是shopping_carts表、products表和shopping_carts_products表，第三张表是用来连接前两张表的。如果，因为某些原因，我们从shopping_carts表中删除一些行，我们同样需要在shopping_carts_products表删除与之相关的行。否则，我们的数据库可能会有大量垃圾或者未完成的引用。 To make this kind of operation easy to maintain, SQLAlchemy ORM enables developers to map cascade behavior when using relationship() constructs. Like that, when operations are performed on parent objects, child objects get updated/deleted as well. The following list provides a brief explanation of the most used cascade strategies on SQLAlchemy ORM: 为了让这类操作更易维护，SQLAlchemy ORM允许开发者通过relationship()来映射级联行为。这样一来，在父类上的操作生效后，子类也会跟着更新/删除。以下列表简要说明了SQLAlchemy ORM上最常用的级联策略： save-update: Indicates that when a parent object is saved/updated, child objects are saved/updated as well. 当父对象保存/更新时，子对象也会跟着保存/更新 delete: Indicates that when a parent object is deleted, children of this object will be deleted as well. 当父对象被删除时，该对象的后代也会被删除 delete-orphan: Indicates that when a child object loses reference to a parent, it will get deleted. 当子对象失去对父对象的引用时，就会被删除 merge: Indicates that merge() operations propagate from parent to children. merge()操作从父级传播到子级 If more information about this feature is needed, the SQLAlchemy documentation provides an excellent chapter about Cascades. SQLAlchemy SessionsSessions, on SQLAlchemy ORM, are the implementation of the Unit of Workdesign pattern. As explained by Martin Fowler, a Unit of Work is used to maintain a list of objects affected by a business transaction and to coordinate the writing out of these changes. This means that all modifications tracked by Sessions (Units of Works) will be applied to the underlying database together, or none of them will. In other words, Sessions are used to guarantee the database consistency. The official SQLAlchemy ORM documentation about Sessions gives a great explanation how changes are tracked, how to get sessions, and how to create ad-hoc sessions. However, in this article, we will use the most basic form of session creation: 1234567891011from sqlalchemy import create_enginefrom sqlalchemy.orm import sessionmaker# create an engineengine = create_engine('postgresql://usr:pass@localhost:5432/sqlalchemy')# create a configured "Session" classSession = sessionmaker(bind=engine)# create a Sessionsession = Session() As we can see from the code snippet above, we only need one step to get sessions. We need to create a session factory that is bound to the SQLAlchemy engine. After that, we can just issue calls to this session factory to get our sessions. SQLAlchemy in PracticeNow that we got a better understanding of the most important pieces of SQLAlchemy, it’s time to start practicing it. In the following sections, we will create a small project based on pipenv—a Python dependency manager—and add some classes to it. Then we will map these classes to tables persisted to a PostgreSQL database and learn how to query data. Starting the Tutorial ProjectTo create our tutorial project, we have to have Python installed on our machine and pipenv installed as a global Python package. The following commands will install pipenv and set up the project. These commands are dependent on Python, so be sure to have it installed before proceeding: 1234567891011# install pipenv globallypip install pipenv# create a new directory for our projectmkdir sqlalchemy-tutorial# change working directory to itcd sqlalchemy-tutorial# create a Python 3 projectpipenv --three Running PostgreSQLTo be able to practice our new skills and to learn how to query data on SQLAlchemy, we will need a database to support our examples. As already mentioned, SQLAlchemy provides support for many different databases engines, but the instructions that follow will focus on PostgreSQL. There are many ways to get an instance of PostgreSQL. One of them is to use some cloud provider like Heroku or ElephantSQL (both of them have free tiers). Another possibility is to install PostgreSQL locally on our current environment. A third option is to run a PostgreSQL instance inside a Docker container. The third option is probably the best choice because it has the performance of an instance running locally, it’s free forever, and because it’s easy to create and destroy Docker instances. The only (small) disadvantage is that we need to install Docker locally. After having Docker installed, we can create and destroy dockerized PostgreSQL instances with the following commands: 12345678910111213# create a PostgreSQL instancedocker run --name sqlalchemy-orm-psql \ -e POSTGRES_PASSWORD=pass \ -e POSTGRES_USER=usr \ -e POSTGRES_DB=sqlalchemy \ -p 5432:5432 \ -d postgres# stop instancedocker stop sqlalchemy-orm-psql# destroy instancedocker rm sqlalchemy-orm-psql The first command, the one that creates the PostgreSQL instance, contains a few parameters that are worth inspecting: --name: Defines the name of the Docker instance. -e POSTGRES_PASSWORD: Defines the password to connect to PostgreSQL. -e POSTGRES_USER: Defines the user to connect to PostgreSQL. -e POSTGRES_DB: Defines the main (and only) database available in the PostgreSQL instance. -p 5432:5432: Defines that the local 5432 port will tunnel connections to the same port in the Docker instance. -d postgres: Defines that this Docker instance will be created based on the official PostgreSQL repository. Installing SQLAlchemy DependenciesIn this tutorial, we will need to install only two packages: sqlalchemy and psycopg2. The first dependency refers to SQLAlchemy itself and the second one, psycopg2, is the PostgreSQL driver that SQLAlchemy will use to communicate with the database. To install these dependencies, we will use pipenv as shown: 12# install sqlalchemy and psycopg2pipenv install sqlalchemy psycopg2 This command will download both libraries and make them available in our Python virtual environment. Note that to run the scripts that we are going to create, we first need to spawn the virtual environment shell. That is, before executing python somescript.py, we need to execute pipenv shell. Otherwise, Python won’t be able to find the installed dependencies, as they are just available in our new virtual environment. Mapping Classes with SQLAlchemyAfter starting the dockerized PostgreSQL instance and installing the Python dependencies, we can begin to map Python classes to database tables. In this tutorial, we will map four simple classes that represent movies, actors, stuntmen, and contact details. The following diagram illustrates these entities’ characteristics and their relations. To start, we will create a file called base.py in the main directory of our project and add the following code to it: 12345678910# coding=utf-8from sqlalchemy import create_enginefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import sessionmakerengine = create_engine('postgresql://usr:pass@localhost:5432/sqlalchemy')Session = sessionmaker(bind=engine)Base = declarative_base() This code creates: a SQLAlchemy Engine that will interact with our dockerized PostgreSQL database, a SQLAlchemy ORM session factory bound to this engine, and a base class for our classes definitions. Now let’s create and map the Movie class. To do this, let’s create a new file called movie.py and add the following code to it: 1234567891011121314151617# coding=utf-8from sqlalchemy import Column, String, Integer, Datefrom base import Baseclass Movie(Base): __tablename__ = 'movies' id = Column(Integer, primary_key=True) title = Column(String) release_date = Column(Date) def __init__(self, title, release_date): self.title = title self.release_date = release_date The definition of this class and its mapping characteristics is quite simple. We start by making this class extend the Base class defined in the base.pymodule and then we add four properties to it: A __tablename__ to indicate what is the name of the table that will support this class. An id to represent the primary key in the table. A title of type String. A release_date of type Date. The next class that we will create and map is the Actor class. Let’s create a file called actor.py and add the following code to it: 1234567891011121314151617# coding=utf-8from sqlalchemy import Column, String, Integer, Datefrom base import Baseclass Actor(Base): __tablename__ = 'actors' id = Column(Integer, primary_key=True) name = Column(String) birthday = Column(Date) def __init__(self, name, birthday): self.name = name self.birthday = birthday The definition of this class is pretty similar to the previous one. The differences are that the Actor has a name instead of a title, a birthday instead of a release_date, and that it points to a table called actors instead of movies. As many movies can have many actors and vice-versa, we will need to create a Many To Many relationship between these two classes. Let’s create this relationship by updating the movie.py file as follows: 12345678910111213141516171819202122232425# coding=utf-8from sqlalchemy import Column, String, Integer, Date, Table, ForeignKeyfrom sqlalchemy.orm import relationshipfrom base import Basemovies_actors_association = Table( 'movies_actors', Base.metadata, Column('movie_id', Integer, ForeignKey('movies.id')), Column('actor_id', Integer, ForeignKey('actors.id')))class Movie(Base): __tablename__ = 'movies' id = Column(Integer, primary_key=True) title = Column(String) release_date = Column(Date) actors = relationship("Actor", secondary=movies_actors_association) def __init__(self, title, release_date): self.title = title self.release_date = release_date The difference between this version and the previous one is that: we imported three new entities: Table, ForeignKey, and relationship; we created a movies_actors_association table that connects rows of actors and rows of movies; and we added the actors property to Movie and configured the movies_actors_association as the intermediary table. The next class that we will create is Stuntman. In our tutorial, a particular Actor will have only one Stuntman and this Stuntman will work only with this Actor. This means that we need to create the Stuntman class and a One To One relationship between these classes. To accomplish that, let’s create a file called stuntman.py and add the following code to it: 123456789101112131415161718192021# coding=utf-8from sqlalchemy import Column, String, Integer, Boolean, ForeignKeyfrom sqlalchemy.orm import relationship, backreffrom base import Baseclass Stuntman(Base): __tablename__ = 'stuntmen' id = Column(Integer, primary_key=True) name = Column(String) active = Column(Boolean) actor_id = Column(Integer, ForeignKey('actors.id')) actor = relationship("Actor", backref=backref("stuntman", uselist=False)) def __init__(self, name, active, actor): self.name = name self.active = active self.actor = actor In this class, we have defined that the actor property references an instance of Actor and that this actor will get a property called stuntman that is not a list (uselist=False). That is, whenever we load an instance of Stuntman, SQLAlchemy will also load and populate the Actor associated with this stuntman. The fourth and final class that we will map in our tutorial is ContactDetails. Instances of this class will hold a phone_number and an address of a particular Actor, and one Actor will be able to have many ContactDetails associated. Therefore, we will need to use the Many To One relationship pattern to map this association. To create this class and this association, let’s create a file called contact_details.py and add the following source code to it: 123456789101112131415161718192021# coding=utf-8from sqlalchemy import Column, String, Integer, ForeignKeyfrom sqlalchemy.orm import relationshipfrom base import Baseclass ContactDetails(Base): __tablename__ = 'contact_details' id = Column(Integer, primary_key=True) phone_number = Column(String) address = Column(String) actor_id = Column(Integer, ForeignKey('actors.id')) actor = relationship("Actor", backref="contact_details") def __init__(self, phone_number, address, actor): self.phone_number = phone_number self.address = address self.actor = actor As we can see, creating a Many To One association is kinda similar to creating a One To One association. The difference is that in the latter we instructed SQLAlchemy not to use lists. This instruction ends up restricting the association to a single instance instead of a list of instances. Persisting Data with SQLAlchemy ORMNow that we have created our classes, let’s create a file called inserts.py and generate some instances of these classes to persist to the database. In this file, let’s add the following code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# coding=utf-8# 1 - importsfrom datetime import datefrom actor import Actorfrom base import Session, engine, Basefrom contact_details import ContactDetailsfrom movie import Moviefrom stuntman import Stuntman# 2 - generate database schemaBase.metadata.create_all(engine)# 3 - create a new sessionsession = Session()# 4 - create moviesbourne_identity = Movie("The Bourne Identity", date(2002, 10, 11))furious_7 = Movie("Furious 7", date(2015, 4, 2))pain_and_gain = Movie("Pain &amp; Gain", date(2013, 8, 23))# 5 - creates actorsmatt_damon = Actor("Matt Damon", date(1970, 10, 8))dwayne_johnson = Actor("Dwayne Johnson", date(1972, 5, 2))mark_wahlberg = Actor("Mark Wahlberg", date(1971, 6, 5))# 6 - add actors to moviesbourne_identity.actors = [matt_damon]furious_7.actors = [dwayne_johnson]pain_and_gain.actors = [dwayne_johnson, mark_wahlberg]# 7 - add contact details to actorsmatt_contact = ContactDetails("415 555 2671", "Burbank, CA", matt_damon)dwayne_contact = ContactDetails("423 555 5623", "Glendale, CA", dwayne_johnson)dwayne_contact_2 = ContactDetails("421 444 2323", "West Hollywood, CA", dwayne_johnson)mark_contact = ContactDetails("421 333 9428", "Glendale, CA", mark_wahlberg)# 8 - create stuntmenmatt_stuntman = Stuntman("John Doe", True, matt_damon)dwayne_stuntman = Stuntman("John Roe", True, dwayne_johnson)mark_stuntman = Stuntman("Richard Roe", True, mark_wahlberg)# 9 - persists datasession.add(bourne_identity)session.add(furious_7)session.add(pain_and_gain)session.add(matt_contact)session.add(dwayne_contact)session.add(dwayne_contact_2)session.add(mark_contact)session.add(matt_stuntman)session.add(dwayne_stuntman)session.add(mark_stuntman)# 10 - commit and close sessionsession.commit()session.close() This code is split into 10 sections. Let’s inspect them: The first section imports the classes that we created, the SQLAlchemy engine, the Base class, the session factory, and date from the datetimemodule. The second section instructs SQLAlchemy to generate the database schema. This generation occurs based on the declarations that we made while creating the four main classes that compose our tutorial. The third section extracts a new session from the session factory. The fourth section creates three instances of the Movie class. The fifth section creates three instances of the Actor class. The sixth section adds actors to movies. Note that the Pain &amp; Gain movie references two actors: Dwayne Johnson and Mark Wahlberg. The seventh section creates instances of the ContactDetails class and defines what actors these instances are associated to. The eighth section defines three stuntmen and also defines what actors these stuntmen are associated to. The ninth section uses the current session to save the movies, actors, contact details, and stuntmen created. Note that we haven’t explicitly saved actors. This is not needed because SQLAlchemy, by default, uses the save-update cascade strategy. The tenth section commits the current session to the database and closes it. To run this Python script, we can simply issue the python inserts.pycommand (let’s not to run pipenv shell first) in the main directory of our database. Running it will create five tables in the PostgreSQL database and populate these tables with the data that we created. In the next section, we will learn how to query these tables. Querying Data with SQLAlchemy ORMAs we will see, querying data with SQLAlchemy ORM is quite simple. This library provides an intuitive, fluent API that enables developers to write queries that are easy to read and to maintain. On SQLAlchemy ORM, all queries start with a Query Object that is extracted from the current session and that is associated with a particular mapped class. To see this API in action, let’s create a file called queries.py and add to it the following source code: 12345678910111213141516171819# coding=utf-8# 1 - importsfrom actor import Actorfrom base import Sessionfrom contact_details import ContactDetailsfrom movie import Movie# 2 - extract a sessionsession = Session()# 3 - extract all moviesmovies = session.query(Movie).all()# 4 - print movies' detailsprint('\n### All movies:')for movie in movies: print(f'&#123;movie.title&#125; was released on &#123;movie.release_date&#125;')print('') The code snippet above—that can be run with python queries.py,—shows how easy it is to use SQLAlchemy ORM to query data. To retrieve all movies from the database, we just needed to fetch a session from the session factory, use it to get a query associated with Movie, and then call the all() function on this query object. The Query API provides dozens of useful functions like all(). In the following list, we can see a brief explanation about the most important ones: count(): Returns the total number of rows of a query. filter(): Filters the query by applying a criteria. delete(): Removes from the database the rows matched by a query. distinct(): Applies a distinct statement to a query. exists(): Adds an exists operator to a subquery. first(): Returns the first row in a query. get(): Returns the row referenced by the primary key parameter passed as argument. join(): Creates a SQL join in a query. limit(): Limits the number of rows returned by a query. order_by(): Sets an order in the rows returned by a query. To explore the usage of some of these functions, let’s append the following code to the queries.py script: 123456789101112131415161718192021222324252627282930313233343536# 1 - importsfrom datetime import date# other imports and sections...# 5 - get movies after 15-01-01movies = session.query(Movie) \ .filter(Movie.release_date &gt; date(2015, 1, 1)) \ .all()print('### Recent movies:')for movie in movies: print(f'&#123;movie.title&#125; was released after 2015')print('')# 6 - movies that Dwayne Johnson participatedthe_rock_movies = session.query(Movie) \ .join(Actor, Movie.actors) \ .filter(Actor.name == 'Dwayne Johnson') \ .all()print('### Dwayne Johnson movies:')for movie in the_rock_movies: print(f'The Rock starred in &#123;movie.title&#125;')print('')# 7 - get actors that have house in Glendaleglendale_stars = session.query(Actor) \ .join(ContactDetails) \ .filter(ContactDetails.address.ilike('%glendale%')) \ .all()print('### Actors that live in Glendale:')for actor in glendale_stars: print(f'&#123;actor.name&#125; has a house in Glendale')print('') The fifth section of the updated script uses the filter() function to fetch only movies that were released after January the first, 2015. The sixth section shows how to use join() to fetch instances of Movie that the Actor Dwayne Johnson participated in. The seventh and last section, shows the usage of join() and ilike() functions to retrieve actors that have houses in Glendale. Running the new version of the script (python queries.py) now will result in the following output: 123456789101112131415### All movies:The Bourne Identity was released on 2002-10-11Furious 7 was released on 2015-04-02No Pain No Gain was released on 2013-08-23### Recent movies:Furious 7 was released after 2015### Dwayne Johnson movies:The Rock starred in No Pain No GainThe Rock starred in Furious 7### Actors that live in Glendale:Dwayne Johnson has a house in GlendaleMark Wahlberg has a house in Glendale As we can see, using the API is straightforward and generates a code that is readable. To see other functions supported by the Query API, and their description, take a look at the official documentation. “Querying data with SQLAlchemy ORM is easy and intuitive.”TWEET THIS Securing Python APIs with Auth0Securing Python APIs with Auth0 is very easy and brings a lot of great features to the table. With Auth0, we only have to write a few lines of code to get: A solid identity management solution, including single sign-on User management Support for social identity providers (like Facebook, GitHub, Twitter, etc.) Enterprise identity providers (Active Directory, LDAP, SAML, etc.) Our own database of users For example, to secure Python APIs written with Flask, we can simply create a requires_auth decorator: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# Format error response and append status codedef get_token_auth_header(): """Obtains the access token from the Authorization Header """ auth = request.headers.get("Authorization", None) if not auth: raise AuthError(&#123;"code": "authorization_header_missing", "description": "Authorization header is expected"&#125;, 401) parts = auth.split() if parts[0].lower() != "bearer": raise AuthError(&#123;"code": "invalid_header", "description": "Authorization header must start with" " Bearer"&#125;, 401) elif len(parts) == 1: raise AuthError(&#123;"code": "invalid_header", "description": "Token not found"&#125;, 401) elif len(parts) &gt; 2: raise AuthError(&#123;"code": "invalid_header", "description": "Authorization header must be" " Bearer token"&#125;, 401) token = parts[1] return tokendef requires_auth(f): """Determines if the access token is valid """ @wraps(f) def decorated(*args, **kwargs): token = get_token_auth_header() jsonurl = urlopen("https://"+AUTH0_DOMAIN+"/.well-known/jwks.json") jwks = json.loads(jsonurl.read()) unverified_header = jwt.get_unverified_header(token) rsa_key = &#123;&#125; for key in jwks["keys"]: if key["kid"] == unverified_header["kid"]: rsa_key = &#123; "kty": key["kty"], "kid": key["kid"], "use": key["use"], "n": key["n"], "e": key["e"] &#125; if rsa_key: try: payload = jwt.decode( token, rsa_key, algorithms=ALGORITHMS, audience=API_AUDIENCE, issuer="https://"+AUTH0_DOMAIN+"/" ) except jwt.ExpiredSignatureError: raise AuthError(&#123;"code": "token_expired", "description": "token is expired"&#125;, 401) except jwt.JWTClaimsError: raise AuthError(&#123;"code": "invalid_claims", "description": "incorrect claims," "please check the audience and issuer"&#125;, 401) except Exception: raise AuthError(&#123;"code": "invalid_header", "description": "Unable to parse authentication" " token."&#125;, 400) _app_ctx_stack.top.current_user = payload return f(*args, **kwargs) raise AuthError(&#123;"code": "invalid_header", "description": "Unable to find appropriate key"&#125;, 400) return decorated Then use it in our endpoints: 1234567891011121314# Controllers API# This doesn't need authentication@app.route("/ping")@cross_origin(headers=['Content-Type', 'Authorization'])def ping(): return "All good. You don't need to be authenticated to call this"# This does need authentication@app.route("/secured/ping")@cross_origin(headers=['Content-Type', 'Authorization'])@requires_authdef secured_ping(): return "All good. You only get this message if you're authenticated" To learn more about securing Python APIs with Auth0, take a look at this tutorial. Alongside with tutorials for backend technologies (like Python, Java, and PHP), the Auth0 Docs webpage also provides tutorials for Mobile/Native appsand Single-Page applications. Next StepsWe have covered a lot of ground in this article. We’ve learned about basic SQLAlchemy concepts like Engines, Connection Pools, and Dialects. After that, we’ve learned about how SQLAlchemy addresses ORM topics like Relationship Patterns, Cascade strategies, and the Query API. In the end, we applied this knowledge in a small exercise. In summary, we had the chance to learn and practice the most important pieces of SQLAlchemy and SQLAlchemy ORM. In the next article, we are going to use these new skills to implement RESTful APIs with Flask—the Python microframework for the web. Stay tuned!]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests自定义retries]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F17%2Frequests%E8%87%AA%E5%AE%9A%E4%B9%89retries%2F</url>
    <content type="text"><![CDATA[有时候在发起请求时会因为网络原因而导致请求失败，总是通过捕获错误来处理也是挺烦的。在网上搜索了一番，找到这么个方法，挺不错的，所以在此基础上进行了修改。 123456789101112131415161718192021222324252627282930313233343536import requestsfrom requests.adapters import HTTPAdapterfrom urllib3.util.retry import Retrybase_header = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',&#125;def request_retry_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504), session=None): """遇到网络问题最多重试三次""" session = session or requests.session() retry = Retry(total=retries, backoff_factor=backoff_factor, status_forcelist=status_forcelist) adapter = HTTPAdapter(max_retries=retry) session.mount('http', adapter=adapter) session.mount('https', adapter=adapter) return sessiondef get_page(url, method='get', params=None, options=&#123;&#125;): """请求页面""" headers = dict(base_header, **options) session = request_retry_session() print('正在抓取：', url) try: if method == 'get': response = session.get(url, params=params, headers=headers) elif method == 'post': response = session.post(url, data=params, headers=headers) if response.status_code == 200: return response except Exception as e: print('网页请求失败: ', e) return None 使用的时候，也是很简单的 123url = 'https://httpbin.org/post'html = get_page(url, method='post', params=&#123;'name': 'me'&#125;)print(html.text) Done!]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>requests</tag>
        <tag>retry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python处理多个sheet]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F17%2Fpython%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AAsheet%2F</url>
    <content type="text"><![CDATA[使用python处理excel表中多个sheet的数据时，可以使用xlrd读取数据。 12345678910111213141516171819import pandas as pdimport xlrdfrom pandas import DataFramefile = '临时表.xls'# 处理数据，合并多个sheet，同时去除气化率为空的rowwb = xlrd.open_workbook(file)# 获取全部的工作簿sheets = wb.sheets()df = DataFrame()# 从第10个sheet开始合并for i in range(10, len(sheets)): # header=1，那么数据会从第二行开始读取 temp_df = pd.read_excel(file, sheet_name=i, header=1, index=False) # how='all'：只要存在数据为空的行都删除，还有其他的方法 dropna_df = temp_df.dropna(how='all')[:-2] df = df.append(dropna_df)df.head()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过经纬度获取详细地址]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F14%2F%E9%80%9A%E8%BF%87%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%8E%B7%E5%8F%96%E8%AF%A6%E7%BB%86%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[如题，想要通过经纬度知道附近都有哪些地点，这个可以通过高德地图提供的地理逆地理编码API和shapely库来获取数据。不开心的是，其实高德地图都提供了合适的端口可以直接拿到数据，眼瞎没看到，哎～不过了解到其他的知识也是很开心的 申请key要使用高德地图的开放平台，需要先申请key。 应用管理-&gt;创建新应用-&gt;填写应用相关信息，包括应用名称、用途（限制了url链接的可使用范围）。 获取地图边界如果是需要整个城市或者区的的边界，高德地图有API可以直接调用。如果是需要某块区域，可以使用高德地图的坐标拾取器，也非常方便。 在浏览器打开高德地图-&gt;选择底部的“开放平台”-&gt;进入开放平台后-&gt;选择”地图工具”-&gt;选择”坐标拾取器”，然后就可以在坐标上根据实际需求选择范围并获得对应的坐标。 比如有这样一张图需要获取边界，就可以大概对比着来点点点啦！ 123456789101112# 拿到的边界坐标值origin_points = [(110.987984, 21.506764), (110.990366, 21.514026), (110.993027, 21.530474), (110.999378, 21.535085), (110.998027, 21.540195), (111.01324, 21.542869), (111.025149, 21.543069), (111.026286, 21.535843), (111.035492, 21.524246), (111.01309, 21.499532), (111.005751, 21.491027), (110.995752, 21.494741), (110.987984, 21.506764)]# 处理数据origin_points = list(map(lambda x: (int(x[0] * 1000000), int(x[1] * 1000000)), origin_points))max_long = max([a[0] for a in origin_points])min_long = min([a[0] for a in origin_points])max_lat = max([a[1] for a in origin_points])min_lat = min([a[1] for a in origin_points]) 效果如下： 获取符合条件的坐标点 因为要抓取的区域比较小，所以每个点之间的距离也尽可能的小，不过这样拿到的数据就会有很多重复的。关于这一块的计算是参考L同学_的。 123456789101112131415# 获取所有可能的坐标点# 纬度1度为111.3195km，经度1度是111.319x(cos 0)。每隔1k米取一个点，那么任意两个地点之间的最大距离为2km，# 也就是正方形对角线的距离必须小于等于2。根据勾股定理得到x=1.414214km，有六位小数，所以取1414214x = 100000 # 数据量太少，取个100米吧t = round(111.319 * cos(21 / 180 * pi), 4) # 以电白区中心来计算经纬度，也就是经度1度对应的长度103.925239kmg = round((max_long - min_long) * t / x) # 该地区总纬度差可以划分为几个数据点g1 = [round(min_long + i * x / t) for i in range(g)]h = round((max_lat - min_lat) * 111.3195 / x)h1 = [round(min_lat + i * x / t) for i in range(h)]coords = []while g1: g11 = g1.pop() for j in h1: coords.append((g11, j)) 因为纬度1°为111.3195km，经度每度为111.3195cos(纬度)，遍历之后得到以下的数据（蓝色范围）。经过筛选之后，只有红色点的数据满足。 123456789101112131415# 筛选数据 poly = MultiPoint(origin_points).convex_hull res = [] for p in coords: if poly.contains(Point(p)): res.append(p) print(res) # 画图 long = [a[0] for a in origin_points] lat = [a[1] for a in origin_points] plt.plot(long, lat) # 地区范围曲线 plt.scatter([i[0] for i in coords], [i[1] for i in coords]) # 分割后的散点图 plt.scatter([i[0] for i in res], [i[1] for i in res], edgecolors='red') # 符合条件的点 plt.show() 获取结果根据高德地图提供的API构建请求链接，然后发起请求就可以啦。具体需求可以查看手册进行设置，比如只需要抓取中餐厅，就可以设置poitype=&quot;中餐厅&quot;，详见poi类型. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# -*- coding:utf-8 -*-# Time: 2018/9/13 16:54import jsonfrom pprint import pprintimport osimport pandas as pdimport openpyxlfrom pandas import ExcelWriterimport requestsfrom points import get_pointfile = '高德地图坐标.xlsx'def parse_info(data): res = [] try: if data['info'] == 'OK': for item in data['regeocode']['pois']: d = &#123; 'name': item['name'], 'lon': parse_location(item['location'])[0], 'lat': parse_location(item['location'])[1], 'address': item['address'], &#125; res.append(d) save(res) except Exception as e: print(e)def parse_location(s): return s.split(',')def run(): gd_url = 'https://restapi.amap.com/v3/geocode/regeo?output=json&amp;location=&#123;long&#125;,&#123;lat&#125;&amp;key=&#123;key&#125;&amp;radius=&#123;radius&#125;&amp;extensions=all' key = '47939581b9d62ed52efa9e875c137a13' radius = 100 # 每个坐标点搜索半径 for lon, lat in get_point(): print(lon, lat) html = requests.get(gd_url.format(long=lon, lat=lat, radius=radius, key=key)) print(html.text) data = json.loads(html.text, encoding='utf8') parse_info(data)def save(data): columns = ['name', 'lon', 'lat', 'address'] df = pd.DataFrame(data, columns=columns) exist = os.path.exists(file) if not exist: writer = ExcelWriter(file, engine='openpyxl') df.to_excel(writer, sheet_name='Main', header=True, index=False, columns=columns) writer.save() else: book = openpyxl.load_workbook(file) writer = ExcelWriter(file, engine='openpyxl') writer.book = book row = book['Main'].max_row writer.sheets = dict((ws.title, ws) for ws in book.worksheets) df.to_excel(writer, sheet_name='Main', startrow=row, index=False, header=False, columns=columns) writer.save()if __name__ == '__main__': run()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>高德地图</tag>
        <tag>shapely</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[why are you late]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F11%2Fwhy-are-you-late%2F</url>
    <content type="text"><![CDATA[reasons for being lateThere are various reasons for being late: didn’t hear the alarm She overslept because she didn’t hear her alarm. I slept right through the alarm.(sleep through = oversleep) the alarm didn’t go off If the alarm doesn’t go off, you probably won’t get up until you naturally wake up. underestimate the time you need You may think you only need 20 minutes to finish a task, but actually it can take up to 40 minutes. I left my phone at home and had to go back. bad sense of direction I went the wrong way Actually people can easily get lost when they go to a new place. traffic jam Sometimes, even though we leave home early and go the right way, we still end up being late because of the heavy traffic. I got stuck/caught in traffic. (我堵在路上了) There was a traffic jam. vehicle Bus No.38 was supposed to come at 8:40, but it was 5 minutes late. My flight was delayed and I missed my meeting. My car broke down. I couldn’t find a parking space. what to do when you’re lateAs the saying goes, better than late than never. you should call the people and apologize. I’m sorry, but I am running late. I’m afraid that I’m going to be late for work/the meeting/the interview. tell they the reasons for being late person reasons I overslept. I went the wrong way. I left my phone at home and had to go back. objective reasons There was a traffic jam. My flight was delayed. The subway broke down. inform the people how late you are going to be I’ll be there in 5 minutes. It’s hard to say, but I’ll be there as soon as possible. make up for mistakes late for work Sorry, I was late. This won’t happen again. I’ll finish my work on time. I’m terribly sorry. I won’t be late again. I’ll be working late tonight. late for job interview I’m sorry, but I’m normally on time. late for a meeting Try to make the most of your time Review the meeting minutes. late for date Dinner’s on me. I will buy you lunch. I’ll treat you to a movie. how to avoid being late Put your alarm on the other side of room. Set multiple alarms. Leave home early. To plan the route in advance. Put your bag the night before. Set out the clothes in advance.(提前准备好衣物)]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>英语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python下载哔哩哔哩视频]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F11%2F%E7%94%A8python%E4%B8%8B%E8%BD%BD%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9%E8%A7%86%E9%A2%91%2F</url>
    <content type="text"><![CDATA[没想到我还从来没用python下载过视频，这次决定拿B站来练习一下。 用开发者工具查看页面元素，也没找到视频的真实链接，只好用charles抓包一下。具体的下载和证书安装过程忽略。 设置charles 设置proxy settings 设置ssl proxy settings，抓取https链接，不然会出现乱码 设置macOS Proxy，抓取PC端数据 4.刷新视频页面，应该就会出现很多链接了。仔细查找一下，就可以找到视频链接 下载视频找到视频链接后，以为就可发起请求了，结果把自己给坑了一下。因为视频链接的请求参数有个别一直在变化，并且在全局搜索的时候也没找到生成参数的方法。倒是在页面源码里可以找到链接，还是get请求，嗯，还是应该感到开心的😅 12345678910111213141516171819202122232425262728293031323334353637383940414243def download_video(): headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36', &#125; video_url = 'https://www.bilibili.com/video/av28518492' res = requests.get(video_url, headers=headers, verify=False) origin_txt = re.findall(r'&lt;script&gt;window.__playinfo__=(\&#123;.*?\&#125;)&lt;/script&gt;', res.text, re.S)[0] origin_json = json.loads(origin_txt, encoding='utf-8') urls = origin_json['durl'] size = 0 chunk = 1024 content_size = sum([i['size'] for i in urls]) print('file size is %0.2f MB' % (content_size / chunk / 1024)) # 创建存放视频片段的临时文件夹 if not os.path.exists('temp_video'): os.makedirs('temp_video') start = time.time() # 循环下载视频 for i, data in enumerate(urls): url = data['url'] header = &#123; 'Origin': 'https://www.bilibili.com', 'Referer': video_url, &#125; headers.update(header) try: response = requests.get(url, headers=headers, verify=False, stream=True) video_path = 'temp_video/' + '&#123;&#125;.mp4'.format(i) # 下载视频 with open(video_path, 'wb') as file: for item in response.iter_content(chunk): file.write(item) # 写入视频 file.flush() # 清空缓存 size += len(item) print('\r' + '[下载进度]：%s %0.2f%%' % ('&gt;' * int(size * 50 / content_size), float(size / content_size) * 100), end='') # end=‘’不换行打印 except Exception as e: print(e) stop = time.time() print('\n' + '视频下载完成，耗时%.2f秒' % (stop-start)) 合并视频下载下来的视频，其实是分段的，不方面查看，所以还需要将视频给合并起来，这里用的是ffmpeg命令，最后合并的视频保存为output.mp4，存在当前路径下。 ffmpeg -f concat -safe 0 -i file.txt -c copy output.mp4 其中file.txt是所有视频片段路径，格式如下： 123file &apos;video/v_1.mp4&apos;file &apos;video/v_2.mp4&apos;file &apos;video/v_3.mp4&apos; 合并视频是用subprocess模块来运行ffmpeg命令，详细代码如下： 1234567891011121314151617181920212223242526272829303132def concatenate(path, dest='video'): """ 将给定路径下的视频进行合并，同时删除原本的视频 :param path: 需要合并的视频所在文件夹名字，一般是视频名字 :param dest: 合并之后的视频存放路径，默认为video文件夹 :return: """ with open('file.txt', 'a', encoding='utf-8') as f: for root, dirs, files in os.walk(path): for file in files: # 如果给定的路径下有视频，则将视频路径信息写入到txt中 if os.path.splitext(file)[1] in ['.flv', '.mkv', '.mp4']: video_path = os.path.join(root, file) line = "file '&#123;&#125;'\n".format(video_path) f.writelines(line) # 合并视频 if os.path.exists('file.txt'): if not os.path.exists(dest): os.makedirs(dest) video_save_path = os.path.join(dest, path) try: print('开始合并视频...') print(path) ffmpeg_command = ["ffmpeg", "-f", "concat", "-safe", "0", "-i", "file.txt", "-c", "copy", video_save_path + ".mp4"] subprocess.run(ffmpeg_command) subprocess.run(["rm", "file.txt"]) subprocess.run(["rm", "-r", path]) print('视频合并完成！') except Exception as e: print('视频合并失败') print(e) 代码示例 参考文章How to join two video files using Python? python爬虫抓取B站小视频排行榜]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>B站</tag>
        <tag>视频下载</tag>
        <tag>ffmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python生成二维码]]></title>
    <url>%2Famberwest.github.io%2F2018%2F09%2F06%2F%E7%94%A8python%E7%94%9F%E6%88%90%E4%BA%8C%E7%BB%B4%E7%A0%81%2F</url>
    <content type="text"><![CDATA[用python生成二维码真是简单又方便，现在简单记录一下qrcode库的使用。 安装依赖库1pip install qrcode 使用 使用qr命令 安装了qrcode之后，就可以用一句命令生成二维码，很方便有没有！！！ 1qr "hello world" &gt; test.png 用脚本实现 即使用脚本，也可以一行代码搞定 12img = qrcode.make('use make method')img.save('make.png') 高级用法如果想要更多的设置，可以使用QRCode类 123456789101112qr = qrcode.QRCode( version=1, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=10, border=4, )qr.add_data('use QRCode')qr.make(fit=True)img = qr.make_image(fill_color='blue', back_color='white') img.save('QRCode.png') 参数说明 version：1到40的整数，用来控制二维码的大小，最小是1。生成二维码时设置为none并使用fit参数，version默认为1 rror_correction：控制二维码使用过程中的容错能力 ERROR_CORRECT_L：约7%或者更少的错误能够被修正 ERROR_CORRECT_M：默认值。约15%或者更少的错误能够被修正 ERROR_CORRECT_Q：约25%或者更少的错误能够被修正 ERROR_CORRECT_H：约30%或者更少的错误能够被修正 box_size：控制二维码每个“box“包含的像素数 border：默认为4，最小值。控制二维码边框有多少个“box” svg还可以保存为svg格式，既可以使用命令行，也可以在代码中实现 123qr --factory=svg-path "svg" &gt; test.svgqr --factory=svg "Some text" &gt; test.svgqr --factory=svg-fragment "Some text" &gt; test.svg 在python中，用法如下 12345678910111213141516def make_svg(method): if method == 'basic': # simple factory, just a set of rects factory = qrcode.image.svg.SvgImage elif method == 'fragment': # Fragment factory (also just a set of rects) factory = qrcode.image.svg.SvgFragmentImage else: # Combined path factory, fixes white space that may occur when zooming factory = qrcode.image.svg.SvgPathImage img = qrcode.make('svg', image_factory=factory) img.save('test.svg')if __name__ == '__main__': make_svg('basic') 参考文章qrcode]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>二维码</tag>
        <tag>qrcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片列表布局]]></title>
    <url>%2Famberwest.github.io%2F2018%2F08%2F10%2F%E5%9B%BE%E7%89%87%E5%88%97%E8%A1%A8%E5%B8%83%E5%B1%80%2F</url>
    <content type="text"><![CDATA[今天要解决的问题就是：图片水平垂直居中，同时还要在外面加上边框。最终效果图如下： html代码如下： 1234567891011121314151617&lt;div class="pic"&gt; &lt;div class="content"&gt; &lt;div class="out"&gt; &lt;a href="#"&gt;&lt;img src="../day9-11/images/Facebook.png" alt="facebook"&gt;&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="content"&gt; &lt;div class="out"&gt; &lt;a href="#"&gt;&lt;img src="../day9-11/images/Twitter.png" alt="twitter"&gt;&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="content"&gt; &lt;div class="out"&gt; &lt;a href="#"&gt;&lt;img src="../day9-11/images/Instagram.png" alt="instagram"&gt;&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 由于图片的大小不太一样，这里强制性设置为统一宽高。 1234img &#123; height: 20px; width: 20px;&#125; 为了让三个图片能并排，需要设置display: inline-block，同时也要给容器content宽高。图片边框是个圆形的，这里有个技巧，就是让边框的radius为50%。 12345678910.content &#123; display: inline-block; margin-left: 15px; position: relative; width: 50px; height: 50px; /*设置边框*/ border: 1px solid #acacac; border-radius: 50%;&#125; 因为图片是在边框中间位置，所以我们再用一个容器div.out把内容包起来，设置absolute，并将其top/left设置为父容器content的一半。根据参考文章，这里就是利用absolute的包裹性来获取包含块的宽度，也就是img的大小。这样就把整个图片的左上角对其到容器out的中心点，此时图片并没有居中，因此还要设置margin-top/margin-left为-10px，也就是图片宽高的一半。 1234567.out &#123; position: absolute; left: 50%; top: 50%; margin-left: -10px; margin-top: -10px;&#125; 完整css代码如下： 1234567891011121314151617181920212223242526272829303132333435* &#123; margin: 0; padding: 0;&#125;.pic &#123; background-color: black; height: 80px; width: 250px; padding-top: 20px; margin: 20px auto;&#125;.content &#123; display: inline-block; margin-left: 15px; position: relative; width: 50px; height: 50px; border: 1px solid #acacac; border-radius: 50%;&#125;.out &#123; position: absolute; left: 50%; top: 50%; margin-left: -10px; margin-top: -10px;&#125;img &#123; height: 20px; width: 20px;&#125; 如果图片/内容的大小不确定，就像参考文章中提到的一样，在需要居中的元素外面套一个空的div元素，再重新设置一下即可。如下： html代码： 123456&lt;div class="content"&gt; &lt;div class="out"&gt; &lt;div class="in"&gt;&lt;a href="#"&gt;&lt;img src="../day9-11/images/Facebook.png" alt="facebook"&gt;&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; css修改部分如下，其他的不变： 12345678910111213141516.out &#123; position: absolute; left: 50%; top: 50%;&#125;img &#123; height: 20px; width: 20px;&#125;.in &#123; /*这里的比例可能需要根据内部元素的宽高比进行调整*/ margin-left: -50%; margin-top: -50%;&#125; 参考文章： div+css图片列表布局（二） 深入理解CSS中的margin负值]]></content>
      <tags>
        <tag>css</tag>
        <tag>图片</tag>
        <tag>布局</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鼠标经过图片时增加文字]]></title>
    <url>%2Famberwest.github.io%2F2018%2F08%2F09%2F%E9%BC%A0%E6%A0%87%E7%BB%8F%E8%BF%87%E5%9B%BE%E7%89%87%E6%97%B6%E5%A2%9E%E5%8A%A0%E6%96%87%E5%AD%97%2F</url>
    <content type="text"><![CDATA[在前端学习过程中，遇到这样一个需求，鼠标停在图片上时显示文字介绍和社交图标等。前后效果如下所示： 这里主要是用到了position属性。本文用一个简单的例子来记录一下该效果的实现。 将文字浮在图片上HTML代码如下： 123456&lt;div class="content"&gt; &lt;img src="https://scontent-sea1-1.cdninstagram.com/vp/26fafc3889d4437c79ba872a8260cb1d/5B9DF8FD/t51.2885-15/s480x480/e35/c0.123.1080.1080/30079276_1572484689529899_7245510271010078720_n.jpg" alt="song"&gt; &lt;div class="cover"&gt; Aimee Song &lt;/div&gt;&lt;/div&gt; 图片和文字是同级的，都在content盒子里。要实现文字浮于图片上，需要设置父盒子position属性为relative，文字对应盒子为absolute，也就是相对于父盒子的绝对定位，这样才能使文字浮起来。同时还需要设置inline-block属性，这样content下的元素既可以拥有block元素可以设置width和height的特性，还能保持inline元素不换行的特性。 css设置如下： 123456789.content &#123; position: relative; display: inline-block;&#125;.cover&#123; position: absolute; /*bottom设置文字处于图片的什么位置，相应的还有top, left, right可以设置*/ bottom: 50px;&#125; 鼠标悬浮上一步实现了文字置于图片之前的静态效果，现在使用css的hover来实现鼠标移动到父级身上时，显示文字。所以需要先将cover元素隐藏，新增的css代码如下： 123456.cover&#123; display: none;&#125;.content:hover .cover&#123; display: block;&#125; 到这里就可以完成啦！当然，也可以对文字增加样式，比如添加背景颜色，设置字体大小之类的。具体请查看demo。 参考文献：鼠标悬浮控制元素隐藏与显示-css中鼠标的hover状态 详解CSS display:inline-block的应用 学习css布局]]></content>
      <tags>
        <tag>css</tag>
        <tag>position</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用github展示前端页面]]></title>
    <url>%2Famberwest.github.io%2F2018%2F08%2F02%2F%E4%BD%BF%E7%94%A8github%E5%B1%95%E7%A4%BA%E5%89%8D%E7%AB%AF%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[最近在百度前端技术学院学习前端，需要将自己写的静态页面展示出来。根据网站提供的资源了解到原来可以使用github page来展示前端页面，真的是很赞了。 要展示已有代码的repository的页面，只需要三步即可。 创建gh-pages分支 之前上传的代码都默认存在master分支上，所以需要创建新的分支并切换 1git checkout --orphan gh-pages 上传代码切换到gh-pages分支之后，需要将之前的代码重新提交一遍 123git add *git commit -m "first commit"git push -u origin gh-pages 这个分支里的代码就是用来展示页面的 页面访问现在我们就可以通过下面的链接就可以查看前端页面了。 http(s)://&lt;username&gt;.github.io/&lt;projectname&gt; 原本的页面源码链接长这样： https://github.com/amberwest/Front_End_Study/blob/master/day3/index.html 现在将链接修改为下面这样，打开就是对应的前端页面了： https://amberwest.github.io/Front_End_Study/day3/index.html。 参考文章怎么预览 GitHub 项目里的网页或 Demo？]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用python第三方库发送邮件]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F26%2F%E4%BD%BF%E7%94%A8python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[在python中用来发送邮件的第三方库主要是smtplib和yagmail。这里简单记录一下这两个库的使用。 SMTP是发送邮件的协议，python对SMTP的支持有smtplib和email两个模块，email负责构造邮件，smtplib负责发送邮件。yagmail库也是用SMTP协议来发送邮件的，不过整个过程相对于smtplib来说就简单多了。 SMTP发送邮件现在来构造一个简单的纯文本邮件。 在该例子中，发件人使用的是QQ邮箱，密码则是开启POP3/SMTP服务时的授权码，对应的host是smtp.qq.com。收件人用的是163邮箱，对应的host是smtp.163.com，可以把这些信息都暂时地写入环境变量，然后使用os模块的getenv方法获取。 1234567891011121314151617181920212223242526272829303132from email.header import Headerfrom email.mime.text import MIMETextfrom email.utils import parseaddr, formataddrimport smtplibimport osusername = '1000000@qq.com'password = 'qq_email_password'host = 'smtp.163.com'to = 'another_email@163.com'def _format_add(s): """格式化邮件地址""" name, addr = parseaddr(s) return formataddr((Header(name, 'utf-8').encode(), addr))# MIMEText三个参数： _text, _subtype='plain', _charset=Nonemsg = MIMEText('The first email', 'plain', 'utf-8')msg['From'] = _format_add('spider &lt;%s&gt;' % username)msg['To'] = _format_add('管理员 &lt;%s&gt;' % to)# 因为主题包含中文，所以需要Header对象进行编码msg['Subject'] = Header('来自SMTP的邮件', 'utf-8').encode()# 链接并发送邮件server = smtplib.SMTP(host, 25)# 使用server.set_debuglevel(1)可以打印出和SMTP服务器交互的所有信息server.set_debuglevel(1)server.login(username, password)# to_addr可以有多个邮箱server.sendmail(from_addr=username, to_addr=[to], msg.as_string())server.quit() yagmail发送邮件yagmail的使用就超级简单，不用email去构建邮件，直接上代码。 12345678910import yagmailyag = yagmail.SMTP(user=username, password=password, host=host)# 邮箱正文contents = ['This is the body']attachments = ['louis.png']# 发送邮件, 没提供邮箱则默认是发送给自己yag.send(to, subject=['test yagmail'], contents=contents, attachments=attachments) 为了避免密码暴露在脚本中，可以使用keyring模块，将密码保存在系统keyring服务中，成功设置之后就不用在脚本中提供密码了。 123import keyringkeyring.set_password('yagmail', username, password) 更多的用法可以查看官方文档。 完整的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding:utf-8 -*-# Time: 2018/7/26 09:57import yagmailfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.utils import parseaddr, formataddrimport smtplibimport osusername = os.getenv('QQ_USERNAME')password = os.getenv('QQ_PASSWORD')host = os.getenv('SMTP_SERVER')to = os.getenv('WY_USERNAME')def spider(): try: 1 / 0 except ZeroDivisionError as e: msg = ''.join(msg) yagmail_send(msg) smtplib_send(msg) print(msg)def yagmail_send(m): """将报错信息发送到邮件""" yag = yagmail.SMTP(username) contents = [m, 'louis.png'] yag.send(to, subject='error from spider', contents=contents)def smtplib_send(m): """使用smtplib模块来发送邮件""" msg = MIMEText(m, 'plain', 'utf-8') msg['From'] = _format_add('spider &lt;%s&gt;' % username) msg['To'] = _format_add('管理员 &lt;%s&gt;' % to) msg['Subject'] = Header('爬虫出错！！', 'utf-8').encode() server = smtplib.SMTP(host, 25) server.login(username, password) server.sendmail(username, [to], msg.as_string()) server.quit()def _format_add(s): name, addr = parseaddr(s) return formataddr((Header(name, 'utf-8').encode(), addr))if __name__ == '__main__': spider() 参考文章：SMTP发送邮件 yagmail]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>yagmail</tag>
        <tag>smtplib</tag>
        <tag>email</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[supervisor：配置及使用]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F24%2Fsupervisor%EF%BC%9A%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[supervisor是python2写的一款运维工具，不过目前并不支持python3。所以直接用pip3 install supervisor或者pipenv直接安装都会报错。而Mac自带的python版本是2.7，可以直接用brew来安装supervisor，之后在对应的项目指定运行python3去运行代码即可。 下面简单记录之前使用supervisor的过程： 安装supervisor直接使用brew安装 1brew install supervisor 生成默认配置文件存放配置文件的路径可以任意指定，只需要确保路径存在即可 1echo_supervisord_conf &gt; /usr/local/etc/supervisord.conf 另外，我在etc/目录下创建一个文件夹来存放其他项目的配置文件（如proj.conf，后文会提到，结构如下： 修改配置文件最后的include尤其重要，可以方便地为每个项目单独设置conf文件，这样就不用全部写在全局配置里面，也方便修改。 增加项目的配置文件这个配置文件也可以直接写入全局配置文件里的(supervisord.conf)。这里是单独设置，必须放在supervisor/下，也就是上一步中incluede指定的位置。 启动supervisordsupervisord是supervisor的服务端程序，启动supervisor管理的子进程，响应来自clients的请求。 在终端输入以下命令： 1supervisord -c /usr/local/etc/supervisord.conf 如果在配置文件中有开启[inet_http_server]和port=127.0.0.1:9001，那么成功启动之后就可以在浏览器里打开，查看项目的运行状态，重启、关闭项目等操作，非常便捷。 关闭supervisord时，通过ps aux | grep supervisord获得pid之后kill。更多信息可以查看官方文档。 启动supervisorctlsupervisorctl提供了一个类型shell的命令行界面，可以对子进程进行相关操作，比如查看状态、启动/停止/重启子进程等。 在终端输入以下命令(可以不用指定配置文件)： 1supervisorctl [-c …/*.conf] 退出supervisorctl时，在supervisorctl shell中输入exit退出。 进程组的使用supervisor同时还提供了另外一种进程组的管理方式，通过这种方式，可以使用supervisorctl命令来管理一组进程。更[program:x]的进程组不同的是，这里的进程是一个个的[program:x]。 当添加了上述配置后，progname1和progname2的进程名会变成thegroupname:progname1 和thegroupname:progname2，以后就要用后两个名字来管理进程了。 以后执行supervisorctl stop thegroupname:就能同时结束progname1和progname2，执行supervisorctl stop thegroupname:progname1就能结束progname1`。 参考文章：supervisor(一)基础篇 Running Supervisor]]></content>
      <tags>
        <tag>supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python：字符串格式化方法]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F22%2Fpython%EF%BC%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[python中常用的字符串格式化方法有：%操作符、str.format()格式，还有python3.6新增的f-strings。不过要尽量使用format方式和f-strings，而不是%操作符。 %操作符基本形式为：%[(name)][flags][width].[precision]typecode flags是格式化字符串转换标记。 转换标记 解释 - 表示左对齐 + 在正数前面加上+ ‘ ’(a space) 表示正数前面保留空格 # 在八进制树前显示零(‘0’)，在十六进制前显示Ox或者OX 0 表示转换值若位数不够用0填充而非默认的空格 typecode是格式化字符串转换类型。 转换类型 解释 s 字符串（采用str()的显示） r 字符串（采用repr()的显示） c 单个字符 b 二进制整数 i、d 十进制整数 o 八进制整数 x、X 十六进制整数 e、E 指数（基底写为e/E） f、F 浮点数 g、G 如果指数大于-4或者小于精度值则和e/E相同，其他情况和f/F相同 %操作符格式化字符串有以下几种常见用法： 直接格式化字符或者数值 1234&gt;&gt;&gt; price = 99&gt;&gt;&gt; s = 'the price is %+10.2f' % price'the price is +99.00' %+10.2f也可以写为%(price)+10.2f，根据基本形式%[(name)][flags][width].[precision]typecode，其中name为price，flages为+，width为10，precision为2，即保留两位小数，typecode为f，即浮点数。 以元组的形式格式化 123&gt;&gt;&gt; s = 'His name is %s, his age is %d' % ('Adam', 22)'His name is Adam, his age is 22' 以字典的形式格式化 1234&gt;&gt;&gt; data = &#123;'name':'Adam', 'age': 22 &#125;&gt;&gt;&gt; s = 'His name is %(name)s, his age is %(age)d' % data'His name is Adam, his age is 22' .format格式化str.format()的基本格式：[[填充符]对齐方式][符号][#][0][宽度][,][.精确度][转换类型]。填充符可以是除了”{“和”}”之外的任意符号，转换类型跟%操作符的转换类型类似。 对齐方式 对齐方式 解释 &lt; 左对齐，是大多数对象默认的对齐方式 &gt; 右对齐，数值默认的对齐方式 = 仅对数值类型又小，如果有符号的话，在符号后数值前进行填充，如-000029 ^ 居中对齐，用空格进行填充 符号列表 符号 解释 + 正数前加+，负数前加- - 正数前不加符号，负数前加-，为数值的默认形式 ‘ ‘(空格) 正数前加空格，负数前加- 跟%操作符不同的一点是，有些转换类型是不同的 转换类型 解释 !s 字符串，调用str()方法 !r 字符串，调用repr()方法 !a ascii() 123456&gt;&gt;&gt; "&#123;0!r:&gt;20&#125;".format("Hello") # '%+20r' % 'Hello'" 'Hello'"&gt;&gt;&gt; "&#123;0:&gt;20&#125;".format("Hello")' Hello' .format方法常见用法如下： 使用位置符号 123&gt;&gt;&gt; "The number &#123;0:,&#125; in hex is: &#123;0:#x&#125;, the number &#123;1&#125; in oct is &#123;1:#0&#125;".format(4746, 45)'The number 4,746 in hex is: 0x128a, the number 45 in oct is 45' 使用名称 123&gt;&gt;&gt; "the max number is &#123;max&#125;, the min number is &#123;min&#125;, the average number is &#123;average:0.3f&#125;".format(max=189, min=12.6, average=23.5)'the max number is 189, the min number is 12.6, the average number is 23.500' 如果需要进行更多的格式处理，就在:后面添加即可。如&quot;{0:^20}&quot;.format(&quot;Hello&quot;). 通过属性 1234567891011&gt;&gt;&gt; class Customer(object):&gt;&gt;&gt; def __init__(self, name, gender):... self.name = name... self.gender = gender &gt;&gt;&gt; def __str__(self):... return "Customer (&#123;self.name&#125;, &#123;self.gender&#125;)".format(self=self) &gt;&gt;&gt; str(Customer('Lisa', 'Female'))'Customer(Lisa, Female)' 格式化元组的具体项 使用这种方式，参数的顺序与格式化的顺序不必完全相同 1234567891011&gt;&gt;&gt; point = (1, 3)&gt;&gt;&gt; print('X:&#123;0[0]&#125;; Y:&#123;0[1]&#125;'.format(point))&gt;&gt;&gt; w = [('a', 10), ('b', 20), ('c', 30)]&gt;&gt;&gt; f = 'this is &#123;0[0]&#125;, this is &#123;0[1]&#125;'.format&gt;&gt;&gt; for i in map(f, w):... print(i) this is a, this is 10this is b, this is 20this is c, this is 30 f-stringspython3.6之后新增了f-strings格式化语法，格式如下： 1f &apos; &lt;text&gt; &#123; &lt;expression&gt; &lt;optional !s, !r, or !a&gt; &lt;optional : format specifier&gt; &#125; &lt;text&gt; ... &apos; 基本用法： 123456&gt;&gt;&gt; import datetime&gt;&gt;&gt; name = 'Fred'&gt;&gt;&gt; age = 50&gt;&gt;&gt; anniversary = datetime.date(1991, 10, 12)&gt;&gt;&gt; f'My name is &#123;name&#125;, my age next year is &#123;age+1&#125;, my anniversary is &#123;anniversary:%A, %B %d, %Y&#125;.''My name is Fred, my age next year is 51, my anniversary is Saturday, October 12, 1991.' 支持表达式： 1234&gt;&gt;&gt; def foo():... return 20&gt;&gt;&gt; f'result=&#123;foo()&#125;''result=20' 格式符： 123456789&gt;&gt;&gt; width = 10&gt;&gt;&gt; precision = 4&gt;&gt;&gt; value = decimal.Decimal('12.34567')&gt;&gt;&gt; f'result: &#123;value:&#123;width&#125;.&#123;precision&#125;&#125;''result: 12.35'# 如果是整数的话，则不能传递精确度age = 50f'&#123;age:^10&#125;' 与原始字符串使用： 123&gt;&gt;&gt; header = 'Subject'&gt;&gt;&gt; fr'&#123;header&#125;:\s+''Subject:\\s+' 使用花括号： 1234&gt;&gt;&gt; f"&#123;&#123; &#123;10 * 8&#125; &#125;&#125;" # 注意空格'&#123; 80 &#125;'&gt;&gt;&gt; f"&#123;&#123; 10 * 8 &#125;&#125;"'&#123; 10 * 8 &#125;' 与匿名函数使用： 12&gt;&gt;&gt; f'&#123;(lambda x: x*2)(3)&#125;' '6' 在f-strings中不能使用\，可以用单双引号进行区分，如果需要跨行，则使用三个单引号。 参考文章：提高python代码编写质量的91个建议 common string operations Literal String Interpolation]]></content>
      <tags>
        <tag>格式化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3：代理设置]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F19%2Fpython3-%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在爬虫过程有时候需要更换ip以防被封，主要有HTTP代理和socks5代理。 使用requests设置HTTP代理12345678import requestsproxies = &#123; "http": "http://127.0.0.1:8118", "https": "http://127.0.0.1:8118",&#125;requests.get("http://httpbin.org/get", proxies=proxies) 如果代理需要HTTP Basic Auth，可以使用http://user:password@host:port/。 使用requests设置socks5代理1、全局设置 使用socks模块，需要先安装依赖库 1pip install PySocks 12345678import socksimport socketimport requestssocks.set_default_proxy(socks.SOCKS5, '127.0.0.1', 1080)socket.socket = socks.socksocketresponse = requests.get('http://httpbin.org/get')print(response.text) 2、局部设置 requests也支持socks5代理，跟设置HTTP代理一样，可以只作用在部分请求上。不过需要先安装第三方库 1pip install requests[socks] 使用方法是之前类似，如果需要验证，格式类似：socks5://user:pass@host:port 123456789101112import requestsproxies = &#123; 'http': 'socks5://127.0.0.1:1080', 'https': 'socks5://127.0.0.1:1080'&#125;try: response = requests.get('http://httpbin.org/get', proxies=proxies) print(response.text)except requests.exceptions.ConnectionError as e: print(e.args) 使用urllib设置HTTP代理123456789101112131415from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_openerproxy = '127.0.0.1:8118'proxy_handler = ProxyHandler(&#123; 'http': 'http://' + proxy, 'https': 'https://' + proxy&#125;)opener = build_opener(proxy_handler)try: response = opener.open('http://httpbin.org/get') print(response.read().decode('utf-8'))except URLError as e: print(e.reason) 使用urllib设置socks5代理12345678910from urllib.request import ProxyHandler, build_opener, urlopenfrom urllib.error import URLErrorproxy_handler = ProxyHandler(&#123;'sock5': 'localhost:1080'&#125;)opener = build_opener(proxy_handler)try: response = urlopen('http://httpbin.org/get') print(response.read())except URLError as e: print(e.reason) urllib也可以像requests库设置全局代理那样设置socks5代理，具体代码跟之前类似： 123456789101112import socksimport socketfrom urllib.request import urlopenfrom urllib.error import URLErrorsocks.set_default_proxy(socks.SOCKS5, '127.0.0.1', 1080)socket.socket = socks.socksockettry: response = urlopen('http://httpbin.org/get') print(response.read())except URLError as e: print(e.reason) 最后一点说明，我在本地1080端口搭了socks5代理，并用privoxy将socks5代理转为HTTP代理，端口为8118。所以测试的时候使用不同端口代表不同的代理类型。具体设置过程可以查看之前的文章《Mac终端使用代理》。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis基本数据结构和基本操作]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F12%2Fredis%E5%B8%B8%E7%94%A8API%2F</url>
    <content type="text"><![CDATA[在终端登录到redis数据库的步骤如下： 登录redis-server 1redis-server /path/to/the/config/file 登录redis-cli 1234567# 如果没有密码就直接redis-cliredis-cli -a your-password# 也可以进入到redis之后再使用auth your-password进行验证# 如果是远程访问redis服务，前提是，远程redis服务也已经开启，方式如上所示redis-cli -h remote_ip -p port 选择数据库 1select 1 通用命令 命令 说明 备注 keys [pattern] 遍历符合条件的所有键 该命令一般不在生产环境中使用 dbsize 显示 当前库的key数目 exists key 查看某个key是否存在 ttl key 查看key还有多少时间才过期 结果&gt;0说明还有对应的秒数才过期； -1说明key存在，并且没有过期时间 -2说明key已经不存在了 persist key 去掉key的过期时间 del key1 key2 删除key-value 可以删除多个key，以空格隔开 expire key seconds（具体秒数） key在seconds秒之后过期 type key 返回key的类型 flushdb 删除当前数据库中所有key flushall 删除所有数据库的所有key config key * 查看所有的可配置选项 rename key newkey 修改key的名字 renamenx key newkey 仅当newkey不存在时，将key修改为newkey move key db 将当前数据库的key移动到给定的数据库db当中 dump key 序列化key，并返回被序列化的值 数据结构字符串（string）mget、mset时间复杂度为O(n)，其他的操作都为O(1) API 说明 例子 备注 set 设置/修改数据 set hello world 只能对单个操作，get和del也是 get 获取 get hello del 删除 del hello incr、decr 自增、自减 incr count key不存在那么执行之后key为1/-1 incrby、decrby 按指定数值进行增加(减少) incrby count 10 incrby/decrby key num setnx key不存在时才能设置成功 setnx php best setnx key value getset 更新并返回旧key getset count 99 getset key newvalue mset、mget 批量操作 mset k1 v1 k2 v2 mset / mget key1 value1 key2 value2 append 将value追加到旧的value后面 append k1 v3 append key value，类似字符串拼接 strlen 返回key的字节长度 strlen k1 strlen key incrbyfloat 给key增加浮点数 incrbyfloat num 3.5 incrbyfloat key num(float) getrange 获取字符串指定下标的所有值 getrange hello 1 3 getrange key start end，包括end setrange 设置指定下标对应的值 setrange hello 1 h setrange key index vale 列表（list） API 说明 例子 备注 rpush 从列表右边插入多个值 rpush listkey c b a rpush key value1 value2 value3 lpush 从列表左边插入多个值 lpush listkey c b a lpush key value1 value2 value3 linsert 在指定的value前面/后面插入newvalue linsert listkey before a java linsert key before / after value newvalue lpop 从列表左边弹出一个item lpop listkey lpop key rpop 从列表右边弹出一个item rpop listkey rpop key lrem 删除count个与value相同的项count&gt;0：从左到右最多删除count个value相等的项 ；count&lt;0：从右到左最多删除count个value相等的项 ； count=0：删除列表中所有等于value的项 lrem listkey -1 c lrem listkey 0 b lrem key count value lrange 获取列表指定索引范围所有item lrange listkey 2 5 lrange listkey -6 -1 lrange key start end(包含end） ltrim 按照索引范围修剪列表 ltrim listkey 0 6 ltrim key start end lindex 根据指定的index来获取对应的item lindex listkey 2 lindex key index llen 获取列表长度 llen listkey llen key lset 设置列表指定索引值为newvalue lset listkey 3 java lset key index newvalue blpop blpop的阻塞版本，指定阻塞时间，timeout=0为永不阻塞，O(1) blpop listkey 3 blpop key timeout brpop brpop的阻塞版本，指定阻塞时间，timeout=0为永不阻塞，O(1) brpop listkey 3 brpop key timeout 可以利用列表的一些特征实现一些功能： 1234567lpush+lpop=stacklpush+rpop=queuelpush+ltrim=capped collectionlpush+brpop=message queue 集合（set） API 说明 例子 备注 sadd 向集合key添加element，只有当element不存在才能成功添加 sadd user:1:follow it music his sports sadd key element1 element2 … srem 将集合key中的element剔除 srem user:1:follow his seem key element scard 获取集合key的长度 scard user:1:follow scard key sismember 判断element是否存在于集合key sismember user:1:follow it sismember key element srandmember 从集合key中随机取出一个item（与spop不同的是，该方法不删除element） srandmember user:1:follow srandmember key smembers 获取集合key中的所有item smembers user:1:follow smembers key spop 随机弹出一个元素（删除） spop user:1:follow spop key sdiff 差集 sdiff user:2:follow user:1:follow sdiff key1 key2 sinter 交集 sinter user:1:follow user:2:follow sinter key1 key2 sunion 并集 sunion user:1:follow user:2:follow sunion key1 key2 集合的一些应用： 12345sadd = tagging（标签管理）spop/srandmember = random item（随机数）sadd + sinter = social graph（社交，如关注） 散列表（hash）所有api都是h开头的，hgetall、hvals、hkeys、hmget、hmset时间复杂度都是O(n) API 说明 例子 备注 hget、hset、hdel 获取、添加、删除单个filed的值 hset user:1:info age 23 hset user:1:info name Tom hget user:1:info age hset key filed1 value1hget key fileddel key field 这个例子的key比较复杂，代表user中1号的信息，一般就是hset info name Tom hsetnx field不存在才能设置 hsetnx user:1:info hobby football hsetnx key field value hexists 判断hash key是否含有filed hexists user:1:info age hexists key field hlen 判断hash key filed的数量 hlen user:1:info hlen key hmget、hmset 批量操作 hmset user:2:info age 30 name kaka page 50 hmget user:2:info age name hmset key field1 value1 filed2 value2 hmget key field1 field2 hincrby 按指定的数量增加 hincrby user:1:info pageview 1 hincrby key field num没有hdecrby hincrbyfloat 增加浮点数 hincrbyfloat user:1:info pageview 3.5 hincrbyfloat key field num(float) hgetall 获取全部的fields hgetall user:1:info hgetall key hvals 获取全部的values hvals user:1:info hvals key hkeys 获取全部的keys hkeys user:1:info hkeys key 有序集合（zset） API 说明 例子 备注 zadd 添加score和elemen zadd user:1:ranking 2 kris zadd key score element(可以多对) zrem 删除元素 zrem user:1:ranking amber zrem key element (可以多对) zscore 返回元素的分数 zscore user:1:ranking tom zscore key element zincrby 对element增加/减少incrScore分数，inrcScore为负数表示减少 zincrby user:1:ranking 60 amber zincrby key incrScore element zcard 返回key的元素个数 zcard user:1:ranking zcard key zrank 返回元素在key中的排名 zrank user:1:ranking amber zrank key element zrange 列出key中元素指定范围的升序元素（分数） zrange user:1:ranking 0 -1 withscores 例子表示对全部的数据排名 zrange key start end [withscores] zrangebyscore 返回指定分数范围内的升序元素（分数） zrangebyscore user:1:ranking 15 25 withscores zrangebyscore key minScore maxScore [withscores] zcount 返回指定分数范围内的元素个数 zcount user:1:ranking 100 180 zcount key minScore maxScore zremrangebyrank 删除指定排名内的升序元素 zremrangebyrank user:1:ranking 0 1 zremrangebyrank key start end(包括end) zrevrange 反向排序，与zrange相反 zrevrange user:1:ranking 0 -1 zrevrange key start end zunionscore/zinterscore 集合间操作 zunionstore/zinterstore 关于key user:1:info和users:1的区别：]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义异常]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F12%2F%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[平时编程时难免会出现错误，一般可以使用try…except…来捕获。也可以使用raise语句主动抛出异常，使程序终止。如果这些都不够用的时候，我们就可以继承Exception然后自定义异常。 try … except123456789try: t = 1/0 print(t)except ZeroDivisionError as e: print(e)# 如果不清楚是什么异常的话，那么就直接使用Exceptionexcept Exception as e: print(e) 使用raise语句1raise ZeroDivisionError('error') 自定义异常1234567891011class TestError(Exception): def __init__(self, info): self.info = info Exception.__init__(self) def __str__(self): return self.infoif __name__ == '__main__': t = TestError('test') print(t) 如果是特定的错误，可以直接在str中定义： 12345678910class TestError(Exception): def __init__(self): Exception.__init__(self) def __str__(self): return repr(‘test’)if __name__ == '__main__': t = TestError() print(t)]]></content>
      <tags>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests：出现max retries exceeded with url错误]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F11%2Frequests%EF%BC%9A%E5%87%BA%E7%8E%B0max-retries-exceeded-with-url%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[之前偶然遇到这样一个错误： 出现这个情况是因为在短时间内打开太多的http链接，而且这些链接都是默认keep-alive的形式。那么可以通过以下几个方法来解决： 设置重试次数123import requestsrequests.adapters.DEFAULT_RETRIES = 5 在headers中修改Connection的方式为不持久1'Connection': 'close' 直接捕获异常1234try: res = requests.get(url)except requests.exceptions.ConnectionError: print('Connection Error') 也可以是因为请求失败而由本身发送重新链接的速度过快而导致该错误123456789import timepage = ''while page == '': try: page = requests.get(url) except: time.sleep(5) continue 其实呢，在遇到这个问题之前，我是遇到另一个错误： 这个错误跟前面一个很像，区别在于ProxyError，出错的原因一般有2个：1、IP代理服务器不能使用；2、IP代理使用的协议不正常。（我设置代理协议的时候把http写成了https） 参考文章：关于python爬虫的深坑：requests抛出异常Max retries exceeded with url python使用IP代理示例及出错解决方法]]></content>
      <tags>
        <tag>max retries</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多个spider写入csv时数据丢失]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F08%2F%E5%A4%9A%E4%B8%AAspider%E5%86%99%E5%85%A5csv%E6%97%B6%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%2F</url>
    <content type="text"><![CDATA[最近遇到一个问题，需要将多个spider抓取到的数据保存到同一个csv文件，但是数据会丢失。在测试的时候，发现数据不仅经常性丢失，而且每个spider写入数据前都会先写入列名，尝试通过判断文件是否存在来决定是否写入header，都不能解决问题。但是呢，分别写入不同的csv文件却不会出现数据丢失的情况，有点懵，看来对scrapy的运行机制的理解还是不够，目前还没找到问题所在。 在同事的帮助下，终于找到解决方法了： 1、先创建一个空的csv文件，只写入header 2、再用pandas将数据以追加的方式写入 12345678910111213141516171819202122232425# pipelines.py# -*- coding: utf-8 -*-import csvimport datetimeimport pandas as pdclass ScrapywebspiderPipeline(object): """将产品信息写入到csv""" columns = ['urls', 'status', 'title','price'] date = datetime.datetime.now().strftime('%Y%m%d%H%M%S') def __init__(self): self.file_name = '&#123;&#125;_&#123;&#125;.csv'.format('spider', self.date) # 创建csv文件，并写入头部 with open(self.file_name, 'w', newline='', encoding='utf-8') as f: c = csv.DictWriter(f, self.columns) c.writeheader() def process_item(self, item, spider): df = pd.DataFrame([item], columns=self.columns) df.to_csv(self.file_name, mode='a', header=False, index=False) return item 在根目录下新建run.py，写上运行多个脚本的代码 12345678910111213141516# -*- coding:utf-8 -*-# Time: 2018/7/2 16:30from scrapy.crawler import CrawlerProcessfrom scrapy.utils.project import get_project_settingsfrom scrapy.spiderloader import SpiderLoadersettings = get_project_settings()spider_loader = SpiderLoader(settings=settings)spiders = spider_loader.list()process = CrawlerProcess(settings=settings)for spider in spiders: process.crawl(spider)process.start() 直接在终端运行（切到scrapy项目根目录下） 1python -m run]]></content>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy中pipeline写入数据的方法]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F08%2Fscrapy%EF%BC%9Apipeline%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[用scrapy来抓取数据真的是非常方便，在数据存储方面也是很方便，同时也很灵活。这里总结了一些常用的数据写入的方法。 feed exporter使用scrapy提供的exporters可以很方便地将数据导出 12345678910111213141516# pipelines.pyfrom scrapy.exporters import JsonItemExporterclass JsonExporterPipeline(object): def __init__(self): self.file = open('test.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding='utf-8', ensure_ascii=False) self.exporter.start_exporting() def process_item(self, item, spider): self.exporter.export_item(item) return item def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() 如果将数据保存为json文件，其实也可以在终端使用-o来指定保存的文件路径。 1scrapy crawl spider -o test.json feed exporters还可以导出为其他格式，比如csv、xml等，具体用法查看官方文档。 123456789&#123; &apos;json&apos;: &apos;scrapy.exporters.JsonItemExporter&apos;, &apos;jsonlines&apos;: &apos;scrapy.exporters.JsonLinesItemExporter&apos;, &apos;jl&apos;: &apos;scrapy.exporters.JsonLinesItemExporter&apos;, &apos;csv&apos;: &apos;scrapy.exporters.CsvItemExporter&apos;, &apos;xml&apos;: &apos;scrapy.exporters.XmlItemExporter&apos;, &apos;marshal&apos;: &apos;scrapy.exporters.MarshalItemExporter&apos;, &apos;pickle&apos;: &apos;scrapy.exporters.PickleItemExporter&apos;,&#125; json1234567891011121314151617181920212223# pipelines.pyfrom scrapy.exceptions import DropItemimport jsonclass DuplicatesPipeline(object): # 自定义json文件的导出 def __init__(self): self.ids_seen = set() self.file = open('article.json', 'w', encoding="utf-8") def process_item(self, item, spider): # 去重 if item['id'] in self.ids_seen: raise DropItem("Duplicate item found: %s" % item) else: self.ids_seen.add(item['id']) lines = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(lines) return item def close_spider(self, spider): self.file.close() csv写入csv文件可以使用csv模块，需要提前指定列名。 1234567891011121314151617# pipelines.pyimport csvclass ScrapywebspiderPipeline(object): def __init__(self): columns = ['name', 'age'] file_name = 'test.csv' # newline为空，写入时不会出现空行 file = open(file_name, 'w', newline='', encoding='utf-8') self.writer = csv.DictWriter(file, columns) self.writer.writeheader() def process_item(self, item, spider): # 可以增加判断去重的条件 self.writer.writerow(item) return item excel用pandas写入excel时需要提供写入的行号，所以这里对写入的excel文件做了判断。 123456789101112131415161718192021222324252627282930313233343536# pipelines.pyimport osimport pandasfrom openpyxl import load_workbookclass ExcelPipeline(object): def process_item(self, item, spider): file_path = '&#123;&#125;.xlsx'.format(spider.name) file_is_exist = os.path.exists(file_path) columns = sorted(item.keys()) df = pandas.DataFrame([item], columns=columns) writer = pandas.ExcelWriter(file_path, engine='openpyxl') if not file_is_exist: df.to_excel( writer, header=True, sheet_name=spider.name, index=False, columns=columns, ) writer.save() else: book = load_workbook(file_path) row_start = book[spider.name].max_row writer.book = book writer.sheets = dict((ws.title, ws) for ws in book.worksheets) df.to_excel( writer, header=True, sheet_name=spider.name, index=False, columns=columns, ) writer.save() return item mongodb在settings.py文件中设置MONGO_URI,MONGO_DATABASE，还需要提前在终端开启mongod服务才能正常链接 1mongod --auth -f /path/to/your/mongod/conf # 如果没有密码，就不需要添加auth参数 1234567891011121314151617181920212223242526import pymongoclass MongoPipeline(object): def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db self.table_name = 'test' @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): # 这里的self.table_name是表名,也可以用spider.name来定义 self.db[self.table_name].update(&#123;'id': item.get('id')&#125;, &#123;'$set': dict(item)&#125;, upsert=True, manipulate=False) return item settings.py文件中增加以下内容，uri中的账号密码对应着database_name这个数据库，所以连接成功之后只看到这个数据库。 12MONG_URI = &apos;mongodb://user:pwd@ip:port/?authSource=database_name&amp;antuMechainsm=SCRAM-SHA-256&apos;MONGO_DATABASE = &apos;database_name&apos; mysql这里用的twisted模块进行异步插入数据。 12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding: utf-8 -*-# pipelines.pyimport copyfrom Jd.db import create_conn, create_database_and_tables, twisted_insert, handle_errorfrom Jd.items import SummaryItem, CommentItemfrom twisted.enterprise import adbapiclass MysqlTwistedPipeline(object): """使用twisted实现异步插入数据到mysql""" def __init__(self, dbpool): self.dbpool = dbpool # 建表 self.dbpool.runInteraction(create_database_and_tables) @classmethod def from_settings(cls, settings): db_args = dict( host=settings['MYSQL_HOST'], db=settings['MYSQL_DB'], user=settings['MYSQL_USER'], passwd=settings['MYSQL_PASSWORD'], charset='utf8', use_unicode=True, ) dbpool = adbapi.ConnectionPool('pymysql', **db_args) return cls(dbpool) def process_item(self, item, spider): # 异步插入数据到mysql，如果有错误则返回exception，正确则会自动commit，不需要人为commit，也不用创建cursor # 使用深拷贝，将引用传递改为值传递，有些数据重复有些数据丢失（总数量不变） asyn_item = copy.deepcopy(item) if isinstance(asyn_item, CommentItem): # runInteraction(callable, *args)，可调用的对象以及该对象所需要的参数 query = self.dbpool.runInteraction(twisted_insert, asyn_item, 'comment') elif isinstance(asyn_item, SummaryItem): query = self.dbpool.runInteraction(twisted_insert, asyn_item,'summary_comment') # 处理异常 query.addErrback(handle_error, asyn_item, spider) return asyn_item def close_spider(self, spider): self.dbpool.close() 该文件与pipelines.py在同级目录下，主要是定义一些常用的方法，比如连接数据库，插入数据等。这里有个点需要注意：comment表使用了id自增，这样会导致插入数据时无法起到去重的作用，因为这里是对全部字段进行比较，如果有重复就更新，但是id这个字段每次都是不同的，所以需要指定unique key。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#! -*- coding:utf-8 -*-# db.pyimport pymysqlfrom Jd.settings import *def create_conn(): conn = pymysql.connect(host=MYSQL_HOST, user=MYSQL_PORT, password=MYSQL_PASSWORD, port=MYSQL_PORT, db=MYSQL_DB, charset='utf8') return conndef create_database_and_tables(cursor): # 建立jingdong数据库 # cursor.execute('CREATE DATABASE jingdong') # cursor.execute('use jingdong') # 建立comment和summary_comment两张表 cursor.execute("""drop table comment""") cursor.execute("drop table summary_comment") comment_sql = """ CREATE TABLE IF NOT EXISTS comment( id int NOT NULL auto_increment, title VARCHAR (200) DEFAULT NULL , nick_name VARCHAR (20) NOT NULL , comment_time DATETIME NOT NULL , PRIMARY KEY (id), UNIQUE (pid, nick_name, comment_time) )ENGINE=InnoDB """ summary_comment_sql = """ CREATE TABLE IF NOT EXISTS summary_comment( pid VARCHAR (50) NOT NULL , title VARCHAR (50) DEFAULT NULL , PRIMARY KEY (pid) )ENGINE=InnoDB """ cursor.execute(comment_sql) cursor.execute(summary_comment_sql)def twisted_insert(cursor, data, table): """异步""" keys = ', '.join(data.keys()) values = ', '.join(['%s'] * len(data)) sql = "INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;) ON DUPLICATE KEY UPDATE".format(table=table, keys=keys, values=values) update = ','.join([" &#123;key&#125; = %s".format(key=key) for key in data]) sql += update try: cursor.execute(sql, tuple(data.values()) * 2) print('insert data successfully') except Exception as e: print(e)def handle_error(failure, item, spider): # 处理异常 print(failure)def insert(conn, data, table): cursor = conn.cursor() keys = ', '.join(data.keys()) values = ', '.join(['%s'] * len(data)) sql = """INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;) ON DUPLICATE KEY UPDATE""".format(table=table, keys=keys, values=values) update = ','.join([" &#123;key&#125; = %s".format(key=key) for key in data]) sql += update try: cursor.execute(sql, tuple(data.values()) * 2) conn.commit() print('insert data successfully') except Exception as e: print('failed to insert data', e) conn.rollback() 连接mysql所需要的字段都会在settings.py中进行设置 123456# 连接mysql的字段MYSQL_HOST = &apos;localhost&apos;MYSQL_PORT = 3306MYSQL_USER = &apos;&apos;MYSQL_PASSWORD = &apos;&apos;MYSQL_DB = &apos;jingdong&apos; 具体例子使用请看代码。 参考文章feed exports item pipelines How to write to an existing excel file without overwriting data (using pandas)? Authentication Examples]]></content>
      <tags>
        <tag>scrapy</tag>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根据部分文件名查找文件]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F08%2F%E6%A0%B9%E6%8D%AE%E9%83%A8%E5%88%86%E6%96%87%E4%BB%B6%E5%90%8D%E6%9F%A5%E6%89%BE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[在工作中，经常需要对抓取到的数据做进一步的处理，但由于文件是根据创建时间自动命名的，所以没办法直接在脚本中写入文件名，另外呢，还需要根据文件名判断哪个是最新创建的文件。最后决定用os模块来获取指定路径下的文件，并对文件名进行判断。 os模块的walk()函数接收一个顶级目录，通过从上到下或者从下到上的方式来遍历目录树，并返回一个三元组(dirpath, dirnames, filenames)，dirpath是相对于查找目录的相对路径，dirnames是该目录下的目录名列表， filenames是top目录下面的文件名列表。 1os.walk(top, topdown=True, onerror=None, followlinks=False) 至于文件名的判断，我用了python内置函数sorted()，通过传递一个匿名函数给key，将文件名中的时间转成数字，然后倒序排列即可。 1sorted(iterable, *, key=None, reverse=False) 1the_one = sorted(all_files, key=lambda x: int(re.search('(\d+)', x).group(1)), reverse=True)[0] 详细代码如下： 123456789101112131415161718192021222324252627import osdef find_the_latest(start, name): """ 根据给定的路径和文件名字，返回文件的路径 :param start: 搜索的最顶级目录 :param name: 要查找的目标文件 :return: 对应文件的完整路径 """ for relpath, dirs, files in os.walk(start): # 该文件名是以spider开头的 all_files = [t for t in files if name in t and t.startswith('spider')] if len(all_files) == 1: return os.path.normpath(os.path.abspath(all_files[0])) elif len(all_files) &gt; 1: # 如果指定目录下已经有多个文件，对文件名中的时间进行判断 the_one = sorted(all_files, key=lambda x: int(re.search('(\d+)', x).group(1)), reverse=True)[0] full_path = os.path.join(start, relpath, the_one) # 返回处理过的文件路径 return os.path.normpath(os.path.abspath(full_path)) else: return Noneif __name__ == '__main__': # 搜索当前目录下以spider开头的最新的csv文件 find_the_latest(‘.’, ’csv’) 参考文章： os sorted]]></content>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim：简单操作]]></title>
    <url>%2Famberwest.github.io%2F2018%2F07%2F06%2Fvim%EF%BC%9A%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[vim是从vi发展出来的一个文本编辑器。代码补完、编译以及错误跳转等方便变成的功能特别丰富。这里根据黑马程序员的基础课程总结了一些vim的基本操作。 基本模式普通模式普通模式下可以使用编辑器命令编辑文本，比如删除文本，移动光标等。这也是vim启动后的默认模式，可以通过按‘i’进入插入模式，当然还有其他的方式。 插入模式插入模式下可以插入文本，可以按ESC进入普通模式。 可视模式可视模式下，通过移动命令可以选中文本区域并高亮显示，执行非移动命令时，命令会被执行到这块高亮区域。可以按ESC键回到普通模式。 命令行模式在命令行模式中可以输入会被解释并执行的文本。例如执行命令’:’，’/‘， ‘?’或者过滤命令’!’，vim返回到命令行模式之前的模式，通常是普通模式。 基本命令进入文件1vim 文件名 [+num] # vim t.txt +3 如果文件名对应文件不存在，则会打开新的文件，编辑之后保存就能生成新的文件。 +num代表打开文件后光标定位到第num行，只带+光标则定位到文件的最后一行，不带+num，打开文件后光标会停在文件第一行。 退出在命令行模式下输入以下命令 12345:w # 保存不退出:q # 退出，如果编辑之后没有保存则会退出失败:q! # 强行退出，不保存改动:wq # 保存并退出:x # 保存并退出，跟:wq的效果是一样的 字符移动键盘上的hjkl分别对应左、下、上、右箭头键，如果要进行多次移动，可以先按数字再按对应的字母键，如‘10j’就是向下移动10行。 行内移动命令行模式下 123450 # 数字0，移动到行首^ # 移动到行首$ # 移动到行尾w # 向后移动一个单词（word）b # 向前移动一个单词（back） 行间移动命令行模式下 1234gg # 文件顶部G # 文件尾部数字gg # 移动到指定行数（3gg移动到第三行）:数字 # 也是移动到指定行数 屏幕移动命令行模式下 12345ctrl+b # 向下翻页ctrl+f # 向上翻页H # 屏幕顶部M # 屏幕中部L # 屏幕底部 段落移动vim中是用空行来区分段落的，以下命令也是在命令行模式下执行的 12&#123; # 上一段&#125; # 下一段 括号切换1% # 括号匹配 标记在开发时如有代码需要稍后处理，可以使用标记，这样可以在需要的时候快速的跳转。标记名称可以是a-zA-Z中任意一个字母。添加了标记的行如果被删除，标记同时被删除。 12mx # x是标记名称'x # 直接定位到标记所在位置 选中文本选中文本的命令需要在可视模式下，可以跟移动命令连用，如ggVG能够选中所有内容 123v # 可视模式，从光标位置开始安装普通模式选择文本V # 可视行模式，选中光标经过的完整行ctrl+v # 可视块模式，垂直方向选中文本 撤销和恢复撤销命令行模式下 12u # undo，撤销上次命令 ctrl+r # redo，恢复上次命令 删除文本1234x # 删除光标所在字符，或者选中文字d(移动命令) # 删除移动命令选中的内容dd # delete 删除光标所在行，可以ndd删除多行D # delete 删除至行尾 常见的删除命令组合 123456dw # 从光标位置删除到单词末尾d0 # 从光标位置删除到所在行的起始位置d&#125; # 从光标位置删除至段落结尾ndd # 从光标位置向下连续删除n行d代码行G # 从光标所在行删除至指定代码行之间的所有代码d'a # 从光标位置所在行删除到标记为a之间的所有代码 复制、粘贴123y(移动命令) # copy，复制yy # copy，复制一行，nyy可以复制多行，n为具体数字p # paste，粘贴 替换替换模式的作用就是不用进入编辑模式，对文件进行轻量级的修改。R命令可以进入替换模式，替换完成后，按下ESC可以回到命令模式。 12r # replace，替换当前字符，命令模式下R # replace，替换当前行光标后的字符，需要在替换模式下 缩进、编排命令行模式下 123&gt;&gt; # 向右缩进&lt;&lt; # 向左缩进. # 重复上一个命令 查找1、常规查找 1/str # 查找str 查找到内容后，使用next跳转到下一个出现的位置 12n # 查找下一个N # 查找上一个 2、单词查找 当要查找光标所在的单词时可以使用的命令 12* # 向后查找光标所在的单词# # 向前查找当前光标所在的单词 查找并替换命令格式如下： 1:%s///g 1、全局替换 一次性替换文件中所有出现的旧文本 1:%s/旧文本/新文本/g 2、可视区替换 先选中要替换的文件范围（可视模式下进行选择） 1:S/旧文本/新文本/g 3、确认替换 1:%s/旧文本/新文本/gc 回车后会有提示，根据提示进行选择对应的操作： y：yes，替换 n：no，不替换 a：all，全部 q：quit，退出替换 l：last，退换最后一个，并把光标定位到行首 ^E：向下滚屏 ^Y：向上滚屏 命令行命令主要是针对文件进行操作的：保存、退出、保存&amp;退出、搜索、替换、另存、新建文件、浏览文件，切换文件之前，必须确保当前文件已经被保存了。 123:e . # edit，会打开内置的文件浏览器，浏览当前目录下的文件:n 文件名 # new，新建文件:w 文件名 # write，另存为新文件，但是仍然编辑当前文件，并不会切换文件 分屏命令可以同时编辑和查看多个文件 12:sp [文件名] # split,横向增加分屏:vsp [文件名] # vertical split，纵向增加分屏 切换分屏窗口分屏窗口都是基于ctrl+w这个快捷键的 12345w # window，切换都下一个窗口r # reverse，互换窗口c # close，关闭当前窗口，但是不能关闭最后一个窗口q # quit，退出当前窗口，如果是最后一个窗口，则关闭vimo # other，关闭其他窗口 编辑命令和数字的联用12345在命令行模式下，输入数字（如5次）按下i进入编辑模式在编辑模式下进行操作（写入内容或者换行等操作都会被复制）按下ESC进入命令行模式，这时步骤2、3的操作会重复了5次如果再按下.则会将重复上一个操作，也就是步骤4，相当于操作了10次]]></content>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mitmproxy：安装配置]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F27%2Fmitmproxy%EF%BC%9A%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[mitmproxy是一个支持HTTP和HTTPS，并带有控制台的抓包工具。 安装mac下使用brew安装mitmproxy，会同时安装mitmdump和mitmweb两个组件。mitmdump是mitmporxy的命令行版本，mitmweb是一个基于web的mitmproxy接口。 1brew install mitmproxy 终端启动mitmproxy以生成CA证书 1mitmproxy -p 8888 # ctrl+c关闭，默认端口是8080，可以使用-p指定端口 生成的证书放在根目录下的隐藏文件.mitmproxy 配置电脑配置证书： 进入.mitmproxy文件夹，将mitmproxy-ca-cert.pem拉到钥匙串访问中，点击选择“始终信任” 手机配置证书： 1、查看电脑的ip 1ifconfig en0 | grep inet 2、设置手机http代理，修改服务器和端口（没有指定则为默认端口8080） 3、在手机浏览器中输入mitm.it，选择对应的证书进行安装、认证 这样就基本配置完成了，整个过程跟charles的配置是类似的，接下来就能在终端监听手机端的数据了。 使用对于数据抓取来说，可以通过charles、fiddler等抓包工具先找到数据接口，再通过mitmproxy来获取数据。 mitmdump可以运行脚本： 1mitmdump -p 8888 -s script.py 脚本如下： 1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-# Time: 2018/6/8 15:40from mitmproxy import ctximport jsonimport csvcolumns = ['title', 'author', 'cover', 'summary', 'price']file = open('books.csv', 'w', newline='', encoding='utf-8')writer = csv.DictWriter(file, columns)writer.writeheader()def response(flow): global writer url = "https://dedao.igetget.com/v3//discover/bookList" if flow.request.url.startswith(url): text = flow.response.text data = json.loads(text) books = data.get('c').get('list') for book in books: data = &#123; 'title': book.get('operating_title'), 'author': book.get('book_author'), 'cover': book.get('cover'), 'summary': book.get('other_share_summary'), 'price': book.get('price') &#125; writer.writerow(data) ctx.log.info(str(data)) 这里有两点需要注意的： mitmproxy需要python3.6及以上才能运行 例子来自崔庆才的《python3网络爬虫开发实战》，原例是将数据保存到mongodb，但是一直有错误，暂时还没解决 参考文章：mitmproxy]]></content>
      <tags>
        <tag>mitmproxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS High Sierra 安装pyspider出错]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F27%2FmacOS-High-Sierra-%E5%AE%89%E8%A3%85pyspider%E5%87%BA%E9%94%99%2F</url>
    <content type="text"><![CDATA[在安装pyspider的时候（已经安装了phantomJS），出现这样的错误提示： 12345 specify the SSL backend manually.&apos;&apos;&apos;) __main__.ConfigurationError: Curl is configured to use SSL, but we have not been able to determine which SSL backend it is using. Please see PycURL documentation for how to specify the SSL backend manually. ----------------------------------------Command &quot;python setup.py egg_info&quot; failed with error code 1 in /private/var/folders/47/d5b7q3_n4vn_1vjptchnm9z00000gp/T/pip-install-ggxo8959/pycurl/ 根据错误提示，应该是没办法获取到openssl的头文件才导致pycurl安装错误。经过一番搜索，了解到macOS High Sierra把SSL从OPenSSL 0.9.8zh 切换到LibreSSL。而且通过homebrew安装的openssl也是key-only，禁止通过brew link openssl添加软链接。所以需要设置一下LDFLAGS，CPPFLAGS，PYCURL_SSL_LIBRARY，以便编译时能够找到openssl。 安装pycurl： 12345pip uninstall pycurl # 卸载库export PYCURL_SSL_LIBRARY=opensslexport LDFLAGS=-L/usr/local/opt/openssl/libexport CPPFLAGS=-I/usr/local/opt/openssl/include # openssl相关头文件路径pip install pycurl --compile --no-cache-dir # 重新编译安装 接下来安装pyspider： 1pip install pyspider 参考文章： Installing PycURL on macOS High Sierra mac导入pycurl出错重装后还是报错]]></content>
      <tags>
        <tag>pyspider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas中reset_index和set_index用法]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F20%2Fpandas-reset-index%E5%92%8Cset-index%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在处理数据的时候，经常需要将抓取到的数据和运营提供的数据进行一个匹配更新，大多数情况下都是以商品的链接作为index进行匹配，所以在读数据的时候我都会使用index_col来指定。这样的话，能解决抓取数据和原始数据的匹配问题，但是呢，抓取数据就少了urls这一项（已作为index），最后更新完数据还是需要将商品链接给补上。使用reset_index()函数就可以轻松解决这个问题。 reset_index()该函数能够将行索引转为列。 1DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill=”) drop为False则索引会被还原为普通列，否则会丢失。 对于单索引问题，使用level=0跟level=‘index_name’的作用是一样的，也可以不提供level参数。 1dataframe.reset_index(level=0) 对于多级索引，情况稍微有点不同。 123456import pandas as pdimport numpy as npindex = pd.MultiIndex.from_product([['TX', 'FL', 'CA'], ['North', 'South']], names=['State', 'Direction'])df = pd.DataFrame(index=index, data=np.random.randint(0, 10, (6,4)), columns=list('abcd')) 直接调用reset_index()，默认是将全部级别的行索引都转为列数据，并使用一个RangeInddex来作为新索引。 1df.reset_index() 使用level参数可以选择指定的行索引。 123df.reset_index(level=1) # or df.reset_index(level=‘Direction’) 需要转换多个行索引时使用列表。 1df.reset_index(level=[‘State’,’Direction’]) 另外，我们将数据关于某个字段进行分组之后，groupby的键将会作为结果的index，如果不想要这样，可以在分组时设置as_index=False，就可以将groupby的键仍然作为列数据。 123df2 = df.reset_index()# 这里的first()是取每个分组的第一条记录df2.groupby('Direction', as_index=False).first() set_index()set_index()则与之相反 ，该函数用来设置行索引。 1DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False) 参数说明： 设置索引的参数是keys append添加新索引 drop为False时，成功设置了index，同时列数据中仍然保存着index字段 inplace为True时，修改了原始数据 单索引直接传入字符串即可。 1df2.set_index('State') 复合索引则输入列表。 1df2.set_index(keys=['State', 'Direction']) 可以很自由的设置复合索引的level，不过在这个例子没有体现出来 1df2.set_index(keys=['Direction', 'State']) 参考链接：How to convert pandas index in a dataframe to a column?]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>index</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas中根据指定list进行排序]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F20%2Fpandas%EF%BC%9A%E6%A0%B9%E6%8D%AE%E6%8C%87%E5%AE%9Alist%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[在实际操作中，经常需要根据指定的list对数据排序，而不单单是根据行索引，所以就需要将数据类型转为category。 注：作为类别的列表元素必须具有唯一性 原数据如下： 12345import pandas as pddata = &#123;'name': ['li', 'wang', 'zhang', 'hua'], 'grade': ['good', 'good', 'bad', 'excellent']&#125;df = pd.DataFrame(data) 现在，将学生按成绩等级从bad good excellent进行排序 将某一列数据转为category（dtype） 将该列按照给定的类别重新归类 12345df['grade'] = df['grade'].astype('category')list_custom = ['bad', 'good', 'excellent']df['grade'].cat.reorder_categories(list_custom, inplace=True)# inplace=True在原数据上生效df.sort_values('grade', inplace=True) 如果要将结果保存到新的变量上，可以省略inplace参数 1new_df = df.sort_values(‘grade') 另一种方法如下： 直接构建类别c 将某一列的dtype转为c 123456from pandas.api.types import CategoricalDtypelist_custom = ['bad', 'good', 'excellent']order = CategoricalDtype(list_custom, ordered=True)df['grade'] = df.grade.astype(order)df.sort_values('grade') 这两种方法，从逻辑上来讲，还是后者比较好理解。]]></content>
      <tags>
        <tag>pandas</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb常见错误]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F14%2Fmongodb%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[(code=exited, status=14)不知道怎么就出现了这个问题。在网上找了好一番，才知道是因为Mongodb SOCK不属于mongodb group和user导致的。 12# 错误信息，自己的忘记截图了，这是来自参考文章的信息mongod.service — High-performance, schema-free document-oriented database Loaded: loaded (/lib/systemd/system/mongod.service; disabled; vendor preset: enabled) Active: failed (Result: exit-code) since Wed 2016–12–07 03:43:00 UTC; 3min 9s ago Docs: https://docs.mongodb.org/manual Process: 24453 ExecStart=/usr/bin/mongod — quiet — config /etc/mongod.conf (code=exited, status=14) Main PID: 24453 (code=exited, status=14) 具体的解决方法： 进入tmp目录 1cd /tmp 查看是否有mongodb sock文件 1ls *.sock 修改user:group权限 1sudo chown mongodb:mognodb &lt;your_sock&gt; # 这里我是mongod 重启mongodb 1sudo service mongod start 查看mongodb状态 1sudo service mongod status 无法连接远程mongodb数据库当我在本地尝试连接到远程服务器时，遇到connection attempt failed这个错误。 一般遇到这种情况，要么是远程服务器没开启mongod服务，要么就是没开放端口。 1、开启远程mongod服务 12ssh user@192.122.122.100 # 登录mongod --auth -f /etc/mongod.conf # Linux中mongod的配置文件默认放在/etc下，auth是需要密码登录 然后就可以在本地尝试连接了 2、开放mongod服务端口 如果启动mongod服务之后还是不行，就需要检查一下防火墙。在centos7查看开放关闭端口都是用防火墙来控制的。 关闭mongod服务 1sudo service mongod stop 查看已经开放的端口 1sudo firewall-cmd --list-ports 开启端口 1sudo firewall-cmd --zone-public --add-port=27017/tcp --permanent 重启防火墙 12firewall-cmd --reloadsudo service firewalld start 重新尝试连接 错误码1和错误100出现错误码1和100很可能因为上次mongodb没有正常关闭. 1、如果是error number 100的话,一般找出之前的进程,然后kill就可以 2、杀掉进程后仍然无法启动mongd实例，很有可能是被锁住了，需要将mongod.lock文件及当天的lock文件给删除 其实还有一个当天的日志文件,不过被手动删除了.参考网上的做法: 123find / -name mongod.lock # 该文件将mongodb锁上防止用户操作rm -rf mongod.lock # 填写正确的文件地址rm -rf log.datetime # 删除当天日志文件 然后再尝试启动mongodb 3、经过上面两步操作之后，悲催地发现还是启动不了mongod实例。无奈啊！！！继续搜索之后发现可能是因为存储空间满了，需要清理空间或者加大存储空间 4、删除数据文件之后，可以启动mongd进程，但是登录验证的时候出错，退出重新登录（以为是密码之类的弄错），启动mongod进程又出现100错误码！！！草泥马～～～继续奋斗！根据网友的实践，可能还需要删除storage文件，结果还是不行！！！ 5、最后，我有个大胆的猜测，就是在清理数据库的时候，因为不清楚哪些是数据，哪些是不可删文件，当时一股脑全清空了（都是自己平时爬着玩的数据），可能将用户信息也可清理了。所以我没有用账号密码登录，居然。。。成。。。功。。。了。。。。 进入mongo shell的第一件事就是验证我的想法，嗯，还真是这样～～看来不带脑子真不行 相关知识点MongoDB连接远程服务器的命令格式如下： 1mongo 远程主机IP或DNS:MongoDB端口号/数据库名 -u user -p password # 如果没有设置账号密码就可以省略不写 常见的连接方式有： 1234567891011# 使用默认端口连接MongoDBmongo 192.122.109.55# 指定端口连接MongoDBmongo 192.122.109.55:27017# 连接MongoDB到指定数据库mongo 192.122.109.55:27017/test# 使用账号密码连接到指定的MongoDB数据库mongo 192.122.109.55:27017/test -u user -p passwd 参考文章MongoDB - Ubuntu 16.04(code=exited, status=14) AWS lightsail problem]]></content>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyenv在centos7和mac下的安装使用]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F14%2Fpyenv%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[由于mitmproxy需要python3.6才能使用，所以得安装新的python版本。pyenv可以非常方便的安装并管理python版本，还能顺带创建虚拟环境，可以说是非常良心了！ 这里简单记录一下mac OS下的安装使用。 pyenv安装pyenv1、mac 需要确保已经安装了Xcode Command Line Tools 和 Homebrew。 1brew install pyenv 2、centos7 安装pyenv依赖包 12yum -y install epel-releaseyum install git gcc zlib-devel bzip2-devel readline-devel sqlite-devel openssl-devel 获取pyenv源码 1git clone https://github.com/pyenv/pyenv.git $HOME/.pyenv 设置环境变量在命令行中运行以下命令 ，centos7则将~/.zshrc改为~/.bashrc 123echo 'export PYENV_ROOT="$HOME/.pyenv"' &gt;&gt; ~/.zshrcecho 'export PATH="$PYENV_ROOT/bin:$PATH"' &gt;&gt; ~/.zshrcecho 'eval "$(pyenv init -)"' &gt;&gt; ~/.zshrc # 启动shell的时候，初始化pyenv 使文件立即生效 1source ~/.zshrc 使用pyenv查看可安装的版本号,如果没有3.6及以上版本，需要先更新一下pyenv。 123cd ~/.pyenv &amp;&amp; git pull # updatepyenv install —list # 可安装列表 安装/卸载指定版本号(如：python3.6.5) 123pyenv install 3.6.5 # 安装 pyenv uninstall 3.6.5 # 卸载 查看已安装的python版本 12pyenv versions # 全部pyenv version # 当前的python版本 切换到指定python版本 1234cd … pyenv local 3.6.5 # 局部，当前目录有效 pyenv local system # 切换到系统自带版本 pyenv global 3.6.5 # 设置全局的python版本 *号代表当前的python版本号 取消已经设置的python版本 12pyenv local -—unset pyenv global --unset 接下来就可以创建虚拟环境了。 pyenv-virtualenv先安装pyenv的插件pyenv-virtualenv，作用跟vietualenv一样，可以为某个目录设置独立的python版本，方便项目管理。 1git clone https://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv 如果不指定python版本号（只有一个参数，将作为虚拟环境名称），则默认是当前环境的python版本号，name是虚拟环境的名字 1pyenv virtualenv 3.6.5 name 激活虚拟环境 1pyenv activate name 退出虚拟环境 1pyenv deactivate 删除虚拟环境 1234pyenv uninstall name # 或者删除其真实目录rm -rf ~/.pyenv/versions/name 自动激活/停用虚拟环境 在shell中添加以下命令，pyenv-virtualenv将在进入/离开有.python-version列出有效虚拟环境的文件的目录时会自动激活/停用virtualenv，这样就不用手动去设置。设置完成后需要source一下 1eval "$(pyenv virtualenv-init -)" pycharm中为项目添加对应的虚拟环境，也就是name下的python。 例子最后再来个完整的例子：在projects目录下创建一个新项目test，并设置项目使用的python版本为3.6.1 123456789[~]$ pyenv install 3.6.1 # 安装python3.6.1版本[~]$ cd projects/[projects]$ mkidr test[projects]$ cd test[projects test]$ pyenv virtualenv 3.6.1 py36 # 创建一个python版本为3.6.1的虚拟环境，名为py36[projects test]$ pyenv activate py36 # 激活虚拟环境(py36)[projects test]$ pyenv versions # 带*的就是当前的使用环境# ... start your performance ...(py36)[projects test]$ pyenv deactivate py36 # 退出虚拟环境 参考pyenv pyenv-virtualenv python版本管理]]></content>
      <tags>
        <tag>pyenv</tag>
        <tag>虚拟环境</tag>
        <tag>多版本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb：设置并使用账号密码登录]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F14%2Fmongodb%EF%BC%9A%E8%AE%BE%E7%BD%AE%E5%B9%B6%E4%BD%BF%E7%94%A8%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[登录mongodb12mongod -f /../mongodb/conf/mongo.conf # 具体路径根据所在目录决定mongo 127.0.0.1:28001 由于mongodb是没有默认管理员账号的，所以要先添加管理员账号，再开启权限认证。 创建管理员账号（必须在admin数据库下才是管理员账号）1234567891011121314use admindb.createUser( &#123; user: "adminUser", pwd: "adminPass", roles: [ &#123;role: "userAdminAnyDatabase", # 直接用root也是一样的效果 db: "admin"&#125; ] &#125;)# 认证db.auth("adminUser", "adminPass")# 查看用户db.getUsers() 创建普通用户，包括使用权限以及能够访问的数据库。123456789101112use foodb.createUser( &#123; user: "simpleUser", pwd: "simplePass", roles: [&#123;role: "readWrite", db: "foo"&#125;, &#123;role: "read", db: "bar"&#125;] &#125;)# 认证db.auth("simpleUser", "simplePass") 注意：一定要切换到对应的数据库再创建用户 退出mongodb之后，再次登录需要验证账号密码。没有验证，也可以正常登录，但是没办法进行操作。在终端操作mongodb可以有以下两种方法： 1、类似与mysql，在客户端连接时，指定用户名，密码以及对应的数据库名称12monogd --auth -f /.../mongodb/conf/mongo.confmongo 127.0.0.1:28001 -u "username" -p "password" --authenticationDatabase "db_name" 2、进入客户端之后再验证12monogd --auth -f /.../mongodb/conf/mongo.confmongo 127.0.0.1:28001 进入客户端之后切换到对应的数据库 12use testdb.auth('test', 'test') # username, password pymongo连接mongodb在python中可以通过pymongo模块连接到mongodb，需要先在终端开启mongod服务，然后才能正常使用mongodb。 1、终端开启服务 1mongod --auth -f /Users/amber/opt/mongodb/conf/mongo.conf 2、连接 mongodb没有设置用户时，直接连接就可以使用。 123456# mongodb没有设置账号密码client = pymongo.MongoClient('localhost', 28001)db = client['test'] # 选择数据库collection = db['test'] # 选择collection# 或者用下面这种方法来获得collection，尽量避免db.test这样的写法collection = db.get_collection('test') 当mongodb设置了用户，连接时需要验证。 1234567891011121314151617# 测试的数据库和用户名密码都为test# 方法1、使用uriuri = 'mongodb://username:password@localhost:28001/db_name'client = pymongo.MongoClient(uri)db = client.get_database() # 获取uri中已经指定连接的数据库collection = db['test']# 方法2、直接使用用户名等字段，不指定authSource时默认是admin数据库client = pymongo.MongoClient('localhost', 28001, username='test', password='test', authSource='test')db = client['test']collection = db['test']# 方法3、直接连接数据库之后再验证client = pymongo.MongoClient('localhost', 28001)db = client['test']db.authenticate('test', 'test')collection = db['test'] 附录role角色 数据库用户角色：read、readWrite; 数据库管理角色：dbAdmin、dbOwner、userAdmin； 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； 备份恢复角色：backup、restore； 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 超级用户角色：root 内部角色：__system 角色说明 read：允许用户读取指定数据库 readWrite：允许用户读写指定数据库 dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profile userAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户 clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。 readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限 readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限 userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限 dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。 root：只在admin数据库中可用。超级账号，超级权限 dbOwner: readWrite + dbAdmin + dbAdmin 参考文章：mongodb Authentication Examples]]></content>
      <tags>
        <tag>mongodb</tag>
        <tag>登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb安装]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F14%2Fmongodb%EF%BC%9A%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[在centos7上安装官网上有多种安装方式，这里只选择.rpm Package方式，也是官网推荐的。 1、配置yum安装所需要的包管理系统 1sudo vim /etc/yum.repos.d/mongodb-org-4.0.repo 然后在文件里写入以下内容 123456[mongodb-org-4.0]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.0/x86_64/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc 2、安装Mongo DB安装包 1sudo yum install -y mongodb-org 如果想要安装特定的版本，可以运行下面的命令 1sudo yum install -y mongodb-org-4.0.2 mongodb-org-server-4.0.2 mongodb-org-shell-4.0.2 mongodb-org-mongos-4.0.2 mongodb-org-tools-4.0.2 按理说，你可以指定任何可用的MongoDB版本。但是，一旦有可用的新版本，yum会自动升级软件包，所以为了防止意外，可以将下面的exclude指令添加到/etc/yum.conf文件中： 1exclude=mongodb-org,mongodb-org-server,mongodb-org-shell,mongodb-org-mongos,mongodb-org-tools 到这里就已经安装完成了，配置文件默认在/etc/mongod.conf。 mac下的安装使用brew直接安装12brew updatebrew install mongodb 在根目录下创建/data/db文件夹，用来保存mongodb的数据，并且给该目录设置权限12sudo mkdir -P /data/dbsudo chown -R user_name /data # user_name是电脑用户 在终端启动mongod1mongod 在新窗口连接数据库1mongo 退出时需要先进入admin数据库再退出123use admindb.shutdownServer()exit 将mongodb安装到指定路径下载并解压安装包12curl -o /path/to/download # 安装包下载路径tar -zxvf mongodb-linux-x86_64-3.4.1.tgz # 安装包名字 将解压后的文件拷贝到指定目标1mv mongodb-linux-x86_64-3.4.1 /Users/amber/opt/mongodb 添加mongodb的bin路径到环境变量PATH中1export PATH=/Users/amber/opt/mongodb/bin:$PATH 在mongodb/下创建存放数据的目录，新建conf，log文件夹123mkdir -p /data/dbmkdir confmkdir log 在conf下创建mongo.conf配置文件，添加如下内容：123456789101112131415# 默认端口号是27017port=28001 bind_ip=127.0.0.1logpath=/Users/amber/opt/mongodb/log/28001.loglogappend=Truedbpath=/Users/amber/opt/mongodb/data/28001/# 为每一个数据库安装数据库名建立文件夹存放directoryperdb=True# 进程文件，方便停止mongodb，我直接放到了存放数据的目录下pidfilepath=/Users/amber/opt/mongodb/data/28001/28001.pid# 以守护进程的方式启动mongodfork=TrueoplogSize=10240# 开启权限验证，没设置账号密码之前先注释掉auth=True 用配置文件启动mongod！！！1mongod -f /../mongodb/conf/mongo.conf # 具体路径根据所在目录决定 打开mongo1mongo 127.0.0.1:28001 退出mongodb123use admindb.shutdownServer()exit 参考文章Install MongoDB Community Edition on Red Hat Enterprise or CentOS Linux]]></content>
      <tags>
        <tag>安装</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下更新pandas出现权限问题]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F13%2FMac%E4%B8%8B%E6%9B%B4%E6%96%B0pandas%E5%87%BA%E7%8E%B0%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在更新pandas时出现了这样的错误：1Could not install packages due to an EnvironmentError: [Errno 1] Operation not permitted 看到没有权限，即使觉得不对劲还是默默使用sudo进行更新，结果还是不行！上网一搜才发现问题出现在mac系统上，更新之后新系统多了个sip的机制，所以即使用sudo也没办法解决问题。 最后找到一个完美的解决方法：基于用户的权限来安装/更新(upgrade)包。1pip install &lt;package_name&gt; --user -U 参考文章：解决mac osx下pip安装ipython权限的问题]]></content>
      <tags>
        <tag>Mac</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas修改列名]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F13%2Fpandas%EF%BC%9A%E4%BF%AE%E6%94%B9%E5%88%97%E5%90%8D%2F</url>
    <content type="text"><![CDATA[修改数据的列名，用到的方法是rename()。123import pandas as pddf=pd.DataFrame(&#123;0:['apple', 'orange', 'peach'],1:['red', 'yellow', 'pink'], 'total': [10, 11, 12]&#125;) 我们可以有以下几种方法将total这个列名变为大写： 12345678# old waydf.rename(columns=&#123;'total': 'TOTAL'&#125;)# new waydf.rename(&#123;'total': 'TOTAL'&#125;, axis=1)# tricky way, 这里不适用！！!df.rename(str.upper, axis='columns') 需要注意： 以上方法修改列名并不改变原始数据，可以将修改结果重新赋值给变量，或者使用参数inplace=True直接在原始数据上修改 如果要修改多个列名，也可以用这些方法 axis可以是1，也可以直接用columns 第三种方式是对所有列名都改为大写，如果列名中出现非string类型就会报错 如果数据有很多列，手动写出全部的列名就太费劲了。所以可以先获取原始列名，接着使用dict和zip创建对应的列名字典。123col = df.columnsnew_col = dict(zip(col, ['fruit', 'color', 'total']))df.rename(columns=new_col, inplace=True)]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>修改列名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas中将一列数据转为多列数据]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F12%2Fpandas%EF%BC%9A%E5%88%97%E6%95%B0%E6%8D%AE%E8%BD%AC%E4%B8%BA%E5%A4%9A%E5%88%97%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[在工作中，有时候需要将同一个单元格里的数据进行拆分成多列数据，然后进行统计、分类等其他操作，这时候get_dummies()方法就可以派上用场了。 pandas和pandas.Series都有各自的get_dummies()方法，名字虽然相同，但是结果并不一样。Series的get_dummies()接受一个sep参数，默认是|，根据该分隔符将string进行切分，返回Dataframe。1Series.str.get_dummies(sep=&apos;|&apos;) 在下面两个例子中，可以看到DataFrame也使用了Series的get_dummies()，但是只返回拆分之后的DataFrame，需要再使用concat方法将原始数据合并。12345678910# Series中使用from pandas import SeriesSeries(['a|b', 'a', 'a|c']).str.get_dummies()# 在DataFrame中使用import pandas as pddf = pd.DataFrame(&#123;'id': [1,2,3], 'labels' : ["a,b,c", "c,a", "d,a,b",]&#125;)df['labels'].str.get_dummies(sep=',') 如果使用str.split()方法，也能拆分，但是并不能自动分列。1df2=pd.DataFrame(df['labels'].str.split(',').tolist()) pandas的get_dummies()方法：1pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) 参数说明： data：原始数据prefix：拆分后新列列名的前缀prefix_sep：前缀和原始字段的连接符dummy_na：是否忽略NAN列columns：指定拆分的列名，list.不指定列时,默认是对所有可分的列都进行拆分。sparse：Series和全部列拆分时返回“稀疏”Dataframedrop_first：是否移除拆分后的第一列数据 该方法并不会将列数据string进行分割，而是将指定列的数据的所有可能取值都作为列。 例子如下：1234import pandas as pddf = pd.DataFrame(&#123;'id': [1,2,3], "sex": ['male', 'female', 'male']&#125;)pd.get_dummies(df, columns=['sex']) 如果需要拆分的列数据是字典时，使用str.values。这里使用了pop方法，直接将这一列数据给删除了，也可以直接使用df[‘lables’]来处理之后再删掉多余列。最后一点，需要完整的数据可以使用concat之类的方法将其与原始df进行合并。1234import pandas as pddf = pd.DataFrame(&#123;'id': [1,2], 'labels' : [&#123;'a':11,'b':22&#125;, &#123;'c':33&#125;]&#125;)pd.DataFrame(df.pop('labels').values.tolist())]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>拆分数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac终端使用代理]]></title>
    <url>%2Famberwest.github.io%2F2018%2F06%2F11%2FMac%E7%BB%88%E7%AB%AF%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[由于OS X 10.11之后有SIP安全机制，已经不能够用一条简单的命令来解决，需要关闭SIP或者使用proxychains-ng之类的工具。但是呢，关闭SIP麻烦就算了，居然还有安全问题，像我这种渣渣，真的有点怕把自己给坑了。而privoxy这个软件刚好安装在/usr/local内，就不需要关闭SIP。 privoxy是一个HTTP协议过滤代理，它可以过滤网页，管理cookies，控制访问，除广告等，同时还支持单系统多用户网络。所以我们可以使用privoxy将shadowsocks的socks5代理转为http代理，这样就可以供终端使用了。 搭建梯子安装并设置shadowsocks根据之前的账号密码（搭建ss服务器时设置的）设置ss客户端。ss的默认端口是1080，可以通过命令查看：1cat ~/.ShadowsocksX/gfwlist.js | grep proxy 其中，proxy对应的socks5代理 安装privoxy1brew install privoxy 配置privoxy1vim /usr/local/etc/privoxy/config 在文件末尾输入：12listen-address 0.0.0.0:8118forward-socks5 / localhost:1080 . # 转发socks5协议 第一行是privoxy监听任意IP地址的8118端口，也就是privoxy的默认端口。第二行是ss的默认端口，最后还带有一个空格和句号，也就实现了将socks5代理转为http代理，在爬虫中如果设置代理就可以使用http://127.0.0.1:8118。 启动privoxy1sudo /usr/local/sbin/privoxy /usr/local/etc/privoxy/config 查看privoxy进程1ps -ef | grep privoxy 返回以下内容说明启动成功： 1502 4093 1 0 11:48上午 ?? 0:00.03 privoxy 关闭privoxy进程的话，直接使用kill ID（比如这里的ID是4093）；以后再启动，需要先进入到/usr/local/etc/privoxy，不然可能会报找不到配置文件的错误。 查看端口监听是否成功1netstat -an | grep 8118 测试shadowsocks12telnet 127.0.0.1 1086telnet 127.0.0.1 8118 使用privoxy在~/.zshrc中添加开关函数，也可以使用别名命令。例如：1alias proxy=&apos;export all_proxy=socks5://127.0.0.1:1080&apos; 完整代码如下：123456789101112function proxy_off()&#123; unset http_proxy unset https_proxy echo -e &quot;已关闭代理&quot;&#125;function proxy_on() &#123; export no_proxy=&quot;localhost,127.0.0.1,localaddress,.localdomain.com&quot; export http_proxy=&quot;http://127.0.0.1:8118&quot; export https_proxy=$http_proxy echo -e &quot;已开启代理&quot;&#125; 使配置立即生效1source ~/.zshrc 开启代理要确保privoxy已经开启 1proxy_on 可以使用以下命令查看IP：1curl ip.cn 关闭代理1proxy_off 参考文章：Mac命令行终端下使用shadowsocks翻墙命令行下使用代理Mac OSX终端走shadowsocks代理]]></content>
      <tags>
        <tag>mac</tag>
        <tag>代理</tag>
        <tag>privoxy</tag>
      </tags>
  </entry>
</search>
